% !Mode:: "TeX:UTF-8"
\chapter{visual odometer 1}
\label{cpt:7}
\thispagestyle{empty}

\begin{mdframed}
\textbf{main target}
\begin{enumerate}[labelindent=0em,leftmargin=1.5em]
\item understands the meaning of image feature points, and grasps the method of extracting feature points and matching feature points in multiple images in a single image.
\item Understands the principle of polar geometry, using the constraints on polar geometry to recover the three-dimensional motion of the camera between images.
\item understands the PNP problem and solves the three-dimensional motion of the camera using the correspondence between the known three-dimensional structure and the image.
\item understands the ICP problem and uses the matching relationship of the point cloud to solve the three-dimensional motion of the camera.
\item understands how to obtain the three-dimensional structure of the corresponding points on a two-dimensional image by triangulation.
\end{enumerate}
\end{mdframed}

The specific form of the equation of motion and the equation of observation are introduced in the book, and the solution method based on nonlinear optimization is explained. Beginning with this lecture, we conclude the basics and step into the topic: in the order of the second lecture, we introduce four modules: visual odometer, optimized backend, loopback detection and map construction. This lecture and the next lecture mainly introduce the methods commonly used in two types of visual odometers: feature point method and optical flow method. In this lecture, we will introduce what is a feature point, how to extract and match feature points, and how to estimate camera motion based on paired feature points.

\newpage
\includepdf{resources/other/ch7.pdf}

\newpage

\section{Feature point method}
In the second lecture, we say that a SLAM system is divided into a front end and a back end, and the front end is also called a visual odometer (VO). The VO estimates a rough camera motion based on the information of the adjacent image, providing a good initial value to the back end. The VO algorithm is mainly divided into two major categories: \textbf{feature point method} and \textbf{direct method}. The front end based on the feature point method has long been considered (and until now) the mainstream method of visual odometers. It has the advantages of stability, insensitivity to light and dynamic objects, and is a relatively mature solution. In this lecture, we will start with the feature point method, learn how to extract and match the image feature points, and then estimate the camera motion and scene structure between the two frames to achieve a two-frame visual odometer. This type of algorithm is sometimes referred to as Two-view geometry.

\subsection{feature point}
The core issue with VO is \textbf{how to estimate camera motion based on images}. However, the image itself is a matrix of brightness and color, and it would be very difficult to consider motion estimation directly from the matrix level. Therefore, it is convenient to first: select \textbf{point} from the image to compare \textbf{representative}. These points remain the same after a small change in camera angle, so we can find the same point in each image. Then, based on these points, the problem of camera pose estimation and the positioning of these points are discussed. In the classic SLAM model, we call these points \textbf{roadmap} (Landmark). In visual SLAM, road signs refer to image features.

According to Wikipedia's definition, image features are a set of information related to computing tasks, and the computing tasks depend on the specific application \textsuperscript{\cite{wiki:featurecv}}. In short, \textbf{features are another form of digital representation of image information}. A good set of features is critical to the ultimate performance on a given task, so researchers have spent a lot of time researching features over the years. Digital images are stored in a computer as a matrix of gray values, so the simplest, single image pixel is also a "feature." However, in visual odometers, we hope that \textbf{feature points remain stable after camera motion}, while gray values ​​are severely affected by illumination, deformation, and object material, and vary greatly between images and are not stable enough. Ideally, when there are small changes to the scene and camera perspective, the algorithm can also determine from the image which places are the same point. Therefore, only the gray value is not enough, we need to extract feature points from the image.

The feature points are some \textbf{special places} in the image. Take \autoref{fig:corner-feature}~ as an example. We can use the corners, edges and blocks in the image as representative places in the image. However, it is easier to pinpoint that the same corner appears in two images; the same edge is slightly more difficult, because the image is similar along the edge; the same block is the most difficult of. We found that the corners and edges in the image are more "special" than the pixel blocks, and the recognition between the different images is stronger. Therefore, an intuitive way to extract features is to identify corner points between different images and determine their correspondence. In this approach, corner points are so-called features. There are many corner extraction algorithms, such as Harris corner point \textsuperscript{\cite{Harris1988}}, FAST corner point \textsuperscript{\cite{Rosten2006}}, GFTT corner point \textsuperscript{\cite{Shi1994}}, etc. . Most of them are algorithms proposed before 2000.

However, in most applications, pure corners still do not meet many of our needs. For example, where it looks like a corner from a distance, when the camera approaches, it may not appear as a corner. Or, when the camera is rotated, the appearance of the corners changes, and it is not easy to recognize that it is the same corner. To this end, researchers in the field of computer vision have designed many more stable local image features in years of research, such as the famous SIFT\textsuperscript{\cite{Lowe2004}}, SURF\textsuperscript{\cite{Bay2006}} , ORB\textsuperscript{\cite{Rublee2011}}, and so on. Compared to plain corners, these artificially designed feature points can have the following properties:

\begin{enumerate}
\item \emph{Repeatability}: The same feature can be found in different images.
\item \emph{distinctness}: Different features have different expressions.
\item \emph{Efficiency}: In the same image, the number of feature points should be much smaller than the number of pixels.
\item \emph{Locality}: The feature is only related to a small image area.
\end{enumerate}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\linewidth]{vo1/corner-flat-line}\\
    \caption{ can be used as part of the image feature: corner points, edges, blocks. }
    \label{fig:corner-feature}
\end{figure}

The feature points consist of two parts: \textbf{key-point} and \textbf{descriptor}. For example, when we say "calculate SIFT feature points in an image", it means "extracting SIFT key points and calculating SIFT descriptors". The key point refers to the position of the feature point in the image, and some feature points also have information such as orientation, size, and the like. A descriptor is usually a vector that describes the information about the pixels around the key in a way that is artificially designed. The descriptor is designed according to the principle that "\textbf{similar appearance features should have similar descriptors}". Therefore, as long as the descriptors of the two feature points have similar distances in the vector space, they can be considered as the same feature points.

Historically, researchers have proposed many image features. They are somewhat accurate and still have similar expressions under camera motion and illumination changes, but correspondingly require a large amount of computation. Among them, SIFT (Scale-Invariant Feature Transform) is the most classic one. It fully considers the changes in illumination, scale, rotation, etc. that occur during image transformation, but it is followed by a large amount of computation. Since the extraction and matching of image features in the whole SLAM process is only one of many links, up to now (2016), the CPU of ordinary PC can not calculate SIFT features in real time, and locate and map \footnote{here refers to 30Hz real-time speed. }. So in SLAM we rarely use this "luxury" image feature.

For other features, consider reducing the accuracy and robustness to improve the speed of the calculation. For example, the FAST key points are a feature point that is particularly fast (note the "keypoint" expression here, indicating that it has no descriptors), while the ORB (Oriented FAST and Rotated BRIEF) feature is currently very representative. Real-time image features. It improves the FAST detection \textsuperscript{\cite{Rosten2006}} without directionality, and uses the extremely fast binary descriptor BURIEF\textsuperscript{\cite{calonder2010brief}} to make the whole image feature extraction greatly accelerate. According to the test described by the author in the paper, in the case of extracting about 1000 feature points simultaneously in the same image, the ORB takes about 15.3 ms, the SURF takes about 217.3 ms, and the SIFT takes about 5228.7 ms. It can be seen that the ORB maintains the characteristics of the rotation and scale invariance, and the speed is obviously improved. It is a good choice for the SLAM with high real-time requirements.

Most feature extractions have good parallelism and can be accelerated by devices such as GPUs. After GPU-accelerated SIFT, real-time computing requirements can be met. However, the introduction of GPU will bring about an increase in the cost of the entire SLAM. Whether the resulting performance improvement is sufficient to offset the computational cost of the system requires careful consideration by the system designer.

Obviously, there are a large number of feature points in the field of computer vision, which we cannot introduce in the book. In the current SLAM scheme, ORB is a good compromise between quality and performance. Therefore, we introduce the whole process of extracting features by ORB. If the reader is interested in feature extraction and matching algorithms, we recommend reading the related book \cite{Nixon2012}.
\subsection{ORB feature}

The ORB feature is also composed of \textbf{keypoint} and \textbf{descriptor}. Its key point is called "Oriented FAST", which is an improved FAST corner point. We will introduce below on what is a FAST corner point. Its descriptor is called the BRIEF (Binary Robust Independent Elementary Feature). Therefore, extracting the ORB feature is divided into the following two steps:
\begin{enumerate}
\item FAST corner extraction: Find the "corner points" in the image. Compared with the original FAST, the main direction of the feature points is calculated in the ORB, and the rotation invariant characteristics are added for the subsequent BRIEF descriptors.
\item BRIEF Descriptor: Describes the surrounding image area from which the feature points were extracted in the previous step. The ORB has made some improvements to the BRIEF, mainly referring to the use of previously calculated direction information in the BRIEF.
\end{enumerate}

The following describes FAST and BRIEF respectively.
\subsubsection{FAST keypoint}

FAST is a kind of corner point, which mainly detects the obvious change of the gray level of local pixels, and is known for its fast speed. The idea is that if a pixel differs greatly from the pixels in the neighborhood (too bright or too dark), then it is more likely to be a corner. Compared to other corner detection algorithms, FAST only needs to compare the brightness of pixels, which is very fast. Its detection process is as follows (see \autoref{fig:fastcorner}~):

\begin{enumerate}
\item picks the pixel $p$ in the image, assuming its brightness is $I_{p}$.
\item sets a threshold of $T$ (for example, 20\% of $I_{p}$).
\item takes the pixel $p$ as the center and selects 16 pixels on a circle with a radius of 3.
\item If the brightness of consecutive $N$ points on the selected circle is greater than $I_{p}+T$ or less than $I_{p}-T$, the pixel $p$ can be considered a feature point ($ N$ is usually taken as 12, which is FAST-12. Other commonly used $N$ are 9 and 11, which are called FAST-9 and FAST-11, respectively.
\item loops through the above four steps, performing the same operation for each pixel.
\end{enumerate}
In the FAST-12 algorithm, for more efficiency, a pre-test operation can be added to quickly eliminate most pixels that are not corner points. The specific operation is to directly detect the brightness of the first, fifth, 9, and 13 pixels on the neighborhood circle for each pixel. Only when 3 of these 4 pixels are greater than $I_{p}+T$ or less than $I_{p}-T$ at the same time, the current pixel may be a corner point, otherwise it should be directly excluded. Such pre-test operations greatly speed up corner detection. In addition, the original FAST corners often appear to be "stacked". Therefore, after the first pass detection, it is also necessary to use non-maximal suppression to preserve only the corner points of the response maxima in a certain area, thereby avoiding the problem of corner point concentration.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.9\linewidth]{vo1/fast-corner}
\caption{FASTFeatures\textsuperscript{\cite{Rosten2006}}. }
\label{fig:fastcorner}
\end{figure}

The calculation of FAST feature points is only to compare the difference in brightness between pixels, so the speed is very fast, but it also has the disadvantages of less repeatability and uneven distribution. In addition, the FAST corner points do not have direction information. At the same time, since it is fixed to take a circle with a radius of 3, there is a scale problem: where it looks like a corner point in the distance, it may not be a corner point when it is close. For the weakness of the FAST corner point without directionality and scale, the ORB adds a description of the scale and rotation. Scale invariance by constructing an image pyramid \footnote{pyramid refers to different levels of downsampling of the image to obtain images of different resolutions. }, and detect corner points on each layer of the pyramid to achieve. The rotation of the feature is achieved by the Intensity Centroid.

A common method of processing in the calculation of graphs in the pyramid. See \autoref{fig:pyramid} for a schematic diagram. The bottom layer of the pyramid is the original image. Each time we go up one level, we scale the image at a fixed magnification so that we have images of different resolutions. Smaller images can be seen as scenes that are seen from a distance. In the feature matching algorithm, we can match the images on different layers to achieve scale invariance. For example, if the camera is moving backwards, then we should be able to find a match in the upper layer of the previous image pyramid and the lower layer of the next image.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{vo1/pyramid}\\
    \caption{Use pyramids to match images at different zoom ratios. }
    \label{fig:pyramid}
\end{figure}

In terms of rotation, we calculate the image gray center of mass near the feature point. The so-called centroid refers to the center of the weight of the image block as the weight. The specific steps are as follows: \textsuperscript{\cite{Rosin1999}}:
\begin{enumerate}
\item In a small image block $B$, define the moment of the image block as
\[
M_{pq}=\sum_{x,y \in B}x^{p}y^{q}I(x,y), \quad p, q = \{0,1\}.
\]
\item The moment of the image block can be found by the moment:
\[
C=(\frac{m_{10}}{m_{00}},\frac{m_{01}}{m_{00}}).
\]
\item connects the geometric center of the image block $O$ with the centroid $C$, and gets a direction vector $\overrightarrow{OC}$, so the direction of the feature point can be defined as
\[
\theta = \arctan(m_{01}/m_{10}).
\]
\end{enumerate}
Through the above method, the FAST corner point has a description of scale and rotation, which greatly enhances the robustness of its representation between different images. So in the ORB, this improved FAST is called Oriented FAST.
\subsubsection{BRIEF descriptor}
After extracting the Oriented FAST key points, we calculate their descriptors for each point. The ORB uses a modified BRIEF feature description. Let us first introduce what is theRIEF.

BRIEF is a \textbf{binary} descriptor whose description vector consists of a number of 0s and 1s, where 0 and 1 encode the size relationship of two random pixels near the key (such as $p$ and $q$). : If $p$ is larger than $q$, take 1 and vice versa. If we take 128 such $p,q$, we end up with a 128-dimensional vector of 0,1 \textsuperscript{\cite{calonder2010brief}}. BRIEF uses a random selection of comparisons, very fast, and because of the use of binary expressions, it is also very convenient to store, suitable for real-time image matching. The original BRIEF specifier does not have rotational invariance and is therefore easily lost when the image is rotated. The ORB calculates the direction of the key points in the FAST feature point extraction stage, so the direction information can be used to calculate the "Steer BRIEF" feature after the rotation so that the ORB descriptor has better rotation invariance.

Thanks to rotation and scaling, the ORB still performs well under the transformation of translation, rotation and scaling. At the same time, the combination of FAST and BREIF is also very efficient, making ORB features very popular in real-time SLAM. We show the result of extracting an ORB from an image in \autoref{fig:ORB}~. Here's how to match features between different images.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=0.9\linewidth]{vo1/feature}\\
    \caption{The result of the ORB feature point detection provided by OpenCV. }
    \label{fig:ORB}
\end{figure}

\subsection{feature matching}

Feature matching (shown in \autoref{fig:feature-matching}~) is a critical step in visual SLAM. Broadly speaking, feature matching solves the data association in SLAM, which determines the current view. Correspondence between the road sign and the road sign seen before. By accurately matching the image and the image or the descriptor between the image and the map, we can alleviate a lot of burden for subsequent gesture estimation, optimization and other operations. However, due to the local characteristics of image features, mismatching is widespread and has not been effectively solved for a long time. It has become a major bottleneck in the performance improvement of visual SLAM. Part of the reason is that there are often a large number of repeating textures in the scene, making the descriptions very similar. In this case, it is very difficult to solve the mismatch using only the local features.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=0.9\linewidth]{vo1/feature-matching}
    \caption{Feature matching between two frames of images. }
    \label{fig:feature-matching}
\end{figure}

However, let's look at the correct match first, and then go back and discuss the mismatch problem after doing the experiment. Consider an image of two moments. If the feature point $x_{t}^{m} is extracted in the image $I_{t}$, m=1, 2, ..., M$ is extracted in the image $I_{t+1}$ Feature points $x_{t+1}^{n}, n=1,2,...,N$, how do you find the correspondence between these two collection elements? The simplest feature matching method is \textbf{Brute-Force Matcher}. That is, for each feature point $x_{t}^{m}$ and all $x_{t+1}^{n}$ measure the distance of the descriptor, and then sort, taking the nearest one as the matching point. The descriptor distance represents the \textbf{similarity degree} between the two features, but in practice, different distance metric norms can be taken. For the description of the floating point type, measure with Euclidean distance. For binary descriptors (such as the BRIEF), we often use the Hamming distance as a measure - the Hamming distance between two binary strings, which refers to its \textbf{different digits number}.

However, when the number of feature points is large, the amount of computation of the brute force matching method will become very large, especially when it is desired to match a certain frame and a map. This does not meet our real-time needs in SLAM. At this point, the \textbf{Fast Approximate Nearest Neighbor (FLANN)} algorithm is more suitable for the case of a large number of matching points. Since the theory of these matching algorithms has matured and the implementation has been integrated into OpenCV, its technical details are not described here. Interested readers can refer to the reading \cite{Muja2009}.

\section{Practice: Feature Extraction and Matching}
\begin{figure}[!htp]
\centering
\includegraphics[width=0.9\linewidth]{vo1/exp1-images.pdf}
\caption{Two frames of images used by the experiment. }
\label{fig:exp1-images}
\end{figure}

OpenCV has integrated most of the mainstream image features that we can easily call. Let's complete two experiments: In the first experiment, we demonstrate the feature matching of ORB using OpenCV. In the second experiment, we demonstrate how to handwrite a simple ORB feature according to the principle described above. Through the process of handwriting, the reader can understand the calculation process of the ORB more clearly and analogize it to other features.
\subsection{OpenCV ORB feature}
First we call OpenCV to extract and match the ORB. I have prepared two images for this experiment, located at 1.png and 2.png under slambook2/ch7/, as shown by \autoref{fig:exp1-images}~. They are two images from the public dataset \cite{Sturm2012} and we see a tiny movement in the camera. This section demonstrates how to extract and match ORB features. In the next section, we will demonstrate how to use the matching results to estimate camera motion.

The following program demonstrates how to use ORB:
\begin{lstlisting}[language=c++,caption=slambook2/ch7/orb_cv.cpp]
#include <iostream>
#include <opencv2/core/core.hpp>
#include <opencv2/features2d/features2d.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <chrono>

using namespace std;
using namespace cv;

int main(int argc, char **argv) {
    if (argc != 3) {
        cout << "usage: feature_extraction img1 img2" << endl;
        return 1;
    }
    //-- 读取图像
    Mat img_1 = imread(argv[1], CV_LOAD_IMAGE_COLOR);
    Mat img_2 = imread(argv[2], CV_LOAD_IMAGE_COLOR);
    assert(img_1.data != nullptr && img_2.data != nullptr);
    
    //-- 初始化
    std::vector<KeyPoint> keypoints_1, keypoints_2;
    Mat descriptors_1, descriptors_2;
    Ptr<FeatureDetector> detector = ORB::create();
    Ptr<DescriptorExtractor> descriptor = ORB::create();
    Ptr<DescriptorMatcher> matcher = DescriptorMatcher::create("BruteForce-Hamming");
    
    //-- 第一步:检测 Oriented FAST 角点位置
    chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
    detector->detect(img_1, keypoints_1);
    detector->detect(img_2, keypoints_2);
    
    //-- 第二步:根据角点位置计算 BRIEF 描述子
    descriptor->compute(img_1, keypoints_1, descriptors_1);
    descriptor->compute(img_2, keypoints_2, descriptors_2);
    chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
    chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
    cout << "extract ORB cost = " << time_used.count() << " seconds. " << endl;
    
    Mat outimg1;
    drawKeypoints(img_1, keypoints_1, outimg1, Scalar::all(-1), DrawMatchesFlags::DEFAULT);
    imshow("ORB features", outimg1);
    
    //-- 第三步:对两幅图像中的BRIEF描述子进行匹配，使用 Hamming 距离
    vector<DMatch> matches;
    t1 = chrono::steady_clock::now();
    matcher->match(descriptors_1, descriptors_2, matches);
    t2 = chrono::steady_clock::now();
    time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
    cout << "match ORB cost = " << time_used.count() << " seconds. " << endl;
    
    //-- 第四步:匹配点对筛选
    // 计算最小距离和最大距离
    auto min_max = minmax_element(matches.begin(), matches.end(),
        [](const DMatch &m1, const DMatch &m2) { return m1.distance < m2.distance; });
    double min_dist = min_max.first->distance;
    double max_dist = min_max.second->distance;
    
    printf("-- Max dist : %f \n", max_dist);
    printf("-- Min dist : %f \n", min_dist);
    
    //当描述子之间的距离大于两倍的最小距离时,即认为匹配有误.但有时候最小距离会非常小,设置一个经验值30作为下限.
    std::vector<DMatch> good_matches;
    for (int i = 0; i < descriptors_1.rows; i++) {
        if (matches[i].distance <= max(2 * min_dist, 30.0)) {
            good_matches.push_back(matches[i]);
        }
    }
    
    //-- 第五步:绘制匹配结果
    Mat img_match;
    Mat img_goodmatch;
    drawMatches(img_1, keypoints_1, img_2, keypoints_2, matches, img_match);
    drawMatches(img_1, keypoints_1, img_2, keypoints_2, good_matches, img_goodmatch);
    imshow("all matches", img_match);
    imshow("good matches", img_goodmatch);
    waitKey(0);
    
    return 0;
}
\end{lstlisting}
Run this program (you need to enter two image locations) and the output will be output:
\begin{lstlisting}[language=sh,caption=terminal input:]
% build/orb_cv 1.png 2.png
Extract ORB cost = 0.0229183 seconds.
Match ORB cost = 0.000751868 seconds.
-- Max dist : 95.000000
-- Min dist : 4.000000
\end{lstlisting}

\autoref{fig:exp1-results}~ shows the results of the routine. We see a large number of mismatches in unfiltered matches. After a single screening, the number of matches has been reduced a lot, but most matches are correct. Here, the basis of the screening is \textbf {Hamming distance is less than twice the minimum distance}, which is an empirical method of engineering, not necessarily theoretical basis. However, although the correct match can be filtered out in the sample image, we still cannot guarantee that the match obtained in all other images is correct. Therefore, in the latter motion estimation, it is also necessary to use an algorithm for removing mismatch. On my machine, the ORB extraction took 22.9 milliseconds (two images) and the matching took 0.75 milliseconds, showing that most of the computation was spent on feature extraction.

\begin{figure}[!htp]
\centering
\includegraphics[width=1.0\linewidth]{vo1/exp1-result}
\caption{Feature extraction and matching results. }
\label{fig:exp1-results}
\end{figure}

\subsection{Handwritten ORB feature}
Below we demonstrate the method of handwritten ORB features. This part of the code is more, the book only shows the core part of the code, the rest of the surrounding code, please readers from the code base.
\begin{lstlisting}[language=c++,caption=slambook2/ch7/orb_self.cpp（片段）]
typedef vector<uint32_t> DescType;
// ... 省略图片读取部分代码和测试代码
// compute the descriptor
void ComputeORB(const cv::Mat &img, vector<cv::KeyPoint> &keypoints, vector<DescType> &descriptors) {
    const int half_patch_size = 8;
    const int half_boundary = 16;
    int bad_points = 0;
    for (auto &kp: keypoints) {
        if (kp.pt.x < half_boundary || kp.pt.y < half_boundary ||
        kp.pt.x >= img.cols - half_boundary || kp.pt.y >= img.rows - half_boundary) {
            // outside
            bad_points++;
            descriptors.push_back({});
            continue;
        }
    
        float m01 = 0, m10 = 0;
        for (int dx = -half_patch_size; dx < half_patch_size; ++dx) {
            for (int dy = -half_patch_size; dy < half_patch_size; ++dy) {
                uchar pixel = img.at<uchar>(kp.pt.y + dy, kp.pt.x + dx);
                m01 += dx * pixel;
                m10 += dy * pixel;
            }
        }
    
        // angle should be arc tan(m01/m10);
        float m_sqrt = sqrt(m01 * m01 + m10 * m10);
        float sin_theta = m01 / m_sqrt;
        float cos_theta = m10 / m_sqrt;
        
        // compute the angle of this point
        DescType desc(8, 0);
        for (int i = 0; i < 8; i++) {
            uint32_t d = 0;
            for (int k = 0; k < 32; k++) {
                int idx_pq = i * 8 + k;
                cv::Point2f p(ORB_pattern[idx_pq * 4], ORB_pattern[idx_pq * 4 + 1]);
                cv::Point2f q(ORB_pattern[idx_pq * 4 + 2], ORB_pattern[idx_pq * 4 + 3]);
        
                // rotate with theta
                cv::Point2f pp = cv::Point2f(cos_theta * p.x - sin_theta * p.y, sin_theta * p.x + cos_theta * p.y) + kp.pt;
                cv::Point2f qq = cv::Point2f(cos_theta * q.x - sin_theta * q.y, sin_theta * q.x + cos_theta * q.y) + kp.pt;
                if (img.at<uchar>(pp.y, pp.x) < img.at<uchar>(qq.y, qq.x)) {
                    d |= 1 << k;
                }
            }
            desc[i] = d;
        }
        descriptors.push_back(desc);
    }
    
    cout << "bad/total: " << bad_points << "/" << keypoints.size() << endl;
}

// brute-force matching
void BfMatch(
    const vector<DescType> &desc1, const vector<DescType> &desc2, vector<cv::DMatch> &matches) {
    const int d_max = 40;
    
    for (size_t i1 = 0; i1 < desc1.size(); ++i1) {
        if (desc1[i1].empty()) continue;
        cv::DMatch m{i1, 0, 256};
        for (size_t i2 = 0; i2 < desc2.size(); ++i2) {
            if (desc2[i2].empty()) continue;
            int distance = 0;
            for (int k = 0; k < 8; k++) {
                distance += _mm_popcnt_u32(desc1[i1][k] ^desc2[i2][k]);
            }
            if (distance < d_max && distance < m.distance) {
                m.distance = distance;
                m.trainIdx = i2;
            }
        }
        if (m.distance < d_max) {
            matches.push_back(m);
        }
    }
}
\end{lstlisting}
In this demo we only show the calculation code and matching code of the ORB. In the calculation, we use a 256-bit binary description, which corresponds to eight 32-bit unsigned int data, and express it as DescType with typedef. Then, we calculate the angle of the FAST feature point according to the principle described above, and then use the angle to calculate the descriptor. In this code, the principle of trigonometric functions avoids the complicated $\arctan$ and $\sin$, $\cos$ calculations, thus achieving an accelerated effect. In the BfMatch function, we also use the \_mm\_popcnt\_u32 function in the SSE instruction set to calculate the number of 1 in an unsigned int variable to achieve the effect of calculating the Hamming distance. The running result of this program is as follows, and the matching result is as shown in \autoref{fig:matches}:

\begin{lstlisting}[language=sh,caption=Terminal output:]
Bad/total: 43/638
Bad/total: 8/595
Extract ORB cost = 0.00390721 seconds.
Match ORB cost = 0.000862984 seconds.
Matches: 51
\end{lstlisting}

\begin{figure}[!htp]
    \centering
    \includegraphics[width=0.8\linewidth]{vo1/matches}
    \caption{match result}
    \label{fig:matches}
\end{figure}

It can be seen that in this program, the extraction of the ORB takes only 3.9 milliseconds, and the matching takes only 0.86 milliseconds. We accelerated the extraction of ORB by 5.8 times with some simple algorithm modifications. Please note that compiling this program requires your CPU to support the SSE instruction set, which should be supported on most modern home CPUs. If we can further parallelize the extracted features, the algorithm can also have room for acceleration.

\subsection{Calculate camera motion}
We already have matching pairs of points. Next, we need to estimate the camera's motion based on the point pairs. Here the situation has changed due to the different principles of the camera:

\begin{enumerate}
\item When the camera is monocular, we only know the 2D pixel coordinates, so the problem is to estimate the motion based on \textbf{two sets of 2D points}. This problem is solved with \textbf{polar geometry}.
\item When the camera is binocular, RGB-D, or by some way to get the distance information, then the problem is to estimate the motion according to \textbf{two sets of 3D points}. This problem is usually solved by ICP.
\item If a group is 3D and a group is 2D, ie we get some 3D points and their projection position in the camera, we can also estimate the camera's motion. This problem is solved by \textbf{PnP}.
\end{enumerate}

Therefore, the following sections describe camera motion estimation in these three scenarios. We will start with the 2D−2D case with the least information, to see how it solves, and what are the troublesome problems in the solution process.

\section{2D−2D: opposite geometry}
\label{sec:epipolar-geometry}

\subsection{polar constraint}

Now, let's say we get a pair of paired feature points from the two images, as shown by \autoref{fig:doubleview}~. If there are a number of such matching points, the motion of the camera between the two frames can be recovered by the correspondence of these two-dimensional image points. How many pairs of "several pairs" are there? We will introduce it below. Let's first look at the geometric relationship between the matching points in the two images.
\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.6\linewidth]{vo1/fundamental}
	\caption{对极几何约束。}
	\label{fig:doubleview}
\end{figure}

Taking \autoref{fig:doubleview}~ as an example, we want to find the motion between two frames of images $I_{1}, I_{2}$, and set the motion from the first frame to the second frame as $\bm{ R}, \bm{t}$. The two camera centers are $O_{1}, O_{2}$. Now, consider that there is a feature point $p_{1}$ in $I_{1}$, which corresponds to the feature point $p_{2}$ in $I_{2}$. We know that the two are obtained by feature matching. If the matches are correct, they are indeed \textbf{projections of the same spatial point on the two imaging planes}. Here some terms are needed to describe the geometric relationship between them. First, connect $\overrightarrow{O_{1}p_{1}}$ and connect $\overrightarrow{O_{2}p_{2}}$ to intersect at $P$ in 3D space. At this time, point $O_{1}, O_{2}, P$ three points can determine a plane, called \textbf{Epipolar plane}. The intersection of the $O_{1}O_{2}$ line and the image plane $I_{1}, I_{2}$ is $e_{1}, e_{2}$. $e_{1}, e_{2}$ is called \textbf{Epipoles}, and $O_{1}O_{2}$ is called \textbf{Baseline}. We call the intersection line between the polar plane and the two image planes $I_{1}, I_{2}$, and $l_{2}$ is \textbf{Epipolar line}.

Intuitively, from the perspective of the first frame, the ray $\overrightarrow{O_1 p_1}$ is \textbf{the spatial position at which a pixel may appear}—because all points on the ray are projected to the same pixel. At the same time, if you don't know the position of $P$, then when we look at the second image, connect $\overrightarrow{e_2 p_2}$ (that is, the polar line in the second image) is $P$ possible The position of the projected projection, which is the projection of the ray $\overrightarrow{O_1 p_1}$ in the second camera. Now, since we determine the pixel position of $p_2$ by feature point matching, we can infer the spatial position of $P$ and the motion of the camera. To remind the reader, \textbf{this is thanks to the correct feature matching}. Without feature matching, we can't determine where $p_2$ is in the polar line. At that time, you must search on the polar line to get the correct match, which will be mentioned in the 12th lecture.

Now let's look at the geometric relationship here from an algebraic perspective. In the coordinate system of the first frame, the spatial position of $P$ is set to
\[
\bm{P}=[X,Y,Z]^\mathrm{T}.
\]
According to the pinhole camera model introduced in the fifth lecture, we know that the pixel position of two pixels $\bm{p}_1, \bm{p}_2$ is
\begin{equation}
S_1 {\bm{p}_1} = \bm{KP},\quad s_2 \bm{p}_2 = \bm{K}\left( \bm{RP + t} \right).
\end{equation}

Here $\bm{K}$ is the camera internal reference matrix, $\bm{R}, and \bm{t}$ is the camera motion of the two coordinate systems. Specifically, the $\bm{R}_{21}$ and $\bm{t}_{21}$ are calculated here because they convert the coordinates in the first coordinate system to the second coordinate system. under. If we want, we can also write them in Lie algebra.

Sometimes we use homogeneous coordinates to represent pixels. When using homogeneous coordinates, a vector will be equal to itself multiplied by any non-zero constant. This is usually used to express a projection relationship. For example, $s_1 \bm{p}_1$ and $\bm{p}_1$ are projection relationships, which are equal in the sense of homogeneous coordinates. We call this equality relationship \textbf{equal up to a scale}, which is recorded as:
\begin{equation}
s\bm{p} \simeq \bm{p}.
\end{equation}
Then, the above two projection relationships can be written as:
\begin{equation}
 {\bm{p}_1} \simeq \bm{KP},\quad \bm{p}_2 \simeq \bm{K}\left( \bm{RP + t} \right).
\end{equation}

Now, take:
\begin{equation}
{\bm{x}_1} = {\bm{K}^{ - 1}}{\bm{p}_1}, \quad {\bm{x}_2} = {\bm{K}^{ - 1}}{\bm{p}_2}.
\end{equation}

Here $\bm{x}_1, \bm{x}_2$ is the coordinate on the normalized plane of two pixels. Substituting the above formula, you get:
\begin{equation}
{\bm{x}_2} \simeq \bm{R} {\bm{x}_1} + \bm{t}.
\end{equation}

Both sides are left by $\bm{t}^\wedge$. Recall the definition of $^\wedge$, which is equivalent to doing the outer product with $\bm{t}$ on both sides:
\begin{equation}
\bm{t}^\wedge \bm{x}_2 \simeq \bm{t}^\wedge \bm{R} \bm{x}_1.
\end{equation}

Then, both sides are simultaneously multiplied by $\bm{x}_2^\mathrm{T}$:
\begin{equation}
\bm{x}_2^\mathrm{T} \bm{t}^\wedge \bm{x}_2 \simeq \bm{x}_2^\mathrm{T} \bm{t}^\wedge \bm {R} \bm{x}_1.
\end{equation}

Looking at the left side of the equation, $\bm{t}^\wedge \bm{x}_2$ is a vector that is perpendicular to both $\bm{t}$ and $\bm{x}_2$. When you make it inner with $\bm{x}_2$, you get 0. Since the left side of the equation is strictly zero, multiplying by any non-zero constant is also zero, so we can write $\simeq$ as the usual equal sign. So we got a succinct expression:
\begin{equation}
 \bm{x}_2^\mathrm{T} \bm{t}^\wedge \bm{R} \bm{x}_1 = 0.
\end{equation}

Re-substitute $\bm{p}_1, \bm{p}_2$, with:
\begin{equation}
\bm{p}_2^\mathrm{T} \bm{K}^{-\mathrm{T}} \bm{t}^\wedge \bm{R} \bm{K}^{-1} \ Bm{p}_1 = 0.
\end{equation}

These two expressions are called \textbf{polar constraint}, which is known for its simplicity. Its geometric meaning is $O_1, P, O_2$. Both the translation and the rotation are included in the polar constraint. We refer to the middle part as two matrices: the Fundamental Matrix $\bm{F}$ and the Essential Matrix $\bm{E}$, which further simplifies the pole constraint:
\begin{equation}
\bm{E} = \bm{t}^ \wedge \bm{R}, \quad \bm{F} = \bm{K}^{ -\mathrm{T}} \bm{E} {\bm {K}^{ - 1}}, \quad \bm{x}_2^\mathrm{T} \bm{E} {\bm{x}_1} = \bm{p}_2^\mathrm{T} \bm{F} {\bm{p}_1} = 0.
\end{equation}

The polar constraint gives a succinct representation of the spatial positional relationship of two matching points. Therefore, the camera pose estimation problem becomes the following two steps:

\begin{enumerate}
\item finds $\bm{E}$ or $\bm{F}$ based on the pixel location of the pairing point.
\item finds $\bm{R}, \bm{t}$ from $\bm{E}$ or $\bm{F}$.
\end{enumerate}

Since $\bm{E}$ and $\bm{F}$ only differ from the camera internal parameters, the internal parameters are usually known in SLAM\footnote{in the SfM study it may be unknown and to be estimated. }, so in practice it is often easier to use $\bm{E}$. Let's take $\bm{E}$ as an example to show how to solve the above two problems.

\subsection{essence matrix}
By definition, the essence matrix $\bm{E} = \bm{t}^\wedge \bm{R}$. It is a matrix of $3\times 3$ with 9 unknowns. So, isn't any matrix of $3 \times 3$ available as a cost matrix? From the construction of $\bm{E}$, there are some notable points:

\begin{itemize}
	\item The essence matrix is ​​defined by the pole constraint. Since the polar constraint is a constraint of \textbf{the equation is zero}, after multiplying $\bm{E}$ by any non-zero constant, the \textbf{polar constraint remains satisfied}. We call this thing $\bm{E}$ which is equivalent at different scales.
	\item According to $\bm{E} = \bm{t}^ \wedge \bm{R}$, you can prove that \textsuperscript{\cite{Hartley2003}}, the singular value of the essence matrix $\bm{E}$ must be Is the form of $[\sigma, \sigma, 0]^\mathrm{T}$. This is called the intrinsic property of \textbf{the essence matrix}.
	\item On the other hand, since there are 3 degrees of freedom for translation and rotation, $\bm{t}^\wedge \bm{R}$ has 6 degrees of freedom. But because of the scale equivalence, $\bm{E}$ actually has 5 degrees of freedom.
\end{itemize}

The fact that $\bm{E}$ has 5 degrees of freedom indicates that we can solve $bm{E}$ with at least 5 pairs of points. However, the intrinsic property of $\bm{E}$ is a non-linear property that can cause trouble in estimation. Therefore, it can also be considered only by considering its \textbf{scale equivalence}, using 8 pairs of points to estimate $\bm{E}$ - This is the classic \textbf{Eight-point-algorithm}\textsuperscript{\cite{Hartley1997, Longuet-Higgins1987}}. The eight-point method only takes advantage of the linear nature of $\bm{E}$, so it can be solved in a linear algebra framework. Let's look at how the eight-point method works.

Consider a pair of matching points whose normalized coordinates are $\bm{x}_{1}=[u_{1}, v_{1}, 1]^\mathrm{T}$, $\bm{x }_{2}=[u_{2}, v_{2}, 1]^{\mathrm{T}}$. According to the pole constraint, there are:
\begin{equation}
\begin{pmatrix}
U_{2}, v_{2}, 1
\end{pmatrix}
\begin{pmatrix}
 E_{1} & e_{2} & e_{3}\\
 E_{4} & e_{5} & e_{6}\\
 E_{7} & e_{8} & e_{9}
\end{pmatrix}
\begin{pmatrix}
U_{1}\\v_{1}\\1
\end{pmatrix}
=0.
\end{equation}

We expand the matrix $\bm{E}$ and write it as a vector:
\[
\bm{e}= [e_{1}, e_{2}, e_{3}, e_{4}, e_{5}, e_{6}, e_{7}, e_{8}, e_{9 }]^{\mathrm{T}},
\]
Then the polar constraint can be written as a linear form related to $\bm{e}$:
\begin{equation}
[u_{2}u_{1}, u_{2}v_{1}, u_{2}, v_{2}u_{1}, v_{2}v_{1}, v_{2}, u_{1 },v_{1},1] \cdot \bm{e}=0.
\end{equation}

For the same reason, the same representation is given for other point pairs. We put all the points into an equation and become linear equations ($u^i, v^i$ for the $i$ feature points, and so on):
\begin{equation}
\label{Eq:eight-point}
\begin{pmatrix}
U_{2}^{1}u_{1}^{1}& u_{2}^{1}v_{1}^{1}& u_{2}^{1}& v_{2}^{1 }u_{1}^{1}& v_{2}^{1}v_{1}^{1}& v_{2}^{1} &u_{1}^{1} &v_{1}^{1 }&1\\
U_{2}^{2}u_{1}^{2}& u_{2}^{2}v_{1}^{2}& u_{2}^{2}& v_{2}^{2 }u_{1}^{2}& v_{2}^{2}v_{1}^{2}& v_{2}^{2} &u_{1}^{2} &v_{1}^{2 }&1\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
U_{2}^{8}u_{1}^{8}& u_{2}^{8}v_{1}^{8}& u_{2}^{8}& v_{2}^{8 }u_{1}^{8}& v_{2}^{8}v_{1}^{8}& v_{2}^{8} &u_{1}^{8}&v_{1}^{8 }&1\\
\end{pmatrix}
\begin{pmatrix}
E_{1}\\ e_{2}\\ e_{3}\\ e_{4}\\ e_{5}\\ e_{6}\\ e_{7}\\ e_{8}\\ e_{ 9}  
\end{pmatrix}
=0.
\end{equation}

These eight equations form a linear system of equations. Its coefficient matrix consists of feature point locations and is $8 \times 9$. $\bm{e}$ is in the zero space of the matrix. If the coefficient matrix is ​​full rank (ie rank is 8), then its zero space dimension is 1, ie $\bm{e}$ constitutes a line. This is consistent with the scale equivalence of $\bm{e}$. If the matrix consisting of 8 pairs of matching points satisfies the condition of rank 8, then each element of $\bm{E}$ can be solved by the above equation.

The next question is how to recover the camera's motion $\bm{R}, \bm{t}$ based on the estimated essence matrix $\bm{E}$. This process is derived from singular value decomposition (SVD). Set the SVD of $\bm{E}$ to
\begin{equation}
\bm{E} = \bm{U} \bm{\Sigma} \bm{V}^\mathrm{T},
\end{equation}
Where $\bm{U}, \bm{V}$ is an orthogonal matrix, and $\bm{\Sigma}$ is a singular value matrix. According to the intrinsic nature of $\bm{E}$, we know $\bm{\Sigma} = \mathrm{diag}( \sigma, \sigma, 0 )$. In the SVD decomposition, for any $\bm{E}$, there are two possible $\bm{t}, \bm{R}$ corresponds to it:
\begin{equation}
\begin{array}{l}
\bm{t}_1^ \wedge = \bm{U}{\bm{R}_Z}(\frac{\pi }{2}) \bm{\Sigma} {\bm{U}^\mathrm{ T}}, \quad {\bm{R}_1} = \bm{U} \bm{R}_Z^\mathrm{T}(\frac{\pi }{2}){ \bm{V}^\mathrm{T}}\\
\bm{t}_2^ \wedge = \bm{U}{\bm{R}_Z}( - \frac{\pi }{2})\bm{\Sigma} {\bm{U}^\mathrm {T}}, \quad {\bm{R}_2} = \bm{U} \bm{R}_Z^\mathrm{T}( - \frac{\pi }{2}){\bm{V}^\mathrm{T}}.
\end{array}
\end{equation}

Where $\bm{R}_Z(\frac{\pi }{2})$ represents the rotation matrix obtained by rotating $90^\circ$ along the $Z$ axis. Also, since $-\bm{E}$ is equivalent to $\bm{E}$, taking the negative sign for any $\bm{t}$ will give the same result. Therefore, when decomposing from $\bm{E}$ to $\bm{t}, \bm{R}$, there are a total of \textbf{4} possible solutions.

\autoref{fig:epipolar-solution}~ visually shows the four solutions obtained by decomposing the essential matrix. We know the projection of the spatial point on the camera (blue line) (red dot) and want to solve the camera's motion. In the case of keeping the red point unchanged, four possible situations can be drawn. Fortunately, only the first solution, $P$, has a positive depth in both cameras. Therefore, as long as you substitute any point into the four solutions and detect the depth of the point under the two cameras, you can determine which solution is correct.

\begin{figure}[!htp]
\centering
\includegraphics[width=1.0\linewidth]{vo1/epipolar-solution}
\caption{Decompose the four solutions obtained by the essence matrix. There are four possible cases for the two cameras and space points while keeping the projection point (red point) unchanged. }
\label{fig:epipolar-solution}
\end{figure}

If you take advantage of the intrinsic nature of $\bm{E}$, then it has only 5 degrees of freedom. So at least 5 pairs of points can be used to solve camera motion \textsuperscript{\cite{Li2006, Nister2004a}}. However, this kind of practice is complicated. From the perspective of engineering implementation, since there are usually dozens of pairs or even hundreds of pairs of matching points, the significance of reducing from 8 pairs to 5 pairs is not obvious. To keep things simple, we will only introduce the basic eight-point method here.

There is one more problem left: $\bm{E}$, which is solved according to the linear equation, may not satisfy the intrinsic property of $\bm{E}$ - its singular value is not necessarily ${\sigma}, {\sigma}, 0$. At this time, we will deliberately adjust the $\bm{\Sigma}$ matrix to the above. The usual practice is to get the singular value matrix $\bm{\Sigma} = \mathrm{diag} ( \sigma_1, \sigma_2, \sigma_3)$ after SVD decomposition of the $\bm{E}$ obtained by the eight-point method. You can set $\sigma_1 \geqslant \sigma_2 \geqslant \sigma_3$. take:
\begin{equation}
\bm{E} = \bm{U} \mathrm{diag} (\frac{\sigma_1+\sigma_2}{2}, \frac{\sigma_1+\sigma_2}{2}, 0) \bm{V}^\mathrm{T}.
\end{equation}
This is equivalent to projecting the found matrix onto the manifold where $\bm{E}$ is located. Of course, the simpler approach is to take the singular value matrix as $\mathrm{diag} (1,1,0)$, because $\bm{E}$ has scale equivalence, so it is reasonable to do so.
\subsection{单应矩阵}
In addition to the basic and essential matrices, there is another common matrix in the two-view geometry: Homography $\bm{H}$, which describes the mapping between two planes. If the feature points in the scene fall on the same plane (such as wall, ground, etc.), motion estimation can be performed by homography. This is more common in overhead cameras that are carried by overhead drones or Sweepers. Since there was no mention of the singles before, I will introduce them here.

A homography matrix typically describes the transformation of some points on a common plane between two images. Consider having a pair of matching feature points $p_{1}$ and $p_{2}$ in the images $I_{1}$ and $I_{2}$. These feature points fall on the plane $P$, and this plane satisfies the equation:
\begin{equation}
\bm{n}^\mathrm{T} \bm{P} + d = 0.
\end{equation}
Slightly sorted out, got:
\begin{equation}
- \frac{\bm{n}^\mathrm{T} \bm{P} }{d} = 1.
\end{equation}

Then, reviewing equation (7.1) at the beginning of this section, you get:
\begin{align*}
\bm{p}_2 &\simeq \bm{K} ( \bm{R} \bm{P} + \bm{t} ) \\
&\simeq \bm{K} \left( \bm{R} \bm{P} + \bm{t} \cdot (- \frac{\bm{n}^\mathrm{T} \bm{P} }{d}) \right) \\
&\simeq \bm{K} \left( \bm{R} - \frac{\bm{t} \bm{n}^\mathrm{T} }{d} \right) \bm{P} \\
&\simeq \bm{K} \left( \bm{R} - \frac{\bm{t} \bm{n}^\mathrm{T} }{d} \right) \bm{K}^{ -1} \bm{p}_1.
\end{align*}

So, we get a transformation that directly describes the image coordinates $\bm{p}_1$ and $\bm{p}_2$, and writes the middle part as $\bm{H}$, so:
\begin{equation}
\bm{p}_2 \simeq \bm{H} \bm{p}_1.
\end{equation}

Its definition is related to the parameters of rotation, translation and plane. Similar to the base matrix $\bm{F}$, the homography matrix $\bm{H}$ is also a matrix of $3 \times 3$, and the idea of ​​solving is similar to $\bm{F}$, as well as Calculate $\bm{H}$ based on the matching point and then decompose it to calculate rotation and translation. Expand the above formula and get:
\begin{equation}
\begin{pmatrix}
U_{2}\\v_{2}\\1
\end{pmatrix}
\simeq
\begin{pmatrix}
 H_{1} & h_{2} & h_{3}\\
 H_{4} & h_{5} & h_{6}\\
 H_{7} & h_{8} & h_{9}
\end{pmatrix}
\begin{pmatrix}
U_{1}\\v_{1}\\1
\end{pmatrix}.
\end{equation}

Note that the equal sign here is still $\simeq$ instead of the normal equal sign, so the $\bm{H}$ matrix can also be multiplied by any non-zero constant. We can make $h_9 = 1$ in actual processing (when it takes a non-zero value). Then according to the third line, remove this non-zero factor, so there are:
\[
\begin{aligned}
U_{2}&=\frac{h_{1}u_{1}+h_{2}v_{1}+h_{3}}{h_{7}u_{1}+h_{8}v_{1} +h_{9}}\\
V_{2}&=\frac{h_{4}u_{1}+h_{5}v_{1}+h_{6}}{h_{7}u_{1}+h_{8}v_{1} +h_{9}}.
\end{aligned}
\]
Finished up:
\[
\begin{gathered}
H_{1}u_{1}+h_{2}v_{1}+h_{3}-h_{7}u_{1}u_{2}-h_{8}v_{1}u_{2}=u_ {2}\\
H_{4}u_{1}+h_{5}v_{1}+h_{6}-h_{7}u_{1}v_{2}-h_{8}v_{1}v_{2}=v_ {2}.
\end{gathered}
\]

Such a set of matching point pairs can construct two constraints (in fact, there are three constraints, but because of the linear correlation, only the first two), so the homography matrix with a degree of freedom of 8 can be calculated by four pairs of matching feature points ( In the case of non-degenerate, that is, the case where these feature points cannot have three points collinear), the following linear equations are solved (when $h_9 = 0$, the right side is zero):
\begin{equation}
\begin{pmatrix}
U_{1}^{1}& v_{1}^{1}& 1 & 0 & 0 & 0 & -u_{1}^{1}u_{2}^{1} & -v_{1}^ {1}u_{2}^{1}\\
0 & 0 & 0& u_{1}^{1}& v_{1}^{1}& 1 & -u_{1}^{1}v_{2}^{1} & -v_{1}^{ 1}v_{2}^{1}\\
U_{1}^{2}& v_{1}^{2}& 1 & 0 & 0 & 0 & -u_{1}^{2}u_{2}^{2} & -v_{1}^ {2}u_{2}^{2}\\
0 & 0 & 0& u_{1}^{2}& v_{1}^{2}& 1 & -u_{1}^{2}v_{2}^{2} & -v_{1}^{ 2}v_{2}^{2}\\
U_{1}^{3}& v_{1}^{3}& 1 & 0 & 0 & 0 & -u_{1}^{3}u_{2}^{3} & -v_{1}^ {3}u_{2}^{3}\\
0 & 0 & 0& u_{1}^{3}& v_{1}^{3}& 1 & -u_{1}^{3}v_{2}^{3} & -v_{1}^{ 3}v_{2}^{3}\\
U_{1}^{4}& v_{1}^{4}& 1 & 0 & 0 & 0 & -u_{1}^{4}u_{2}^{4} & -v_{1}^ {4}u_{2}^{4}\\
0 & 0 & 0& u_{1}^{4}& v_{1}^{4}& 1 & -u_{1}^{4}v_{2}^{4} & -v_{1}^{ 4}v_{2}^{4}
\end{pmatrix}
\begin{pmatrix}
 H_{1}\\h_{2}\\h_{3}\\ h_{4}\\h_{5}\\h_{6}\\ h_{7}\\h_{8}\\
\end{pmatrix}
=
\begin{pmatrix}
U_{2}^{1}\\ v_{2}^{1}\\ u_{2}^{2}\\ v_{2}^{2}\\u_{2}^{3}\\ V_{2}^{3}\\u_{2}^{4}\\ v_{2}^{4}
\end{pmatrix}.
\end{equation}

This approach treats the $\bm{H}$ matrix as a vector and restores $\bm{H}$ by solving the linear equation of the vector, also known as Direct Linear Transform. Similar to the essence matrix, the homography matrix needs to be decomposed in order to obtain the corresponding rotation matrix $\bm{R}$ and the translation vector $\bm{t}$. The methods of decomposition include the numerical method \textsuperscript{\cite{faugeras1988motion, Zhang1996}} and the parsing method \textsuperscript{\cite{malis2007deeper}}. Similar to the decomposition of the essential matrix, the decomposition of the homography matrix also returns four sets of rotation matrices and translation vectors, and at the same time, the normal vectors of the planes of the corresponding scene points are calculated. If the depth of the imaged map points is known to be all positive (ie in front of the camera), then the two sets of solutions can be excluded. In the end, there are only two sets of solutions, and more need to be judged by more a priori information. Usually we can solve by assuming the normal vector of the known scene plane, such as the scene plane parallel to the camera plane, then the theoretical value of the normal vector $\bm{n}$ is $\bm{1}^\mathrm{T}$ .

Homology is of great importance in SLAM. When the feature points are coplanar or the camera rotates purely, the degree of freedom of the base matrix decreases, and so-called degenerate occurs. The actual data always contains some noise. If you continue to use the eight-point method to solve the basic matrix, the excess degree of the basic matrix will be mainly determined by noise. In order to avoid the effects of degradation, we usually estimate the base matrix $\bm{F}$ and the homography matrix $\bm{H}$, and choose the one with the smaller reprojection error as the final motion estimation matrix.

\section{Practice: Solving camera motion with polar constraints}
Below, let's practice how to solve camera motion through the essential matrix. The program in the previous section provides feature matching, and this time we use the matching feature points to calculate $\bm{E}, \bm{F}$ and $\bm{H}$, which in turn breaks $ \bm{E}$Get $\bm{R}, \bm{t}$. The entire program is solved using the algorithm provided by OpenCV. We encapsulate the feature extraction from the previous section into a function for later use. This section only shows the code for the pose estimation section.
\begin{lstlisting}[language=c++,caption=slambook2/ch7/pose_estimation_2d2d.cpp （片段）]
void pose_estimation_2d2d(std::vector<KeyPoint> keypoints_1,
    std::vector<KeyPoint> keypoints_2,
    std::vector<DMatch> matches,
    Mat &R, Mat &t) {
    // 相机内参,TUM Freiburg2
    Mat K = (Mat_<double>(3, 3) << 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);
    
    //-- 把匹配点转换为vector<Point2f>的形式
    vector<Point2f> points1;
    vector<Point2f> points2;
    
    for (int i = 0; i < (int) matches.size(); i++) {
        points1.push_back(keypoints_1[matches[i].queryIdx].pt);
        points2.push_back(keypoints_2[matches[i].trainIdx].pt);
    }
    
    //-- 计算基础矩阵
    Mat fundamental_matrix;
    fundamental_matrix = findFundamentalMat(points1, points2, CV_FM_8POINT);
    cout << "fundamental_matrix is " << endl << fundamental_matrix << endl;
    
    //-- 计算本质矩阵
    Point2d principal_point(325.1, 249.7);  //相机光心, TUM dataset标定值
    double focal_length = 521;      //相机焦距, TUM dataset标定值
    Mat essential_matrix;
    essential_matrix = findEssentialMat(points1, points2, focal_length, principal_point);
    cout << "essential_matrix is " << endl << essential_matrix << endl;
    
    //-- 计算单应矩阵
    //-- 但是本例中场景不是平面，单应矩阵意义不大
    Mat homography_matrix;
    homography_matrix = findHomography(points1, points2, RANSAC, 3);
    cout << "homography_matrix is " << endl << homography_matrix << endl;
    
    //-- 从本质矩阵中恢复旋转和平移信息.
    recoverPose(essential_matrix, points1, points2, R, t, focal_length, principal_point);
    cout << "R is " << endl << R << endl;
    cout << "t is " << endl << t << endl;
}
\end{lstlisting}

This function provides the part of the camera that solves the motion of the camera from the feature point. Then we call it in the main function to get the camera's motion:
\begin{lstlisting}[language=c++,caption=slambook2/ch7/pose_estimation_2d2d.cpp （片段）]
int main( int argc, char** argv ){
    if (argc != 3) {
        cout << "usage: pose_estimation_2d2d img1 img2" << endl;
        return 1;
    }
    //-- 读取图像
    Mat img_1 = imread(argv[1], CV_LOAD_IMAGE_COLOR);
    Mat img_2 = imread(argv[2], CV_LOAD_IMAGE_COLOR);
    assert(img_1.data && img_2.data && "Can not load images!");
    
    vector<KeyPoint> keypoints_1, keypoints_2;
    vector<DMatch> matches;
    find_feature_matches(img_1, img_2, keypoints_1, keypoints_2, matches);
    cout << "一共找到了" << matches.size() << "组匹配点" << endl;
    
    //-- 估计两张图像间运动
    Mat R, t;
    pose_estimation_2d2d(keypoints_1, keypoints_2, matches, R, t);
    
    //-- 验证E=t^R*scale
    Mat t_x =
        (Mat_<double>(3, 3) << 0, -t.at<double>(2, 0), t.at<double>(1, 0),
        t.at<double>(2, 0), 0, -t.at<double>(0, 0),
        -t.at<double>(1, 0), t.at<double>(0, 0), 0);
    cout << "t^R=" << endl << t_x * R << endl;
    
    //-- 验证对极约束
    Mat K = (Mat_<double>(3, 3) << 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);
    for (DMatch m: matches) {
        Point2d pt1 = pixel2cam(keypoints_1[m.queryIdx].pt, K);
        Mat y1 = (Mat_<double>(3, 1) << pt1.x, pt1.y, 1);
        Point2d pt2 = pixel2cam(keypoints_2[m.trainIdx].pt, K);
        Mat y2 = (Mat_<double>(3, 1) << pt2.x, pt2.y, 1);
        Mat d = y2.t() * t_x * R * y1;
        cout << "epipolar constraint = " << d << endl;
    }
    return 0;
}
\end{lstlisting}

我们在函数中输出了$\bm{E}, \bm{F}$和$\bm{H}$的数值，然后验证了对极约束是否成立，以及$\bm{t}^\wedge \bm{R}$和$\bm{E}$在非零数乘下等价的事实。现在，调用此程序即可看到输出结果：
\begin{lstlisting}[language=sh,caption=终端输入：]
% build/pose_estimation_2d2d 1.png 2.png
-- Max dist : 95.000000 
-- Min dist : 4.000000 
一共找到了 79 组匹配点
fundamental_matrix is 
[4.844484382466111e-06, 0.0001222601840188731, -0.01786737827487386;
-0.0001174326832719333, 2.122888800459598e-05, -0.01775877156212593;
0.01799658210895528, 0.008143605989020664, 1]
essential_matrix is 
[-0.0203618550523477, -0.4007110038118445, -0.03324074249824097;
0.3939270778216369, -0.03506401846698079, 0.5857110303721015;
-0.006788487241438284, -0.5815434272915686, -0.01438258684486258]
homography_matrix is 
[0.9497129583105288, -0.143556453147626, 31.20121878625771;
0.04154536627445031, 0.9715568969832015, 5.306887618807696;
-2.81813676978796e-05, 4.353702039810921e-05, 1]
R is 
[0.9985961798781875, -0.05169917220143662, 0.01152671359827873;
0.05139607508976055, 0.9983603445075083, 0.02520051547522442;
-0.01281065954813571, -0.02457271064688495, 0.9996159607036126]
t is 
[-0.8220841067933337;
-0.03269742706405412;
0.5684264241053522]

t^R=
[0.02879601157010516, 0.5666909361828478, 0.04700950886436416;
-0.5570970160413605, 0.0495880104673049, -0.8283204827837456;
0.009600370724838804, 0.8224266019846683, 0.02034004937801349]
epipolar constraint = [0.002528128704106625]
epipolar constraint = [-0.001663727901710724]
epipolar constraint = [-0.0008009088410884102]
......
\end{lstlisting}

As can be seen from the output of the program, the accuracy of the polarity constraint is about $10^{-3}$. According to the previous discussion, the decomposition of $\bm{R}, \bm{t}$ has a total of 4 possibilities. However, OpenCV will use the triangulation to detect if the depth of the corner is positive, so as to choose the correct solution.

\subsection*{discussion}
As you can see from the demo, the output of $\bm{E}$ and $\bm{F}$ differs from the camera's internal reference matrix. Although they are not intuitive in value, they can be verified for their mathematical relationship. From $\bm{E}, \bm{F}$ and $\bm{H}$, motion can be decomposed, but $\bm{H}$ needs to assume that the feature points are on the plane. For the data in this experiment, this assumption is not good, so we mainly use $\bm{E}$ to decompose the movement here.

It is worth mentioning that since $\bm{E}$ itself has scale equivalence, it decomposes $\bm{t}, and \bm{R}$ also has a scale equivalence. And $\bm{R} \in \mathrm{SO}(3)$ itself has constraints, so we think $\bm{t}$ has a \textbf{scale}. In other words, in the decomposition process, multiplying $\bm{t}$ by any non-zero constant, the decomposition is true. Therefore, we usually make \textbf{normalized} to $\bm{t}$ and let it be equal to 1.

\subsubsection{Scale uncertainty}
The normalization of the length of $\bm{t}$ directly leads to \textbf{Scale Ambiguity}. For example, the first dimension of $\bm{t}$ output in the program is about 0.822. Whether this 0.822 refers to 0.822 meters or 0.822 cm, we can't be sure. Since multiplying $\bm{t}$ by an arbitrary proportional constant, the polar constraint is still true. In other words, in the monocular SLAM, the trajectory and the map are simultaneously scaled by any multiple, and the image we get is still the same. This has already been introduced to the reader in the second lecture.

In monocular vision, we normalize the $\bm{t}$ of the two images to \textbf{fixed scale}. Although we don't know what the actual length is, we calculate the 3D position of the camera motion and feature points in units of $\bm{t}$. This is called \textbf{initialization} for monocular SLAM. After initialization, you can use 3D−2D to calculate camera motion. The trajectory after initialization and the unit of the map are the fixed scales at initialization. Therefore, the monocular SLAM has one step inevitable \textbf{initialization}. The two images that are initialized must have a certain degree of translation, and the subsequent tracks and maps will be in units of the translation of this step.

In addition to normalizing $\bm{t}$, another method is to make all feature points have an average depth of 1 at initialization or a fixed scale. Compared to the method of making $\bm{t}$ length 1, the feature point depth can be normalized to control the size of the scene, making the calculation more stable. However, there is no theoretical difference.

\subsubsection{Initialized pure rotation problem}
In the process of decomposing from $\bm{E}$ to $\bm{R}, \bm{t}$, if the camera is a pure rotation, causing $\bm{t}$ to be zero, then the result is $\bm{E}$ will also be zero, which will cause us to solve $\bm{R}$. However, at this point we can rely on $\bm{H}$ to get the rotation, but only when we rotate, we can't estimate the spatial position of the feature points with triangulation (this will be mentioned below), so another conclusion is , \textbf {Mono initialization can not only have a pure rotation, there must be a certain degree of translation}. If there is no panning, the monocular will not be initialized. In practice, if the translation is too small during initialization, the pose solution and triangulation results will be unstable, resulting in failure. In contrast, if the camera is moved left and right instead of rotating in place, it is easy to initialize the monocular SLAM. Thus, experienced SLAM researchers often choose to have the camera pan left and right for smooth initialization in the case of a monocular SLAM.

\subsubsection{More than 8 pairs of points}
When the given number of points is more than 8 pairs (for example, the routine finds 79 pairs of matches), we can calculate a least squares solution. To recall the linearized pole constraint in \eqref{Eq:eight-point}, we write the coefficient matrix on the left as $\bm{A}$:
\begin{equation}
\bm{A} \bm{e} = \bm{0} .
\end{equation}

For the eight-point method, the size of $\bm{A}$ is $8 \times 9$. If the given matching point is more than $8$, the equation constitutes an overdetermined equation, ie, there is not necessarily $\bm{e}$ to make the above formula hold. Therefore, you can find by minimizing a quadratic form:
\begin{equation}
\mathop {\min }\limits_{\bm{e}} \left\| \bm{Ae} \right\|_2^2 = \mathop {\min }\limits_{\bm{e}} { \bm {e}^\mathrm{T}} {\bm{A}^\mathrm{T}} \bm{Ae}.
\end{equation}

Then we find the $\bm{E}$ matrix in the sense of least squares. However, when there may be mismatches, we prefer to use \textbf{Random Sample Concensus (RANSAC)} instead of least squares. RANSAC is a common practice for many cases with erroneous data and can handle data with mismatched matches.

\section{Triangulation}
In the previous two sections we estimated camera motion using polar geometry constraints and discussed the limitations of this approach. After getting the motion, the next step is to estimate the spatial position of the feature points using the motion of the camera. In monocular SLAM, the depth information of a pixel cannot be obtained by a single image. We need to estimate the depth of the map point by \textbf{Triangulation (or triangulation)}, such as \autoref{fig: Triangluar} is shown.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.9\linewidth]{vo1/triangularization}
\caption{Triangular to get the depth of the map point. }
\label{fig:triangluar}
\end{figure}

Triangulation refers to inferring the distance of a landmark point from the observed position by observing at the same landmark point at different locations. Triangulation was first proposed by Gauss and applied to surveying. It is used in the measurement of astronomy and geography. For example, we can estimate the distance from the stars by the angles of the stars observed in different seasons. In SLAM, we mainly use triangulation to estimate the distance of pixels.

Similar to the previous section, consider the images $I_{1}$ and $I_{2}$, with the left image as a reference and the right transformation matrix as $\bm{T}$. The camera's optical center is $O_{1}$ and $O_{2}$. There is a feature point $p_{1}$ in $I_{1}$, and there is a feature point $p_{2}$ in $I_{2}$. In theory, the straight line $O_{1}p_{1}$ and $O_{2}p_{2}$ will intersect in the scene at a point of $P$, which is the map point corresponding to the two feature points in the 3D scene. The location in . However, due to the influence of noise, these two lines often cannot intersect. Therefore, it can be solved by the second best multiplication method.

According to the definition in the polar geometry, let $\bm{x}_1, \bm{x}_2$ be the normalized coordinates of the two feature points, then they satisfy:
\begin{equation}
S_2 \bm{x}_2 = s_1 \bm{R} \bm{x}_1 + \bm{t}.
\end{equation}

Now that we know $\bm{R}, \bm{t}$, we want to solve the depth of two feature points $s_1, s_2$. Geometrically, you can find 3D points on the ray $O_1 p_1$ so that its projection position is close to $\bm{p}_2$. Similarly, you can find it on $O_2 p_2$, or in the middle of two lines. . Different strategies correspond to different calculation methods, of course, they are similar. For example, if we want to calculate $s_1$, first multiply the left side of the top by a $\bm{x}_2^\wedge$ to get:
\begin{equation}
\label{eq:x1tox2}
S_2 \bm{x}_2^\wedge \bm{x}_2 = 0 = s_1 \bm{x}_2^\wedge \bm{R} \bm{x}_1 + \bm{x}_2^\wedge \bm{t}.
\end{equation}

The left side of the equation is zero, and the right side can be seen as an equation of $s_2$, which can be directly obtained from $s_2$. With $s_2$, $s_1$ is also very easy to find. So we get the depth of the points under the two frames and determine their spatial coordinates. Of course, due to the existence of noise, we estimate that $\bm{R}, \bm{t}$ does not necessarily make the formula \eqref{eq:x1tox2} zero, so it is more common to find the least squares solution. Not a direct solution.

\section{Practice: Triangulation}
\subsection{Triangulation code}
Below, we demonstrate how to determine the spatial position of the feature points of the previous section by triangulation based on the camera pose previously solved with the polar geometry. We call the triangulation function provided by OpenCV for triangulation.
\begin{lstlisting}[language=c++,caption=slambook2/ch7/triangulation.cpp（片段）]
void triangulation(
	const vector<KeyPoint> &keypoint_1,
	const vector<KeyPoint> &keypoint_2,
	const std::vector<DMatch> &matches,
	const Mat &R, const Mat &t,
	vector<Point3d> &points) {
	Mat T1 = (Mat_<float>(3, 4) <<
		1, 0, 0, 0,
		0, 1, 0, 0,
		0, 0, 1, 0);
	Mat T2 = (Mat_<float>(3, 4) <<
		R.at<double>(0, 0), R.at<double>(0, 1), R.at<double>(0, 2), t.at<double>(0, 0),
		R.at<double>(1, 0), R.at<double>(1, 1), R.at<double>(1, 2), t.at<double>(1, 0),
		R.at<double>(2, 0), R.at<double>(2, 1), R.at<double>(2, 2), t.at<double>(2, 0)
	);
	
	Mat K = (Mat_<double>(3, 3) << 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);
	vector<Point2f> pts_1, pts_2;
	for (DMatch m:matches) {
		// 将像素坐标转换至相机坐标
		pts_1.push_back(pixel2cam(keypoint_1[m.queryIdx].pt, K));
		pts_2.push_back(pixel2cam(keypoint_2[m.trainIdx].pt, K));
	}
	
	Mat pts_4d;
	cv::triangulatePoints(T1, T2, pts_1, pts_2, pts_4d);
	
	// 转换成非齐次坐标
	for (int i = 0; i < pts_4d.cols; i++) {
		Mat x = pts_4d.col(i);
		x /= x.at<float>(3, 0); // 归一化
		Point3d p(
			x.at<float>(0, 0),
			x.at<float>(1, 0),
			x.at<float>(2, 0)
		);
		points.push_back(p);
	}
}
\end{lstlisting}

At the same time, add a triangulation part to the main function, and then draw a depth map of each point. Readers can run this program themselves to view the triangulation results.

\subsection{discussion}
There is one more point to note about triangulation.

Triangulation is obtained by \textbf{translation}, and there is a translation to have a triangle in the opposite geometry, so that triangulation can be said. Therefore, pure rotation is impossible to use triangulation because the polar constraint will always be satisfied. Of course, the actual data is often not exactly equal to zero. In the case of translation, we also care about the uncertainty of triangulation, which leads to a \textbf{triangulation contradiction}.

As shown in \autoref{fig:triangulation-discuss}~, when the translation is small, the uncertainty on the pixel will result in a large depth uncertainty. That is, if the feature point moves by one pixel $\delta x$ such that the line of sight angle changes by an angle of $\delta \theta$, then the depth value has a change of $\delta d$. As you can see from the geometric relationship, when $\bm{t}$ is large, $\delta d$ will be significantly smaller, which means that when the translation is large, the triangulation measurement will be more accurate at the same camera resolution. . Quantitative analysis of the process can be obtained using the sine theorem.

Therefore, to improve the accuracy of triangulation, one is to improve the extraction precision of feature points, that is, to improve the image resolution - but this will lead to larger images and increase computational cost. Another way is to increase the amount of translation. However, this can cause significant changes to the \textbf{appearance} of the image, such as the side of the box that was originally blocked, or the illumination of the object, and so on. Changes in appearance can make feature extraction and matching difficult. In summary, increasing the translation may result in a match failure; while the translation is too small, the triangulation is not accurate enough - this is the contradiction of triangulation. We call this problem paraparx.

\begin{figure}[!ht]
\centering
\includegraphics[width=1.0\linewidth]{vo1/triangulation-discuss.pdf}
\caption{The contradiction of triangulation. }
\label{fig:triangulation-discuss}
\end{figure}

In monocular vision, since the monocular image has no depth information, we wait for the feature point to be tracked for a few frames, then generate enough angle of view, and then use triangulation to determine the depth value of the new feature point. This time is also called delay triangulation \textsuperscript{\cite{Davison2003}}. However, if the camera is rotated in situ, resulting in a small parallax, it is not easy to estimate the depth of the newly observed feature points. This situation is more common in robotic situations because in-situ rotation is often a common command for robots. In this case, monocular vision may be subject to tracking failures, incorrect scales, and so on.

Although this section only introduces the depth estimation of triangulation, we can quantitatively calculate \textbf{location} and \textbf{uncertainty} for each feature point as long as we are willing. Therefore, if we assume that the feature point obeys the Gaussian distribution and continuously observes it, we can expect \textbf{its variance will decrease or even converge when the information is correct}. This gives a \textbf{filter} called \textbf{Depth Filter}. However, because its principle is more complicated, we will stay behind to discuss it in detail. Below, we discuss the estimation of camera motion from the 3D−2D matching points, and the 3D−3D estimation method.

\section{3D−2D:PnP}
PnP (Perspective-n-Point) is a method for solving 3D to 2D point pair motion. It describes how to estimate the pose of the camera when knowing $n$ 3D spatial points and their projected positions. As mentioned earlier, the 2D−2D parapolar geometry method requires 8 or more point pairs (taking the eight-point method as an example), and there are problems with initialization, pure rotation, and scaling. However, if the 3D position of one of the two feature points is known, then at least 3 point pairs (and at least one additional point verification result) are needed to estimate camera motion. The 3D position of the feature point can be determined by a triangulation or a depth map of the RGB-D camera. Therefore, in binocular or RGB-D visual odometers, we can directly estimate camera motion using PnP. In a monocular visual odometer, initialization must be performed before PnP can be used. The 3D−2D method is the most important attitude estimation method without using the polar constraint and obtaining better motion estimation in few matching points.

There are many ways to solve PnP problems, for example, P3P\textsuperscript{\cite{GaoHouTangEtAl2003}}, direct linear transformation (DLT), EPnP (Efficient PnP)\textsuperscript{\cite{LepetitMoreno-NoguerFua2008 with 3-point estimation pose. }}, UPnP\textsuperscript{\cite{Penate-SanchezAndrade-CettoMoreno-Noguer2013}}, and so on. In addition, you can use the \textbf{non-linear optimization} method to construct the least squares problem and iteratively solve it, which is the Bundle Adjustment. Let's look at the DLT first, then the Bundle Adjustment.

\subsection{Direct linear transformation}
We consider the problem of knowing the position of a set of 3D points and their projected position in a camera to find the pose of the camera. This problem can also be used to solve camera state problems for a given map and image. If the 3D point is viewed as a point in another camera coordinate system, it can also be used to solve the relative motion problems of the two cameras. We start with simple questions.

Consider a space point $P$ whose equality coordinate is ${\bm{P}}=(X,Y,Z,1)^{\mathrm{T}}$. In the image $I_{1}$, projected to the feature point ${\bm{x}}_{1}=(u_{1}, v_{1}, 1)^{\mathrm{T}}$( Expressed in normalized plane homogeneous coordinates). At this point the camera's pose $\bm{R}, \bm{t}$ is unknown. Similar to the solution of the homography matrix, we define the augmented matrix $[\bm{R}|\bm{t}]$ as a matrix of $3\times 4$, containing the rotation and translation information \footnote{Please note that This is different from the transformation matrix $\bm{T}$ in $\mathrm{SE}(3)$. }. We will write the expanded form as follows:
\begin{equation}
s
\begin{pmatrix}
U_{1} \\ v_{1} \\ 1
\end{pmatrix}
=
\begin{pmatrix}
T_{1} & t_{2} & t_{3} & t_{4}\\
T_{5} & t_{6} & t_{7} & t_{8}\\
T_{9} & t_{10} & t_{11} & t_{12}
\end{pmatrix}
\begin{pmatrix}
X \\ Y \\ Z \\ 1
\end{pmatrix}.
\end{equation}

Use the last line to eliminate $s$ and get two constraints:
\[
U_{1}=\frac{t_{1}X+t_{2}Y+t_{3}Z+t_{4}}{t_{9}X+t_{10}Y+t_{11}Z+ T_{12}},\quad
V_{1}=\frac{t_{5}X+t_{6}Y+t_{7}Z+t_{8}}{t_{9}X+t_{10}Y+t_{11}Z+ T_{12}}.
\]

To simplify the representation, define the row vector for $\bm{T}$:
\[
\bm{t}_{1}=(t_{1},t_{2},t_{3},t_{4})^\mathrm{T},
\bm{t}_{2}=(t_{5},t_{6},t_{7},t_{8})^\mathrm{T},
\bm{t}_{3}=(t_{9},t_{10},t_{11},t_{12})^\mathrm{T},
\]
Then there are:
\[
\bm{t}_1^\mathrm{T}\bm{P}-\bm{t}_3^\mathrm{T}\bm{P} u_1=0,
\]
with
\[
\bm{t}_2^\mathrm{T}\bm{P}-\bm{t}_3^\mathrm{T}\bm{P} v_1=0.
\]

Note that $\bm{t}$ is the variable to be sought. As you can see, each feature point provides two linear constraints on $\bm{t}$. Assuming a total of $N$ feature points, the following linear equations can be listed:
\begin{equation}
\begin{pmatrix}
\bm{P}_{1}^{\mathrm{T}} & 0 & -u_{1}\bm{P}_{1}^{\mathrm{T}} \\
0 & \bm{P}_{1}^{\mathrm{T}} & -v_{1}\bm{P}_{1}^{\mathrm{T}} \\
\vdots & \vdots & \vdots \\
\bm{P}_{N}^{\mathrm{T}} & 0 & -u_{N}\bm{P}_{N}^{\mathrm{T}} \\
0 & \bm{P}_{N}^{\mathrm{T}} & -v_{N}\bm{P}_{N}^{\mathrm{T}}
\end{pmatrix}
\begin{pmatrix}
\bm{t}_{1} \\ \bm{t}_{2} \\ \bm{t}_{3}
\end{pmatrix}
=0.
\end{equation}

Since $\bm{t}$ has a total of 12 dimensions, a linear solution of the matrix $\bm{T}$ can be achieved with a minimum of 6 pairs of matching points. This method is called Direct Linear Transform (DLT). ). When the matching point is greater than 6 pairs, the least squares solution can be obtained for the overdetermined equation using SVD or the like.

In the DLT solution, we directly consider the $\bm{T}$ matrix as 12 unknowns, ignoring the connection between them. Because the rotation matrix $\bm{R} \in \mathrm{SO}(3)$, the solution obtained by DLT does not necessarily satisfy the constraint, it is a general matrix. The translation vector is easier to handle, it belongs to the vector space. For the rotation matrix $\bm{R}$, we must find a best rotation matrix for the matrix block of the $\bm{T}$ left $3 \times 3$ estimated by the DLT. This can be done by QR decomposition \textsuperscript{\cite{Hartley2003, Chen1994}}, or you can calculate \textsuperscript{\cite{Barfoot2016, Green1952}} like this:
\begin{equation}
\bm{R} \leftarrow {\left( {\bm{R}{\bm{R}^\mathrm{T}}} \right)^{ - \frac{1}{2}}} \bm{ R}.
\end{equation}
This is equivalent to re-projecting the result from the matrix space onto the $\mathrm{SE}(3)$ manifold and converting it into two parts, rotation and translation.

It should be explained that our $\bm{x}_1$ uses the normalized plane coordinates and removes the influence of the internal parameter matrix $\bm{K}$ - this is because the internal parameter $\bm{K}$ It is usually assumed to be known in SLAM. Even if the internal parameters are unknown, PnP can be used to estimate $\bm{K}, \bm{R}, \bm{t}$ three quantities. However, due to the increased amount of unknowns, the effect will be worse.

\subsection{P3P}
The P3P described below is another way to solve PnP. It uses only 3 pairs of matching points and requires less data, so it's also briefly introduced here (this part of the derivation draws on the literature \cite{web:p3p}).

P3P needs to utilize the geometric relationship of a given 3 points. Its input data is 3 pairs of 3D−2D matching points. The 3D points are $A, B, C$, and the 2D points are $a, b, c$, where the lowercase letters represent the points corresponding to the uppercase letters on the camera's imaging plane, such as \autoref{fig: P3p}~ is shown. In addition, P3P also needs to use a pair of verification points to select the correct one from the possible solutions (similar to the polar geometry case). The verification point pair is $D-d$ and the camera's optical center is $O$. Note that we know that $A, B, and C$ are in \textbf{coordinates in the world coordinate system}, not \textbf{coordinates in the camera coordinate system}. Once the coordinates of the 3D point in the camera coordinate system can be calculated, we get the corresponding point of 3D−3D and convert the PnP problem into an ICP problem.
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.54\linewidth]{vo1/p3p.pdf}
	\caption{P3P问题示意图。}
	\label{fig:p3p}
\end{figure}

First of all, it is obvious that there is a correspondence between the triangles:
\begin{equation}
\Delta Oab - \Delta OAB, \quad \Delta Obc - \Delta OBC, \quad \Delta Oac - \Delta OAC.
\end{equation}

Consider the relationship between $Oab$ and $OAB$. Using the cosine theorem, there are:
\begin{equation}
O{A^2} + O{B^2} - 2OA \cdot OB \cdot \cos \left\langle a,b \right \rangle = A{B^2}.
\end{equation}

For the other two triangles, there are similar properties, so there are:
\begin{equation}
\begin{array}{l}
O{A^2} + O{B^2} - 2OA \cdot OB \cdot \cos \left\langle a,b \right \rangle = A{B^2}\\
O{B^2} + O{C^2} - 2OB \cdot OC \cdot \cos \left\langle b,c \right \rangle = B{C^2}\\
O{A^2} + O{C^2} - 2OA \cdot OC \cdot \cos \left\langle a,c \right \rangle = A{C^2}.
\end{array}
\end{equation}

Divide all of the above three formulas by $OC^2$, and record $x=OA/OC, y=OB/OC$, and get:
\begin{equation}
\begin{array}{l}
{x^2} + {y^2} - 2xy\cos \left\langle a,b \right \rangle = A{B^2}/O{C^2}\\
{y^2} + {1^2} - 2y\cos \left\langle b,c \right \rangle = B{C^2}/O{C^2}\\
{x^2} + {1^2} - 2x\cos \left\langle a,c \right \rangle = A{C^2}/O{C^2}.
\end{array}
\end{equation}

Remember $v = AB^2/OC^2, uv = BC^2/OC^2, wv = AC^2/OC^2$, with:
\begin{equation}
\begin{array}{l}
{x^2} + {y^2} - 2xy\cos \left\langle a,b \right \rangle - v = 0\\
{y^2} + {1^2} - 2y\cos \left\langle b,c \right \rangle - uv = 0\\
{x^2} + {1^2} - 2x\cos \left\langle a,c \right \rangle - wv = 0.
\end{array}
\end{equation}

We can put $v$ in the first expression on the side of the equation and substitute the following two equations to get:
\begin{equation}
\begin{array}{l}
\left( {1 - u} \right){y^2} - u{x^2} - \cos \left\langle b,c \right \rangle y + 2uxy\cos \left\langle a,b \right \rangle + 1 = 0 \\
\left( {1 - w} \right){x^2} - w{y^2} - \cos \left\langle a,c \right \rangle x + 2wxy\cos \left\langle a,b \right \rangle + 1 = 0.
\end{array}
\end{equation}

Note the known and unknown quantities in these equations. Since we know the image position of the 2D point, the 3 cosine angles $\cos \left \langle a,b \right \rangle$, $\cos \left\langle b,c \right \rangle$, $\cos \left \langle a,c \right \rangle$ is known. At the same time, $u=BC^2/AB^2, w=AC^2/AB^2$ can be calculated by the coordinates of $A, B, C$ in the world coordinate system, after changing to the camera coordinate system, this The ratio does not change. The $x,y$ in this formula is unknown and will change as the camera moves. Therefore, the system of equations is a binary quadratic equation (polynomial equation) for $x,y$. Solving the equations analytically is a complex process that requires the elimination of the Wu method. The introduction to the solution to this equation is not extended here. For interested readers, please refer to the literature \cite{GaoHouTangEtAl2003}. Similar to the case of breaking $\bm{E}$, the equation may get up to 4 solutions, but we can use the verification point to calculate the most probable solution, and get 3D of $A, B, C$ in the camera coordinate system. coordinate. Then, based on the point pair of 3D−3D, calculate the camera's motion $\bm{R}, \bm{t}$. This section will be introduced in Section 7.9.

As can be seen from the principle of P3P, in order to solve PnP, we use the similarity of triangles to solve the 3D coordinates of projection points $a, b, c$ in the camera coordinate system, and finally convert the problem into a 3D to 3D pose. Estimate the problem. As will be seen later, the 3D−3D pose solution with matching information is very easy to solve, so this idea is very effective. Other methods, such as EPnP, also use this approach. However, P3P also has some problems:

\begin{enumerate}
\item P3P only uses 3 points of information. When there are more than 3 groups of given points, it is difficult to use more information.
\item If the 3D point or 2D point is affected by noise, or if there is a mismatch, the algorithm fails.
\end{enumerate}

So follow-up people have proposed many other methods, such as EPnP, UPnP and so on. They use more information and iteratively optimizes camera poses to eliminate the effects of noise as much as possible. However, the principle is more complicated than P3P, so we recommend that readers read the original paper or understand the PnP process through practice. In SLAM, the usual practice is to estimate the camera pose using methods such as P3P/EPnP, and then construct a least squares optimization problem to adjust the estimate (Bundle Adjustment). When the camera motion is continuous enough, it can also be assumed that the camera does not move or move at a constant speed, and the estimated value is used as an initial value for optimization. Next we look at the PnP problem from the perspective of nonlinear optimization.

\subsection{Minimize reprojection error solver PnP}
\label{sec:BA-vo1}
In addition to using linear methods, we can also construct the PnP problem as a nonlinear least squares problem with reprojection errors. This will use the knowledge in the book \ref{cpt:4} and \ref{cpt:5}. The linear method mentioned above is often \textbf{first seek camera pose, then find the position of the space point}, while the nonlinear optimization is to regard them as optimization variables and put them together for optimization. This is a very versatile solution that we can use to optimize the results given by PnP or ICP. This type of \textbf{to minimize the camera and 3D points together} is collectively referred to as Bundle Adjustment, referred to as BA\footnote{It should be noted that BA is in different documents and contexts. The meaning is not exactly the same. Some scholars only refer to the problem of minimizing the reprojection error as BA, while others have a broader BA concept. Even if the BA has only one camera or other similar sensors, it can be called BA. I personally prefer a broader BA concept, so the method of calculating PnP here is also called BA. }.

We can build a Bundle Adjustment problem in PnP to optimize the camera pose. If the camera is moving continuously (such as most SLAM processes), you can also use BA to solve the camera pose. We will give the basic form of this problem in two views in this section, and then discuss the larger-scale BA problem in Lecture 9.

Considering $n$3D space point $P$ and its projection $p$, we want to calculate the camera's pose $\bm{R}, \bm{t}$, whose Lie group is represented as $\bm{T}$. Suppose the coordinates of a space point are $\bm{P}_i=[X_i,Y_i,Z_i]^\mathrm{T}$, and the pixel coordinates of the projection are $\bm{u}_i=[u_i,v_i]^\mathrm{T}$. According to the content of \ref{cpt:5}, the relationship between pixel position and spatial point position is as follows:
\begin{equation}
S_i \left[
\begin{array}{l}
U_i \\ v_i \\ 1
\end{array}
\right] = \bm{K} \bm{T} \left[
\begin{array}{l}
X_i \\ Y_i \\ Z_i \\ 1
\end{array} \right] .
\end{equation}
Written in matrix form is:
\[
{{s_i {\bm{u}}_i} = \bm{K} \bm{T} \bm{P}}_i.
\]

This expression implies a conversion from homogeneous coordinates to non-homogeneous, otherwise the dimensions are wrong by the multiplication of the matrix. \footnote{ $ \bm{T} {\bm{P}_i}$ $4 \times 1$, and the $\bm{K}$ on the left is $3 \times 3$, so you must take the first 3D of $\bm{T}\bm{P}_i$ In three-dimensional non-homogeneous coordinates. Or, use $\bm{R}\bm{P}+\bm{t}$. }. Now, there is an error in the equation due to the unknown camera pose and the noise at the observation point. Therefore, we sum the errors, build a least squares problem, and then find the best camera pose to minimize it:
\begin{equation}
{\bm{T}^*} = \arg \mathop {\min }\limits_{\bm{T}} \frac{1}{2}\sum\limits_{i = 1}^n {\left\| {{{\bm{u}}_i} - \frac{1}{s_i} \bm{K}\bm{T}{\bm{P}}_i} \right\|_2^2} .
\end{equation}

The error term of this problem is that the projection position of the 3D point is different from the observation position, so it is called \textbf{reprojection error}. When using homogeneous coordinates, this error has 3 dimensions. However, since the last dimension of ${\bm{u}}$ is 1, the error of this dimension is always zero, so we use non-homogeneous coordinates more often, so the error is only 2 dimensions. As shown in \autoref{fig:reprojection}~, we know through feature matching that $p_1$ and $p_2$ are projections of the same space point $P$, but I don't know the pose of the camera. In the initial value, there is a certain distance between the $P$ projection $\hat{p}_2$ and the actual $p_2$. So we adjust the pose of the camera to make this distance smaller. However, since this adjustment requires consideration of many points, the final effect is a reduction in the overall error, and the error at each point is usually not exactly zero.

\begin{figure}[!htp]
\centering
\includegraphics[width=0.8\linewidth]{vo1/reprojection}
\caption{Reprojection error diagram. }
\label{fig:reprojection}
\end{figure}

The least squares optimization problem has been introduced in the \ref{cpt:6}. Using Lie algebra, it is possible to construct an unconstrained optimization problem, which is conveniently solved by an optimization algorithm such as Gauss-Newton method and Levinberg-Marquart method. However, before using the Gauss-Newton method and the Levenberg-Marquart method, we need to know the derivative of each error term with respect to the optimization variable, ie \textbf{linearization}:
\begin{equation}
\bm{e}( \bm{x} + \Delta \bm{x} ) \approx \bm{e}(\bm{x}) + \bm{J} ^\mathrm{T}\Delta \bm {x}.
\end{equation}

The form of $\bm{J}^\mathrm{T}$ is worth discussing, and it can even be said to be the key. We can certainly use numerical derivatives, but if we can derive the analytical form, we will give priority to the analytical derivative. Now, when $\bm{e}$ is the pixel coordinate error (2 dimensions) and $\bm{x}$ is the camera pose (6 dimensions), $\bm{J}^\mathrm{T}$ will Is a matrix of $2 \times 6$. Let us derive the form of $\bm{J}^\mathrm{T}$.

Recalling the contents of Lie algebra, we introduced how to use the perturbation model to find the derivative of Lie algebra. First, the coordinates of the space point transformed into the camera coordinate system are $\bm{P}'$, and the first 3 dimensions are taken out:
\begin{equation}
\bm{P}' = \left( \bm{T}{\bm{P}} \right)_{1:3}= [X', Y', Z']^\mathrm{T}.
\end{equation}

Then, the camera projection model is relative to $\bm{P}'$
\begin{equation}
s {\bm{u}} = \bm{K} \bm{P}'.
\end{equation}

Expand:
\begin{equation}
\left[ \begin{array}{l}
Su\\
Sv\\
s
\end{array} \right] = \left[ {\begin{array}{*{20}{c}}
{{f_x}}&0&{{c_x}}\\
0&{{f_y}}&{{c_y}}\\
0&0&1
\end{array}} \right]\left[ \begin{array}{l}
X'\\
Y'\\
Z'
\end{array} \right].
\end{equation}

Using line 3 to eliminate $s$ (actually the distance of $\bm{P}'$), you get:
\begin{equation}
\label{eq:uv2xyz}
u = {f_x}\frac{{X'}}{{Z'}} + {c_x}, \quad v = {f_y}\frac{{Y'}}{{Z'}} + {c_y}.
\end{equation}

This is consistent with the camera model in \ref{cpt:5}. When we ask for the error, we can compare the $u,v$ here with the actual measured value and find the difference. After defining the intermediate variable, we multiply the $\bm{T}$ left perturbation $\delta \boldsymbol{\xi}$ and then consider the variation of $\bm{e}$ on the derivative of the disturbance. Using the chain rule, you can write the following:
\begin{equation}
\frac{{\partial \bm{e}}}{{\partial \delta \boldsymbol{\xi} }} = \mathop {\lim }\limits_{\delta \boldsymbol{\xi} \to 0} \ Frac{{\bm{e}\left( {\delta \boldsymbol{\xi} \oplus \boldsymbol{\xi} } \right)-\bm{e}(\boldsymbol{\xi})}}{{ \delta \boldsymbol{\xi} }} = \frac{{\partial \bm{e}}}{{\partial \bm{P}'}}\frac{{\partial \bm{P}'}} {{\partial \delta \boldsymbol{\xi} }}.
\end{equation}

Here $\oplus$ refers to the left-handed perturbation on the Lie algebra. The first term is the derivative of the error about the projection point. The relationship between the variables is already listed in the formula \eqref{eq:uv2xyz}, which is easy to get:
\begin{equation}
\frac{{\partial \bm{e}}}{{\partial \bm{P}'}} = -\left[
{\begin{array}{*{20}{c}}
{\frac{{\partial u}}{{\partial X'}}}&{\frac{{\partial u}}{{\partial Y'}}}&{\frac{{\partial u}} {{\partial Z'}}}\\
{\frac{{\partial v}}{{\partial X'}}}&{\frac{{\partial v}}{{\partial Y'}}}&{\frac{{\partial v}} {{\partial Z'}}}
\end{array}} \right]
= - \left[ {\begin{array}{*{20}{c}}
{\frac{{{f_x}}}{Z'}}&0&{ - \frac{{{f_x}X'}}{{{Z'^2}}}}\\
0&{\frac{{{f_y}}}{Z'}}&{ - \frac{{{f_y}Y'}}{Z'^2}}
\end{array}} \right].
\end{equation}

The second term is the derivative of the transformed point on the Lie algebra. According to the derivation in the \ref{sec:se3-diff} section, we get:
\begin{equation}
\frac{{\partial \left( \bm{TP} \right)}}{{\partial \delta \boldsymbol{\xi} }} = {\left( \bm{TP} \right)^ \odot } = \left[
\begin{array}{*{20}{cc}}
\bm{I} &- \bm{P}'^ \wedge \\
\bm{0}^\mathrm{T} &\bm{0}^\mathrm{T}
\end{array}
\right].
\end{equation}

In the definition of $\bm{P}'$, we took out the first 3 dimensions and got:
\begin{equation}
\frac{{\partial \bm{P}'}}{{\partial \delta \boldsymbol{\xi} }} = \left[ { \bm{I}, - {\bm{P}'^ \wedge }} \right].
\end{equation}
将这两项相乘，就得到了$2 \times 6$的雅可比矩阵：
\begin{equation}
\label{eq:jacob-uv2xi}
\frac{{\partial \bm{e}}}{{\partial \delta \boldsymbol{\xi} }} = - \left[ {\begin{array}{*{20}{c}}
	{\frac{{{f_x}}}{Z'}}&0&{ - \frac{{{f_x}X'}}{{{Z'^2}}}}&{ - \frac{{{f_x}X'Y'}}{{{Z'^2}}}}&{{f_x} + \frac{{{f_x}{X'^2}}}{{{Z'^2}}}}&{ - \frac{{{f_x}Y'}}{Z'}}\\
	0&{\frac{{{f_y}}}{Z'}}&{ - \frac{{{f_y}Y'}}{{{Z'^2}}}}&{ - {f_y} - \frac{{{f_y}{Y'^2}}}{{{Z'^2}}}}&{\frac{{{f_y}X'Y'}}{{{Z'^2}}}}&{\frac{{{f_y}X'}}{Z'}}
	\end{array}} \right].
\end{equation}

This Jacobian matrix describes the first-order variation of the reprojection error with respect to the camera pose Lie algebra. We reserved the previous negative sign because the error is defined by \textbf{observation minus the predicted value}. It can of course also be reversed, defined as the form of "predicted value minus observed value". In that case, just remove the minus sign in front. In addition, if $\mathfrak{se}(3)$ is defined by the rotation first and the translation after, just swap the first 3 columns and the last 3 columns of the matrix.

On the other hand, in addition to optimizing the pose, we also want to optimize the spatial position of the feature points. Therefore, you need to discuss the derivative of $\bm{e}$ about the space point $\bm{P}$. Fortunately, this derivative matrix is ​​relatively easy. Still using the chain rule, there are:
\begin{equation}
\frac{{\partial \bm{e}}}{{\partial \bm{P} }} = \frac{{\partial \bm{e}}}{{\partial \bm{P}'}} \frac{{\partial \bm{P}'}}{{\partial \bm{P} }}.
\end{equation}

The first item has been derived before, with regard to the second item, by definition
\[
\bm{P}'= (\bm{T} \bm{P})_{1:3} = \bm{R} \bm{P} + \bm{t},
\]
We found that $\bm{P}'$ will only leave $\bm{R}$ after $\bm{P}$. then:
\begin{equation}
\label{eq:jacob-uv2P}
\frac{{\partial \bm{e}}}{{\partial \bm{P} }} = -\left[
\begin{array}{*{20}{c}}
\frac{f_x}{Z'} & 0 &- \frac{f_x X'}{Z'^2} \\
0 & \frac{f_y}{Z'} & - \frac{f_y Y'}{Z'^2}
\end{array} \right] \bm{R}.
\end{equation}

Thus, we derive the two derivative matrices of the camera equation for camera pose and feature points. They are \textbf{important}, which provides important gradient directions during the optimization process and guides the iteration of optimization.

\section{Practice: Solving PnP}
\subsection{Using EPnP to solve poses}
Below, we understand the process of PnP through experiments. First, we demonstrate how to solve the PnP problem using OpenCV's EPnP and then solve it again through nonlinear optimization. In the second edition, we will add a handwritten optimization experiment. Since PnP needs to use 3D points, in order to avoid the trouble caused by initialization, we used the depth map (1_depth.png) in the RGB-D camera as the 3D position of the feature point. First look at the PnP functions provided by OpenCV:

\begin{lstlisting}[language=c++,caption=slambook2/ch7/pose_estimation_3d2d.cpp(fragment)]
Int main( int argc, char** argv ) {
    Mat r, t;
   solvePnP(pts_3d, pts_2d, K, Mat(), r, t, false); // Call OpenCV's PnP solution, select EPNP, DLS, etc.
   Mat R;
   Cv::Rodrigues(r, R); // r is a rotation vector form, converted to a matrix using the Rodrigues formula
   Cout << "R=" << endl << R << endl;
   Cout << "t=" << endl << t << endl;
}
\end{lstlisting}

In the routine, after getting the paired feature points, we look for their depth in the depth map of the first graph and find the spatial position. The spatial position is 3D point, and then the pixel position of the second image is 2D point, and EPnP is called to solve the PnP problem. The program output is as follows:

\begin{lstlisting}[language=sh,caption=terminal input:]
% build/pose_estimation_3d2d 1.png 2.png d1.png d2.png
-- Max dist : 95.000000
-- Min dist : 4.000000
A total of 79 matching points were found.
3d-2d pairs: 76
R=
[0.9978662025826269, -0.05167241613316376, 0.03991244360207524;
0.0505958915956335, 0.998339762771668, 0.02752769192381471;
-0.04126860182960625, -0.025449547736074, 0.998823919929363]
t=
[-0.1272259656955879;
-0.007507297652615337;
0.06138584177157709]
\end{lstlisting}

The reader can compare the $\bm{R}, \bm{t}$ solved in the previous 2D−2D case to see what is different. It can be seen that when there is 3D information, the estimated $\bm{R}$ is almost the same, and $\bm{t}$ is much different. This is due to the introduction of new depth information. However, since the depth map acquired by Kinect has some errors, the 3D points here are not accurate. In larger BAs, we would like to optimize both pose and all 3D feature points.


\subsection{Handwritten pose estimation}
The following shows how to calculate the camera pose using nonlinear optimization. We first write a PnP of Gaussian Newton method, and then demonstrate how to call g2o to solve.
\begin{lstlisting}[language=c++,caption=slambook2/ch7/pose_estimation_3d2d.cpp（片段）]
void bundleAdjustmentGaussNewton(
const VecVector3d &points_3d,
const VecVector2d &points_2d,
const Mat &K,
Sophus::SE3d &pose) {
	typedef Eigen::Matrix<double, 6, 1> Vector6d;
	const int iterations = 10;
	double cost = 0, lastCost = 0;
	double fx = K.at<double>(0, 0);
	double fy = K.at<double>(1, 1);
	double cx = K.at<double>(0, 2);
	double cy = K.at<double>(1, 2);
	
	for (int iter = 0; iter < iterations; iter++) {
		Eigen::Matrix<double, 6, 6> H = Eigen::Matrix<double, 6, 6>::Zero();
		Vector6d b = Vector6d::Zero();
		
		cost = 0;
		// compute cost
		for (int i = 0; i < points_3d.size(); i++) {
			Eigen::Vector3d pc = pose * points_3d[i];
			double inv_z = 1.0 / pc[2];
			double inv_z2 = inv_z * inv_z;
			Eigen::Vector2d proj(fx * pc[0] / pc[2] + cx, fy * pc[1] / pc[2] + cy);
			Eigen::Vector2d e = points_2d[i] - proj;
			cost += e.squaredNorm();
			Eigen::Matrix<double, 2, 6> J;
			J << -fx * inv_z,
			0,
			fx * pc[0] * inv_z2,
			fx * pc[0] * pc[1] * inv_z2,
			-fx - fx * pc[0] * pc[0] * inv_z2,
			fx * pc[1] * inv_z,
			0,
			-fy * inv_z,
			fy * pc[1] * inv_z,
			fy + fy * pc[1] * pc[1] * inv_z2,
			-fy * pc[0] * pc[1] * inv_z2,
			-fy * pc[0] * inv_z;
			
			H += J.transpose() * J;
			b += -J.transpose() * e;
		}
		
		Vector6d dx;
		dx = H.ldlt().solve(b);
		
		if (isnan(dx[0])) {
			cout << "result is nan!" << endl;
			break;
		}
		
		if (iter > 0 && cost >= lastCost) {
			// cost increase, update is not good
			cout << "cost: " << cost << ", last cost: " << lastCost << endl;
			break;
		}
		
		// update your estimation
		pose = Sophus::SE3d::exp(dx) * pose;
		lastCost = cost;
		
		cout << "iteration " << iter << " cost=" << cout.precision(12) << cost << endl;
		if (dx.norm() < 1e-6) {
			// converge
			break;
		}
	}
	
	cout << "pose by g-n: \n" << pose.matrix() << endl;
}
\end{lstlisting}

In this small function, we implement a simple Gauss-Newton iterative optimization based on the previous theoretical derivation. We will then compare the efficiency differences between OpenCV, handwriting implementations, and g2o implementations.

\subsection{BA optimization with g2o}
After handwriting the optimization process, let's look at how to implement the same operation with g2o (in fact, using Ceres is completely similar). The basics of g2o have already been introduced in the \ref{cpt:6} talk. Before using g2o, we need to model the problem as a graph optimization problem, as shown by \autoref{fig:ba-graph}~.

\begin{figure}[!htp]
\centering
\includegraphics[width=0.9\linewidth]{vo1/ba-graph}
\caption{PnP's Bundle Adjustment's graph optimization representation. }
\label{fig:ba-graph}
\end{figure}

In this graph optimization, the nodes and edges are selected as follows:
\begin{enumerate}
\item \textbf{node}: The pose node of the second camera $\bm{T} \in \mathrm{SE}(3)$.
\item \textbf{edge}: The projection of each 3D point in the second camera, described by the observation equation:
\[
\bm{z}_j = h(\bm{T}, \bm{P}_{j}).
\]
\end{enumerate}

Since the first camera pose is fixed at zero, we didn't write it into the optimization variable, but on more occasions, we will consider more camera estimates. Now we estimate the pose of the second camera based on a set of 3D points and a 2D projection in the second image. So we draw the first camera as a dotted line, indicating that we don't want to consider it.

G2o provides a number of nodes and edges for BA, such as g2o/\\types/sba/types\_six\_dof\_expmap.h which provides nodes and edges for Lie algebra expressions. In the second edition, we implement a VertexPose vertex and an EdgeProjection edge ourselves, as follows:
\begin{lstlisting}[language=c++,caption=slambook2/ch7/pose_estimation_3d2d.cpp（片段）]
/// vertex and edges used in g2o ba
class VertexPose : public g2o::BaseVertex<6, Sophus::SE3d> {
	public:
	EIGEN_MAKE_ALIGNED_OPERATOR_NEW;
	
	virtual void setToOriginImpl() override {
		_estimate = Sophus::SE3d();
	}
	
	/// left multiplication on SE3
	virtual void oplusImpl(const double *update) override {
		Eigen::Matrix<double, 6, 1> update_eigen;
		update_eigen << update[0], update[1], update[2], update[3], update[4], update[5];
		_estimate = Sophus::SE3d::exp(update_eigen) * _estimate;
	}
	
	virtual bool read(istream &in) override {}
	
	virtual bool write(ostream &out) const override {}
};

class EdgeProjection : public g2o::BaseUnaryEdge<2, Eigen::Vector2d, VertexPose> {
	public:
	EIGEN_MAKE_ALIGNED_OPERATOR_NEW;
	
	EdgeProjection(const Eigen::Vector3d &pos, const Eigen::Matrix3d &K) : _pos3d(pos), _K(K) {}
	
	virtual void computeError() override {
		const VertexPose *v = static_cast<VertexPose *> (_vertices[0]);
		Sophus::SE3d T = v->estimate();
		Eigen::Vector3d pos_pixel = _K * (T * _pos3d);
		pos_pixel /= pos_pixel[2];
		_error = _measurement - pos_pixel.head<2>();
	}
	
	virtual void linearizeOplus() override {
		const VertexPose *v = static_cast<VertexPose *> (_vertices[0]);
		Sophus::SE3d T = v->estimate();
		Eigen::Vector3d pos_cam = T * _pos3d;
		double fx = _K(0, 0);
		double fy = _K(1, 1);
		double cx = _K(0, 2);
		double cy = _K(1, 2);
		double X = pos_cam[0];
		double Y = pos_cam[1];
		double Z = pos_cam[2];
		double Z2 = Z * Z;
		_jacobianOplusXi
		<< -fx / Z, 0, fx * X / Z2, fx * X * Y / Z2, -fx - fx * X * X / Z2, fx * Y / Z,
		0, -fy / Z, fy * Y / (Z * Z), fy + fy * Y * Y / Z2, -fy * X * Y / Z2, -fy * X / Z;
	}
	
	virtual bool read(istream &in) override {}
	
	virtual bool write(ostream &out) const override {}
	
	private:
	Eigen::Vector3d _pos3d;
	Eigen::Matrix3d _K;
};
\end{lstlisting}

Here, the update of the vertex and the error calculation of the edge are implemented. Here's how to group them into a graph optimization problem:
\begin{lstlisting}[language=c++,caption=slambook2/ch7/pose_estimation_3d2d.cpp（片段）]
void bundleAdjustmentG2O(
const VecVector3d &points_3d,
const VecVector2d &points_2d,
const Mat &K,
Sophus::SE3d &pose) {
	// 构建图优化，先设定g2o
	typedef g2o::BlockSolver<g2o::BlockSolverTraits<6, 3>> BlockSolverType;  // pose is 6, landmark is 3
	typedef g2o::LinearSolverDense<BlockSolverType::PoseMatrixType> LinearSolverType; // 线性求解器类型
	// 梯度下降方法，可以从GN, LM, DogLeg 中选
	auto solver = new g2o::OptimizationAlgorithmsGaussNewton(
		g2o::make_unique<BlockSolverType>(g2o::make_unique<LinearSolverType>()));
	g2o::SparseOptimizer optimizer;     // 图模型
	optimizer.setAlgorithm(solver);   // 设置求解器
	optimizer.setVerbose(true);       // 打开调试输出
	
	// vertex
	VertexPose *vertex_pose = new VertexPose(); // camera vertex_pose
	vertex_pose->setId(0);
	vertex_pose->setEstimate(Sophus::SE3d());
	optimizer.addVertex(vertex_pose);
	
	// K
	Eigen::Matrix3d K_eigen;
	K_eigen <<
	K.at<double>(0, 0), K.at<double>(0, 1), K.at<double>(0, 2),
	K.at<double>(1, 0), K.at<double>(1, 1), K.at<double>(1, 2),
	K.at<double>(2, 0), K.at<double>(2, 1), K.at<double>(2, 2);
	
	// edges
	int index = 1;
	for (size_t i = 0; i < points_2d.size(); ++i) {
		auto p2d = points_2d[i];
		auto p3d = points_3d[i];
		EdgeProjection *edge = new EdgeProjection(p3d, K_eigen);
		edge->setId(index);
		edge->setVertex(0, vertex_pose);
		edge->setMeasurement(p2d);
		edge->setInformation(Eigen::Matrix2d::Identity());
		optimizer.addEdge(edge);
		index++;
	}
	
	chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
	optimizer.setVerbose(true);
	optimizer.initializeOptimization();
	optimizer.optimize(10);
	chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
	chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
	cout << "optimization costs time: " << time_used.count() << " seconds." << endl;
	cout << "pose estimated by g2o =\n" << vertex_pose->estimate().matrix() << endl;
	pose = vertex_pose->estimate();
}
\end{lstlisting}

The program is roughly similar to g2o in Lecture 6. We first declare the g2o graph optimizer and configure the optimization solver and gradient descent method. Then place the pose and the spatial point into the graph based on the estimated feature points. Finally, the optimization function is called to solve. Finally, the partial output of the run is as follows:
\begin{lstlisting}[language=sh,caption=终端输入：]
./build/pose_estimation_3d2d 1.png 2.png 1_depth.png 2_depth.png
-- Max dist : 95.000000 
-- Min dist : 4.000000 
一共找到了79组匹配点
3d-2d pairs: 76
solve pnp in opencv cost time: 0.000332991 seconds.
R=
[0.9978662025826269, -0.05167241613316376, 0.03991244360207524;
0.0505958915956335, 0.998339762771668, 0.02752769192381471;
-0.04126860182960625, -0.025449547736074, 0.998823919929363]
t=
[-0.1272259656955879;
-0.007507297652615337;
0.06138584177157709]
calling bundle adjustment by gauss newton
iteration 0 cost=645538.1857253
iteration 1 cost=12750.239874896
iteration 2 cost=12301.774589343
iteration 3 cost=12301.427574651
iteration 4 cost=12301.426806652
pose by g-n: 
0.99786618832  -0.0516873580423    0.039893448423   -0.127218696289
0.0506143671126    0.998340854865   0.0274540224544 -0.00738695798083
-0.0412462852904  -0.0253762590968    0.998826706403   0.0617019263823
0                 0                 0                 1
solve pnp by gauss newton cost time: 0.000159492 seconds.
calling bundle adjustment by g2o
iteration= 0	 chi2= 413.390599	 time= 2.7291e-05	 cumTime= 2.7291e-05	 edges= 76	 schur= 0	 lambda= 79.000412	 levenbergIter= 1
iteration= 1	 chi2= 301.367030	 time= 1.47e-05	 cumTime= 4.1991e-05	 edges= 76	 schur= 0	 lambda= 26.333471	 levenbergIter= 1
iteration= 2	 chi2= 301.365779	 time= 1.7794e-05	 cumTime= 5.9785e-05	 edges= 76	 schur= 0	 lambda= 17.555647	 levenbergIter= 1
iteration= 3	 chi2= 301.365779	 time= 1.4875e-05	 cumTime= 7.466e-05	 edges= 76	 schur= 0	 lambda= 11.703765	 levenbergIter= 1
iteration= 4	 chi2= 301.365779	 time= 1.3132e-05	 cumTime= 8.7792e-05	 edges= 76	 schur= 0	 lambda= 7.802510	 levenbergIter= 1
iteration= 5	 chi2= 301.365779	 time= 2.0379e-05	 cumTime= 0.000108171	 edges= 76	 schur= 0	 lambda= 41.613386	 levenbergIter= 3
iteration= 6	 chi2= 301.365779	 time= 3.4186e-05	 cumTime= 0.000142357	 edges= 76	 schur= 0	 lambda= 2859650082279.672363	 levenbergIter= 8
optimization costs time: 0.000763649 seconds.
pose estimated by g2o =
0.997866202583  -0.0516724161336   0.0399124436024   -0.127225965696
0.050595891596    0.998339762772   0.0275276919261 -0.00750729765631
-0.04126860183  -0.0254495477384    0.998823919929   0.0613858417711
0                 0                 0                 1
solve pnp by g2o cost time: 0.000923095 seconds.
\end{lstlisting}

From the estimation results, the three are basically the same. From the optimization time point of view, our own implementation of the Gauss Newton method ranked first with 0.15 milliseconds, followed by OpenCV's PnP, and finally g2o implementation. Despite this, the three uses less than 1 millisecond, which means that the pose estimation algorithm does not consume computation.

Bundle Adjustment is a common practice. It can be not limited to two images. We can iteratively optimize the poses and spatial points matched by multiple images, and even put the entire SLAM process into it. That kind of approach is large, mainly used in the back end, we will encounter this problem again in the 10th lecture. At the front end, we usually consider the small Bundle Adjustment problem of the local camera pose and feature points, and hope to solve and optimize it in real time.

\section{3D−3D:ICP}
Finally, let's introduce the pose estimation problem of 3D−3D. Suppose we have a set of paired 3D points (for example, we matched two RGB-D images):
\[
\bm{P} = \{ \bm{p}_1, \cdots, \bm{p}_n \}, \quad \bm{P}' = \{ \bm{p}_1', \cdots, \ Bm{p}_n'\},
\]
Now, I want to find an Euclidean transformation of $\bm{R}, \bm{t}$, which makes the \footnote{ example slightly different from the symbols in the first two chapters. If you associate them, then look at $\bm{p}_i$ as the data in the second image and $\bm{p}_i'$ as the data in the first image. $\bm{R}, \bm{t}$ is consistent. }:
\[
\forall i, \bm{p}_i = \bm{R} \bm{p}_i' + \bm{t}.
\]

This problem can be solved with the Iterative Closest Point (ICP). The reader should note that the camera model does not appear in the 3D−3D pose estimation problem, that is, it only has no relationship with the camera when considering the transition between the two sets of 3D points. Therefore, ICP is also encountered in the laser SLAM, but because the laser data features are not rich enough, we have no way to know the \textbf{matching relationship} between the two point sets, we can only think that the two closest points are the same. So this method is called the iterative closest point. In the visual, the feature points provide us with a good matching relationship, so the whole problem becomes simpler. In RGB-D SLAM, the camera pose can be estimated in this way. Below we use ICP to refer to the motion estimation problem between the two sets of \textbf{matched} points.

Similar to PnP, ICP is solved in two ways: by linear algebra (mainly SVD) and by nonlinear optimization (similar to Bundle Adjustment). The following is introduced separately.

\subsection{SVD method}
First look at the algebraic method represented by SVD. Based on the ICP problem described earlier, we first define the error term for the $i$ pair:
\begin{equation}
\bm{e}_i = \bm{p}_i - (\bm{R} \bm{p}_i' + \bm{t} ) .
\end{equation}

Then, build a least squares problem and find the sum of the squared errors to a minimum of $\bm{R}, \bm{t}$:
\begin{equation}
\mathop {\min }\limits_{\bm{R}, \bm{t}} \frac{1}{2} \sum\limits_{i = 1}^n\| {\left( {{\bm {p}_i} - \left( {\bm{R}{\bm{p}_i}' + \bm{t}} \right)} \right)} \|^2_2.
\end{equation}

Let's deduce its solution. First, define the centroids of the two sets of points:
\begin{equation}
\bm{p} = \frac{1}{n}\sum_{i=1}^n ( \bm{p}_i ), \quad \bm{p}' = \frac{1}{n} \sum_{i=1}^n ( \bm{p}_i' ).
\end{equation}

Please note that the centroid is not subscripted. Then, do the following processing in the error function:
\begin{align*}
\begin{array}{ll}
\frac{1}{2}\sum\limits_{i = 1}^n {{{\left\| {{\bm{p}_i} - \left( {\bm{R}{ \bm{p}_i}' + \bm{t}} \right)} \right\|}^2}}  & = \frac{1}{2}\sum\limits_{i = 1}^n {{{\left\| {{\bm{p}_i} - \bm{R}{\bm{p}_i}' - \bm{t} - \bm{p} + \bm{Rp}' + \bm{p} - \bm{Rp}'} \right\|}^2}} \\
 & = \frac{1}{2}\sum\limits_{i = 1}^n {{{\left\| {\left( {{\bm{p}_i} - \bm{p} - \bm{R}\left( {{\bm{p}_i}' - \bm{p}'} \right)} \right) + \left( {\bm{p} - \bm{Rp}' - \bm{t}} \right)} \right\|}^2}} \\
& = \frac{1}{2}\sum\limits_{i = 1}^n ( {{\left\| {{\bm{p}_i} - \bm{p} - \bm{R}\left( {{\bm{p}_i}' - \bm{p}'} \right)} \right\|}^2} + {{\left\| {\bm{p} - \bm{Rp}' - \bm{t}} \right\|}^2} +\\
& \quad \quad 2{{\left( {{\bm{p}_i} - \bm{p} - \bm{R}\left( {{\bm{p}_i}' - \bm{p}'} \right)} \right)}^T}\left( {\bm{p} - \bm{Rp}' - \bm{t}} \right)).
\end{array}
\end{align*}

Notice the $\left( {{\bm{p}_i} - \bm{p} - \bm{R}\left( {{\bm{p}_i}' - \bm{p} in the cross section '} \right)} \right)$ is zero after summation, so the optimization objective function can be simplified to
\begin{equation}
\mathop {\min }\limits_{\bm{R}, \bm{t}} J = \frac{1}{2}\sum\limits_{i = 1}^n {{\left\| {{ \bm{p}_i} - \bm{p} - \bm{R}\left( {{\bm{p}_i}' - \bm{p}'} \right)} \right\|}^ 2} + {{\left\| {\bm{p} - \bm{Rp}' - \bm{t}} \right\|}^2} .
\end{equation}

Looking closely at the left and right, we find that the left side is only related to the rotation matrix $\bm{R}$, while the right side has both $\bm{R}$ and $\bm{t}$, but only with centroid. As long as we get $\bm{R}$, we can get $\bm{t}$ if the second item is zero. Therefore, ICP can be solved in the following three steps:

\begin{mdframed}
\begin{enumerate}
\item calculates the centroid positions $\bm{p}, \bm{p}'$ for the two sets of points, and then calculates the \textbf{decent centroid coordinates} for each point:
\[
\bm{q}_i = \bm{p}_i - \bm{p}, \quad \bm{q}_i' = \bm{p}_i' - \bm{p}'.
\]
\item calculates the rotation matrix based on the following optimization problems:
\begin{equation}
\bm{R}^* = \arg \mathop {\min }\limits_{\bm{R}} \frac{1}{2}\sum\limits_{i = 1}^n {{\left\| {{\bm{q}_i} - \bm{R} \bm{q}_i' } \right\|}^2}.
\end{equation}
\item calculates $\bm{t}$ from $\bm{R}$ in step 2:
\begin{equation}
\label{eq:pnp-solve-t}
\bm{t}^* = \bm{p} - \bm{R} \bm{p}'.
\end{equation}
\end{enumerate}
\end{mdframed}
	
We see that as long as the rotation between the two sets of points is found, the amount of translation is very easy to obtain. So we focus on the calculation of $\bm{R}$. Expand the error term for $\bm{R}$ and get:
\begin{equation}
 \frac{1}{2}\sum\limits_{i = 1}^n \left\| {{\bm{q}_i} - \bm{R} \bm{q}_i' } \right\| ^2 = \frac{1}{2}\sum\limits_{i = 1}^n \bm{q}_i^\mathrm{T} \bm{q}_i + \bm{q}_i^{ \ Prime \mathrm{T}} \bm{R}^\mathrm{T} \bm{R} \bm{q}^\prime_i - 2\bm{q}_i^\mathrm{T} \bm{R} \bm{q}^\prime_i.
\end{equation}

Notice that the first item is not related to $\bm{R}$, and the second item is due to $\bm{R}^\mathrm{T}\bm{R}=\bm{I}$, also with $\bm{ R}$ has nothing to do. Therefore, in fact, the optimization objective function becomes
\begin{equation}
\sum\limits_{i = 1}^n - \bm{q}_i^\mathrm{T} \bm{R} \bm{q}^\prime_i = \sum\limits_{i = 1}^n - \mathrm{tr} \left( \bm{R} \bm{q}_i^{\prime} \bm{q}^{\mathrm{T}}_i \right) = - \mathrm{tr} \left ( \bm{R} \sum\limits_{i = 1}^n \bm{q}_i^{\prime} \bm{q}^{\mathrm{T}}_i \ \right).
\end{equation}

Next, we describe how to solve the best $\bm{R}$ of the above problem through SVD. Proof of optimality is more complicated, and interested readers can refer to the literature \cite{Arun1987, PomerleauColasSiegwart2015}. To understand $\bm{R}$, first define the matrix:
\begin{equation}
\bm{W} = \sum\limits_{i = 1}^n \bm{q}_i \bm{q}^{\prime \mathrm{T}}_i.
\end{equation}

$\bm{W}$ is a matrix of $3 \times 3$, SVD decomposition of $\bm{W}$, which gives:
\begin{equation}
\bm{W} = \bm{U \Sigma V}^\mathrm{T}.
\end{equation}

Among them, $\bm{\ Sigma}$ is a diagonal matrix composed of singular values, diagonal elements are arranged from large to small, and $\bm{U}$ and $\bm{V}$ are diagonal matrices. When $\bm{W}$ is full, $\bm{R}$ is
\begin{equation}
\bm{R} = \bm{U} \bm{V}^\mathrm{T}.
\end{equation}

After $\bm{R}$ is solved, $\bm{t}$ can be solved by \eqref{eq:pnp-solve-t}. If the determinant of $\bm{R}$ is negative at this time, then $-\bm{R}$ is taken as the optimal value.

\subsection{Nonlinear optimization method}
Another way to solve ICP is to use nonlinear optimization to find the optimal value in an iterative manner. This method is very similar to the PnP we described earlier. When expressing a pose with Lie algebra, the objective function can be written as
\begin{equation}
\mathop {\min }\limits_{\boldsymbol{\xi}} = \frac{1}{2} \sum\limits_{i = 1}^n\| {\left( {{{\bm{p} }_i} - \exp \left( \boldsymbol{\xi}^\wedge \right) {\bm{p}}'_i} \right)} \|^2_2.
\end{equation}

The derivative of a single error term with respect to pose has been derived previously, using Lie algebra to perturb the model:
\begin{equation}
\frac{{\partial \bm{e}}}{{\partial \delta \boldsymbol{\xi} }} = - {\left( {\exp \left( {{ \boldsymbol{\xi} ^ \wedge }} \right){{\bm{p}}_i}'} \right)^ \odot }.
\end{equation}

Thus, in nonlinear optimization, it is only necessary to continuously iterate to find a minimum value. Moreover, it can be proved that \textsuperscript{\cite{Barfoot2016}} has a unique solution or an infinite number of solutions to the ICP problem. In the case of a unique solution, as long as a minimal solution is found, then \textbf{this minimum value is the global optimal value} - so there is no case of local minima rather than global minima. This also means that the ICP solution can arbitrarily select the initial value. This is a big benefit of solving ICP when matching points.

It should be noted that the ICP we are talking about here refers to the problem of pose estimation under the condition that the image features have been matched. In the case where the matching is known, this least squares problem actually has an analytical solution \textsuperscript{\cite{Faugeras1986, Horn1987, Sharp2002}}, so there is no need for iterative optimization. ICP researchers are often more concerned with matching unknown situations. So why do we want to introduce optimization-based ICP? This is because, in some cases, such as in RGB-D SLAM, the depth data of one pixel may or may not be measured, so we can mix and match PnP and ICP optimization: for feature points with known depth, Model their 3D−3D errors; for feature points with unknown depths, model the 3D−2D reprojection error. Thus, all errors can be considered in the same problem, making the solution more convenient.

\section{Practice: Solving ICP}
\subsection{SVD method}
The following demonstrates how to solve ICP using SVD and nonlinear optimization. In this section, we use two RGB-D images to obtain two sets of 3D points by feature matching, and finally calculate their pose transformation by ICP. Since OpenCV does not currently calculate two sets of ICP methods with matching points, and its principle is not complicated, we implement an ICP ourselves.
\begin{lstlisting}[language=c++,caption=slambook2/ch7/pose\_estimation\_3d3d.cpp（片段）]
void pose_estimation_3d3d(
const vector<Point3f> &pts1,
const vector<Point3f> &pts2,
Mat &R, Mat &t) {
	Point3f p1, p2;     // center of mass
	int N = pts1.size();
	for (int i = 0; i < N; i++) {
		p1 += pts1[i];
		p2 += pts2[i];
	}
	p1 = Point3f(Vec3f(p1) / N);
	p2 = Point3f(Vec3f(p2) / N);
	vector<Point3f> q1(N), q2(N); // remove the center
	for (int i = 0; i < N; i++) {
		q1[i] = pts1[i] - p1;
		q2[i] = pts2[i] - p2;
	}
	
	// compute q1*q2^T
	Eigen::Matrix3d W = Eigen::Matrix3d::Zero();
	for (int i = 0; i < N; i++) {
		W += Eigen::Vector3d(q1[i].x, q1[i].y, q1[i].z) * Eigen::Vector3d(q2[i].x, q2[i].y, q2[i].z).transpose();
	}
	cout << "W=" << W << endl;
	
	// SVD on W
	Eigen::JacobiSVD<Eigen::Matrix3d> svd(W, Eigen::ComputeFullU | Eigen::ComputeFullV);
	Eigen::Matrix3d U = svd.matrixU();
	Eigen::Matrix3d V = svd.matrixV();
	
	cout << "U=" << U << endl;
	cout << "V=" << V << endl;
	
	Eigen::Matrix3d R_ = U * (V.transpose());
	if (R_.determinant() < 0) {
		R_ = -R_;
	}
	Eigen::Vector3d t_ = Eigen::Vector3d(p1.x, p1.y, p1.z) - R_ * Eigen::Vector3d(p2.x, p2.y, p2.z);
	
	// convert to cv::Mat
	R = (Mat_<double>(3, 3) <<
		R_(0, 0), R_(0, 1), R_(0, 2),
		R_(1, 0), R_(1, 1), R_(1, 2),
		R_(2, 0), R_(2, 1), R_(2, 2)
	);
	t = (Mat_<double>(3, 1) << t_(0, 0), t_(1, 0), t_(2, 0));
}
\end{lstlisting}

The implementation of ICP is consistent with the previous one. We call Eigen for SVD and then calculate the $\bm{R}, \bm{t}$ matrix. We output the result of the match, but please note that since the previous derivation was done according to $\bm{p}_i = \bm{R} \bm{p}_i' + \bm{t}$, here $\bm{R}, \bm{t}$ is the transformation from the second frame to the first frame, which is the opposite of the previous PnP part. So in the output, we also printed the inverse transform:

\begin{lstlisting}[language=sh,caption=terminal input:]
./build/pose_estimation_3d3d 1.png 2.png 1_depth.png 2_depth.png
-- Max dist : 95.000000
-- Min dist : 4.000000
A total of 79 matching points were found.
3d-3d pairs: 74
W= 11.9404 -0.567258 1.64182
-1.79283 4.31299 -6.57615
3.12791 -6.55815 10.8576
U= 0.474144 -0.880373 -0.0114952
-0.460275 -0.258979 0.849163
0.750556 0.397334 0.528006
V= 0.535211 -0.844064 -0.0332488
-0.434767 -0.309001 0.84587
0.724242 0.438263 0.532352
ICP via SVD results:
R = [0.9972395977366739, 0.05617039856770099, -0.04855997354553433;
-0.05598345194682017, 0.9984181427731508, 0.005202431117423125;
0.0487753812298326, -0.002469515369266572, 0.9988067198811421]
t = [0.1417248739257469;
-0.05551033302525193;
-0.03119093188273858]
R_inv = [0.9972395977366739, -0.05598345194682017, 0.0487753812298326;
0.05617039856770099, 0.9984181427731508, -0.002469515369266572;
-0.04855997354553433, 0.005202431117423125, 0.9988067198811421]
T_inv = [-0.1429199667309695;
0.04738475446275858;
0.03832465717628181]
\end{lstlisting}

The reader can compare the difference between the ICP and PnP, the motion estimation results of the polar geometry. It can be argued that in this process we are using more and more information (no depth - there is a depth of a graph - there are depths of two graphs), so in the case of depth accuracy, the resulting estimates will also come The more accurate. However, due to the noise of Kinect's depth map and the possibility of data loss, we have to discard some feature points without depth data. This may result in an inaccurate estimation of the ICP, and if the feature points are discarded too much, it may cause a situation in which motion estimation cannot be performed due to too few feature points.

\subsection{Nonlinear optimization method}
Let's consider nonlinear optimization to calculate ICP. We still use Lie algebra to optimize the camera pose. For us, the RGB-D camera can observe the three-dimensional position of the landmark points each time, resulting in a 3D observation. We use VertexPose from the previous experiment and then define the unary side of 3D-3D:
\begin{lstlisting}[language=c++,caption=slambook2/ch7/pose\_estimation\_3d3d.cpp]

/// g2o edge
class EdgeProjectXYZRGBDPoseOnly : public g2o::BaseUnaryEdge<3, Eigen::Vector3d, VertexPose> {
	public:
	EIGEN_MAKE_ALIGNED_OPERATOR_NEW;
	
	EdgeProjectXYZRGBDPoseOnly(const Eigen::Vector3d &point) : _point(point) {}
	
	virtual void computeError() override {
		const VertexPose *pose = static_cast<const VertexPose *> ( _vertices[0] );
		_error = _measurement - pose->estimate() * _point;
	}
	
	virtual void linearizeOplus() override {
		VertexPose *pose = static_cast<VertexPose *>(_vertices[0]);
		Sophus::SE3d T = pose->estimate();
		Eigen::Vector3d xyz_trans = T * _point;
		_jacobianOplusXi.block<3, 3>(0, 0) = -Eigen::Matrix3d::Identity();
		_jacobianOplusXi.block<3, 3>(0, 3) = Sophus::SO3d::hat(xyz_trans);
	}
	
	bool read(istream &in) {}
	
	bool write(ostream &out) const {}
	
	protected:
	Eigen::Vector3d _point;
};
\end{lstlisting}

这是一个一元边，写法类似于前面提到的g2o::EdgeSE3ProjectXYZ，不过观测量从2维变成了3维，内部没有相机模型，并且只关联到一个节点。请读者注意这里雅可比矩阵的书写，它必须与我们前面的推导一致。雅可比矩阵给出了关于相机位姿的导数，是一个$3 \times 6$的矩阵。

调用g2o进行优化的代码是相似的，我们设定好图优化的节点和边即可。这部分代码请读者查看源文件，这里不再列出。现在，来看看优化的结果：

\begin{lstlisting}[language=sh, caption=终端输出：]
iteration= 0	 chi2= 1.811539	 time= 1.7046e-05	 cumTime= 1.7046e-05	 edges= 74	 schur= 0
iteration= 1	 chi2= 1.811051	 time= 1.0422e-05	 cumTime= 2.7468e-05	 edges= 74	 schur= 0
iteration= 2	 chi2= 1.811050	 time= 9.589e-06	 cumTime= 3.7057e-05	 edges= 74	 schur= 0
...中间略
iteration= 9	 chi2= 1.811050	 time= 9.113e-06	 cumTime= 0.000100604	 edges= 74	 schur= 0
optimization costs time: 0.000559208 seconds.

after optimization:
T=
0.99724  0.0561704   -0.04856   0.141725
-0.0559834   0.998418 0.00520242 -0.0555103
0.0487754 -0.0024695   0.998807 -0.0311913
0          0          0          1
\end{lstlisting}

We find that the overall error is stable after only one iteration, indicating that the algorithm converges only after one iteration. It can be seen from the result of the pose solution that it is almost identical to the pose result given by the previous SVD, which indicates that the SVD has given an analytical solution to the optimization problem. Therefore, in this experiment, it can be considered that the result given by SVD is the optimal value of the camera pose.

It should be noted that in the ICP of this example, we used feature points with depth readings in both figures. In reality, however, as long as one of the graph depths is determined, we can add them to the optimization using an error approach similar to PnP. At the same time, in addition to the camera pose, considering the space point as an optimization variable is also a way to solve the problem. We should be clear that the actual solution is very flexible and does not have to be stuck in a fixed form. If you consider points and cameras at the same time, the whole problem becomes \textbf{more free}, and you may get other solutions. For example, you can make the camera turn a little less and move the point a bit more. This reflects on the other hand, in the Bundle Adjustment, we would like to have as many constraints as possible, because multiple observations will bring more information, allowing us to estimate each variable more accurately.

\section{小结}
This lecture introduces several important issues in feature-based visual odometers. include:

\begin{enumerate}
\item How the feature points are extracted and matched.
\item How to estimate camera motion through 2D−2D feature points.
\item How to estimate the spatial position of a point from a 2D−2D match.
\item 3D−2D PnP problem, its linear solution and Bundle Adjustment solution.
\item 3D−3D ICP problem, its linear solution and Bundle Adjustment solution.
\end{enumerate}

This lecture is rich in content, and combined with the basic knowledge of the previous few lectures. If readers find it difficult to understand, they can review the previous knowledge. It is best to do the experiment yourself to understand the content of the entire motion estimation.

It should be explained that in order to ensure smooth writing, we have omitted a lot of discussions about certain special situations. For example, what happens if a given feature point is coplanar during the solution to the polar geometry (this is mentioned in the homography matrix $\bm{H}$)? What happens to the collinear line? What would happen if such a solution was given in PnP and ICP? Can the algorithm identify these particular situations and report that the resulting solution may be unreliable? Can you give an estimate of the uncertainty of $\bm{T}$? Although they are worthy of research and exploration, their discussion is more suitable for staying in specific papers. The goal of this book is extensive knowledge coverage and basic knowledge. We are not working on these issues, and they are rarely seen in engineering implementation. If you are concerned about these rare situations, you can read papers such as \cite{Hartley2003}.

\section*{ Exercises}
\begin{enumerate}
	\item In addition to the ORB feature points introduced in this book, which feature points can you find? Please talk about the principles of SIFT or SURF and compare the advantages and disadvantages between them and ORB.
	\item The design program calls other kinds of feature points in OpenCV. Counts the time spent on your machine when extracting 1000 feature points.
	\item[\optional] We found that the ORB feature points provided by OpenCV are not evenly distributed throughout the image. Can you find or propose a way to make the distribution of feature points more uniform?
	\item investigates why FLANN can handle matching problems quickly. In addition to FLANN, what other means of speeding up the match?
	\item changes the EPnP used by the demo to other PnP methods and studies how they work.
	\item In PnP optimization, the observation of the first camera is also taken into account, how should the program be written? What will happen to the final result?
	\item In the ICP program, the spatial point is also taken into account as an optimization variable. How should the program be written? What will happen to the final result?
	\item[\optional] In the process of feature point matching, it is inevitable that a mismatch will be encountered. What happens if we enter the wrong match into PnP or ICP? What methods can you think of to avoid mismatches?
	\item[\optional] Use Sophus's SE3 class to design g2o nodes and edges to optimize PnP and ICP.
	\item[\optional] Optimize PnP and ICP in Ceres.
\end{enumerate}