% !Mode:: "TeX:UTF-8"
\chapter{backend 1}
\label{cpt:backend1}
\begin{mdframed}
    \textbf{main target}
        \begin{enumerate}[labelindent=0em,leftmargin=1.5em]
            \item understands the concept of the backend.
            \item understands the working principle of the filter backend represented by EKF.
            \item Understand the back end of nonlinear optimization and understand how sparsity is utilized.
            \item uses g2o and Ceres to actually manipulate backend optimization.
        \end{enumerate}
\end{mdframed}

At the beginning of this lecture, we transferred to another important module of the SLAM system: back-end optimization.

We see that the front-end visual odometer can give a short time trajectory and map, but due to the inevitable accumulation of errors, this map is inaccurate for a long time. Therefore, based on the visual odometer, we also hope to construct a scale and scale optimization problem to consider the optimal trajectory and map over a long period of time. However, considering the balance between accuracy and performance, there are many different practices in practice.

\newpage
\includepdf{resources/other/ch10.pdf}

\newpage
\section{overview}
\subsection{Probability interpretation of state estimation}
As mentioned in the second lecture, the visual odometer has only a short memory, and we hope that the entire motion trajectory will remain optimal for a long time. We may use the latest knowledge to update the more distant state - from the perspective of "a long-term state", as if the future information tells it "Where should you be?" So, in back-end optimization, we usually consider a state estimation problem for a longer period of time (or all time), and not only use past information to update its state, but also update itself with future information. The processing method may be called "batch". Otherwise, if the current state is determined only by past moments, or even by the previous moment, it may be called "incremental".

We already know that the SLAM process can be described by equations of motion and observation equations. So, suppose that during the time from $t=0$ to $t=N$, there is a pose $\bm{x}_0$ to $\bm{x}_N$, and there is a road sign $\bm{y}_1 , \cdots, \bm{y}_M$. According to the previous writing, the motion and observation equations are
\begin{equation}
\left\{ \begin{array}{l}
{\bm{x}_k} = f\left( {{\bm{x}_{k - 1}},{\bm{u}_k}} \right) + \bm{w}_k \\
{\bm{z}_{k,j}} = h\left( {{ \bm{y}_j},{ \bm{x}_k}}  \right)+ \bm{v}_{k,j}
   \end{array} \right. \quad k=1, \ldots, N, \  j=1, \ldots, M.
\end{equation}

Note the following:

\begin{enumerate}
    \item In the observation equation, observation data is only generated when $\bm{x}_k$ sees $\bm{y}_j$, otherwise it is not. In fact, only a small number of road signs can usually be seen in one location. Moreover, due to the large number of visual SLAM feature points, the number of observation equations in practice will be much larger than the motion equation.
    \item We may not have a device to measure motion, so there may be no equations of motion. In this case, there are several ways to deal with it: think that there is really no equation of motion, or that the camera is not moving, or that the camera is moving at a constant speed. These methods are all feasible. In the absence of an equation of motion, the entire optimization problem consists of only a few observation equations. This is very similar to the SfM (Structure from Motion) problem, which is equivalent to restoring motion and structure through a set of images. Unlike SfM, images in SLAM have a temporal order, while SfM allows the use of completely unrelated images.
\end{enumerate}

We know that each equation is affected by noise, so we should treat the pose $\bm{x}$ and the signpost $\bm{y}$ as \textbf{random variables with a certain probability distribution} instead of A single number. So, the question we care about becomes: How do I determine the state quantity $\bm{x}$, when I have some motion data $\bm{u}$ and observation data $\bm{z}$ Distribution of $\bm{y}$? Furthermore, if the data of the new moments are obtained, how will their distribution change again? In the more common and reasonable case, we assume that the state quantities and noise terms obey the Gaussian distribution - this means that only the mean and covariance matrices need to be stored in the program. The mean can be seen as an estimate of the optimal value of the variable, while the covariance matrix measures its uncertainty. Then, the question turns into: How do we estimate the Gaussian distribution of state quantities when there are some motion data and observation data?

We still play the role of a small radish. Only the equation of motion is equivalent to walking blindly in an unknown place. Although we know how far we go every step of the way, as time goes by, we will become more and more uncertain about our position – the more uneasy the heart. This shows that when the input data is affected by noise, \textbf{error is gradually accumulated}, we will estimate the position variance will be larger and larger. However, when we open our eyes, we are more and more confident because we can continuously observe external scenes and make the uncertainty of position estimation smaller. If you visually express the covariance matrix with an ellipse or ellipsoid, then the process is a bit like the feeling of walking in mobile map software. Taking \autoref{fig:uncertainty}~ as an example, the reader can imagine that when there is no observation data, the circle will grow larger and larger; if there is correct observation, the circle will shrink to a certain size and keep stable.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.66\textwidth]{backend1/uncertainty.pdf}
\caption{Visual description of uncertainty. Left side: When there is only the motion equation, since the pose at the next moment adds noise on the basis of the previous moment, the uncertainty becomes larger and larger. Right side: When there are landmark points, the uncertainty will be significantly reduced. However, please note that this is just an intuitive diagram, not actual data. }
\label{fig:uncertainty}
\end{figure}

The above process explains the problem in state estimation in the form of a metaphor. Let us look at it quantitatively. In the \ref{cpt:6} lecture, we introduce the maximum likelihood estimation, mentioning that the \textbf{batch state estimation problem can be transformed into the maximum likelihood estimation problem and solved by the least squares method. } In this section, we will explore how to apply this conclusion to progressive problems and get some classic conclusions. At the same time, in the visual SLAM, the least squares method has a special structure.

First, since both the pose and the landmark are variables to be estimated, we change the token so that $\bm{x}_k$ is all unknowns at time $k$. It contains the current camera pose and $m$ landmark points. In the sense of this mark (although slightly different from the previous one, but the meaning is clear), write:
\begin{equation}
    \bm{x}_k  \buildrel \Delta \over =  \{ \bm{x}_k, \bm{y}_1, \ldots, \bm{y}_m \}.
\end{equation}

At the same time, all observations at time $k$ are recorded as $\bm{z}_k$. Thus, the form of the equation of motion and the equation of observation can be written more concisely. $\bm{y}$ won't appear here, but we have to understand that $\bm{y}$ is already included in $\bm{x}$:
\begin{equation}
\left\{ \begin{array}{l}
{\bm{x}_k} = f\left( {{\bm{x}_{k - 1}},{\bm{u}_k}} \right) + \bm{w}_k \\
{\bm{z}_{k}} = h\left( \bm{x}_k  \right)+ \bm{v}_{k}
\end{array} \right. \quad k=1, \ldots, N .
\end{equation}

Now consider the situation at the time of the $k$. We want to use the data from the past $0$ to $k$ to estimate the current state distribution:
\begin{equation}
P(\bm{x}_k | \bm{x}_0, \bm{u}_{1:k}, \bm{z}_{1:k}).
\end{equation}

The subscript $0:k$ represents all data from time $0$ to time $k$. Note that $\bm{z}_k$ means all observations at $k$, it may be more than one, but this is more convenient. At the same time, $\bm{x}_k$ is actually related to $\bm{x}_{k-1}, \bm{x}_{k-2}$, but this is not explicitly Write them out.

Let's look at how to estimate the state. According to Bayes' rule, swap $\bm{z}_k$ with $\bm{x}_k$, with:
\begin{equation}
\label{eq:10-5}
P\left( {{\bm{x}_k}|{\bm{x}_0},{\bm{u}_{1:k}},{\bm{z}_{1:k}}} \right) \propto P\left( {{\bm{z}_k}|{\bm{x}_k}} \right) P\left( {{\bm{x}_k}|{\bm{x}_0},{\bm{u}_{1:k}},{\bm{z}_{1:k - 1}}} \right).
\end{equation}

Readers should not be unfamiliar. The first item here is called \textbf{likelihood} and the second item is called \textbf{priority}. The likelihood is given by the observation equation, and in the a priori part, we have to understand that the current state $\bm{x}_k$ is estimated based on all past states. At the very least, it will be affected by $\bm{x}_{k-1}$, so the conditional probability is expanded according to $\bm{x}_{k-1}$:
\begin{equation}
\label{eq:bayes-estimator}
P\left( {{\bm{x}_k}|{\bm{x}_0},{\bm{u}_{1:k}},{\bm{z}_{1:k - 1}}} \right) = \int {P\left( {{\bm{x}_k}|{\bm{x}_{k - 1}},{\bm{x}_0},{\bm{u}_{1:k}},{\bm{z}_{1:k - 1}}} \right)P\left( {{\bm{x}_{k - 1}}|{\bm{x}_0},{\bm{u}_{1:k}},{\bm{z}_{1:k - 1}}} \right) \mathrm{d}\bm{x}_{k-1} }.
\end{equation}

If you consider the state before the longer, you can continue to expand this style, but now we only care about the time of $k$ and $k-1$. So far, we have given Bayesian estimates, although the above formula does not have a specific form of probability distribution, so it is not practical to operate it. There are some differences in the method for the subsequent processing of this step. In general, there are several choices: one is the assumption of \textbf{Markovity}, and the simple first-order Markov property considers that the state of $k$ is only related to the state of $k-1$, and Nothing before. If we make this assumption, we will get a filter method represented by \textbf{Extended Kalman Filter} (EKF). In the filtering method, we will estimate the state from a certain moment and derive it to the next moment. Another way is to still consider the relationship between the state of $k$ and the previous \textbf{all} state, and then get the \textbf{non-linear optimization} as the main optimization framework. The basics of nonlinear optimization have been introduced in the previous section. At present, the mainstream of visual SLAM is a nonlinear optimization method. However, in order to make this book more comprehensive, we must first introduce the principle of Kalman filter and EKF.

\subsection{Linear System and KF}
Let's first look at the filter model. When we assume Markovity, what changes will happen from a mathematical perspective? First, the current time state is only related to the previous time. The first part of the right side of the formula \eqref{eq:bayes-estimator} can be further simplified:
\begin{equation}
P\left( {{\bm{x}_k}|{\bm{x}_{k - 1}},{\bm{x}_0},{\bm{u}_{1:k}},{\bm{z}_{1:k - 1}}} \right) = P\left( {{\bm{x}_k}|{\bm{x}_{k - 1}},{\bm{u}_k}} \right).
\end{equation}

Here, since the state of $k$ is not related to before $k-1$, it is reduced to a form related only to $\bm{x}_{k-1}$ and $\bm{u}_k$. Corresponds to the equation of motion at the time of ${k}$. The second part can be simplified to
\begin{equation}
P\left( {{\bm{x}_{k - 1}}|{\bm{x}_0},{\bm{u}_{1:k}},{\bm{z}_{1:k - 1}}} \right) = P\left( {{\bm{x}_{k - 1}}|{\bm{x}_0},{\bm{u}_{1:k - 1}},{\bm{z}_{1:k - 1}}} \right).
\end{equation}

This is because the input $bm{u}_k$ at the time of $k$ is not related to the state of $k-1$, so we take $\bm{u}_k$ off. As you can see, this item is actually a state distribution at the time of $k-1$. Thus, this series of equations shows that what we are actually doing is "how to derive the state distribution at the time of $k-1$ to the time of $k$". That is to say, during the running of the program, we only need to maintain a state quantity, and iterate and update it continuously. Further, if we assume that the state quantity obeys the Gaussian distribution, then we only need to consider the mean and covariance of the maintenance state quantity. You can imagine that the positioning system on the radish has been outputting two positioning information outwards: one is his own posture and the other is his own uncertainty. This is often the case in practice.

We start with the simplest linear Gaussian system, and finally we will get the Kalman filter. After clarifying the starting point and the end point, let's consider the middle route. Linear Gaussian systems say that equations of motion and observation equations can be described by linear equations:
\begin{equation}
\left\{ \begin{array}{l}
{\bm{x}_k} = \bm{A}_k {{\bm{x}_{k - 1}}+{\bm{u}_k}} + \bm{w}_k \\
{\bm{z}_{k}} = \bm{C}_k  { \bm{x}_k} + \bm{v}_{k} \end{array} \right. \quad k=1, \ldots, N .
\end{equation}
It is also assumed that all states and noises satisfy the Gaussian distribution. Note that the noise here follows a zero-mean Gaussian distribution:
\begin{equation}
\bm{w}_k \sim N(\bm{0}, \bm{R}). \quad \bm{v}_k \sim N( \bm{0}, \bm{Q}).
\end{equation}

For the sake of brevity, I omitted the subscripts of $\bm{R}$ and $\bm{Q}$. Now, using Markov property, suppose we know the posterior of $k-1$ (in the case of $k-1$) state estimation $\bm{\hat{x}}_{k-1} $ and its covariance $\bm{\hat{P}}_{k-1}$, now the posterior distribution of $\bm{x}_k$ is determined based on the input and observation data at time $k$. To distinguish between a priori and posterior in the derivation, we make a difference in the notation: the above hat $\bm{\hat{x}}_k$ indicates a posteriori, the following hat $\check{\bm{x}}_k $ indicates a prior distribution, please do not confuse the reader.

The first step of the Kalman filter is to determine the prior distribution of $\bm{x}_k$ by the equation of motion. This step is linear, while the linear transformation of the Gaussian distribution is still a Gaussian distribution. So obviously there are:
\begin{equation}
P\left( {{\bm{x}_k}|{\bm{x}_0},{\bm{u}_{1:k}},{\bm{z}_{1:k - 1}}} \right) = N\left( {\bm{A}_k {{\hat{\bm{x}}}_{k - 1}} + {\bm{u}_k}, \bm{A}_k\hat{\bm{P}}_{k-1} {\bm{A}_k^\mathrm{T}} + \bm{R}} \right).
\end{equation}

This step is called \textbf{prediction}, and the principle is in the appendix \ref{sec:gauss-example}. It shows how the state distribution of the current time is inferred from the state of the previous moment based on the input information (but with noise). This distribution is also a priori. Remember:
\begin{equation}
\check{\bm{x}}_k = {\bm{A}_k {{\hat{\bm{x}}}_{k - 1}} + {\bm{u}_k}}, \quad \check{\bm{P}}_k = {\bm{A}_k \hat{\bm{P}}_{k-1} { \bm{A}^\mathrm{T}_k} + \bm{R}}.
\end{equation}

This is very natural. Obviously, the uncertainty of this step state will become larger because noise is added to the system. On the other hand, from the observation equation, we can calculate what kind of observation data should be produced under a certain state:
\begin{equation}
P\left( {{\bm{z}_k}|{\bm{x}_k}} \right) = N\left( {{\bm{C}_k}{\bm{x}_k},\bm{Q}} \right) .
\end{equation}

In order to get the posterior probability, we want to calculate their product, which is the Bayesian formula given by the formula \eqref{eq:10-5}. However, although we know that we will get a Gaussian distribution about $\bm{x}_k$ in the end, it is a bit of a computational problem. Let's first set the result to $\bm{x}_k \sim N(\ Bm{\hat{x}}_k, \bm{\hat{P}}_k )$, then:
\begin{equation}
N(\bm{\hat{x}}_k, \bm{\hat{P}}_k ) = \eta N\left( {{\bm{C}_k}{\bm{x}_k},\bm{Q}} \right) \cdot N( \bm{\check{x}}_k, \bm{\check{P}}_k). 
\end{equation}

Here we use a little bit of a clever way. Now that we know that the equations are Gaussian on both sides, we only need to compare the exponential parts without regard to the factor part before the Gaussian distribution. The index part is very similar to a quadratic formula, let's deduce it. First expand the exponent section, there is \footnote{The equal sign here is not strict, and the constants that are not related to $\bm{x}_k$ are actually allowed. }:
\begin{equation}
{\left( {{\bm{x}_k} - {{\hat{\bm{x}}}_k}} \right)^\mathrm{T}}\hat{\bm{P}}_k^{ - 1}\left( {{\bm{x}_k} - {{\hat{\bm{x}}}_k}} \right) = {\left( {{\bm{z}_k} - {\bm{C}_k} {\bm{x}_k}} \right)^\mathrm{T}}{\bm{Q}^{ - 1}}\left( {{\bm{z}_k} - {\bm{C}_k}{\bm{x}_k}} \right) + {\left( {{\bm{x}_k} - {{\check{\bm{x}}}_k}} \right)^\mathrm{T}}\check{\bm{P}}_k^{ - 1}\left( {\bm{x}_k - {{\check{\bm{x}}}_k}} \right).
\end{equation}

To find $\hat{\bm{x}}_k$ and $\bm{\hat{P}}_k$ on the left, we expand the two sides and compare the second sum of $\bm{x}_k$ Once coefficient. For quadratic coefficients, there are:
\begin{equation}
\label{eq:kalman-cov}
\hat{\bm{P}}_k^{ - 1} = \bm{C}_k^\mathrm{T}{\bm{Q}^{ - 1}}{\bm{C}_k} + \check {\bm{P}}_k^{ - 1}.
\end{equation}

This formula gives the calculation process of covariance. To facilitate the following formula, define an intermediate variable:
\begin{equation}
\label{eq:kalman-K}
\bm{K} = \bm{\hat{P}}_k \bm{C}_k^\mathrm{T} \bm{Q}^{-1}.
\end{equation}

According to this definition, the $\bm{\hat{P}}_k$ is multiplied by the left and right of the formula \eqref{eq:kalman-cov}, with:
\begin{equation}
\bm{I} = \bm{\hat{P}}_k \bm{C}_k^\mathrm{T} \bm{Q}^{-1} \bm{C}_k + \bm{\hat{P}}_k \bm{\check{P}}_k^{-1} = \bm{K} \bm{C}_k + \bm{\hat{P}}_k \bm{\check{P}}_k^{-1}.
\end{equation}

So there is \footnote{ here seems to have a little loop definition. We defined $\bm{K}$ by $\bm{\hat{P}}_k$ and wrote $\bm{\hat{P}}_k$ as an expression for $\bm{K}$ . However, in practice $\bm{K}$ can be calculated without relying on $\bm{\hat{P}}_k$, but this requires the introduction of Sherman-Morrison-Woodbury identity\cite{Sherman1950}, see the exercises in this lecture. }:
\begin{equation}
\bm{\hat{P}}_k = ( \bm{I} - \bm{K} \bm{C}_k ) \bm{\check{P}}_k.
\end{equation}

Then compare the coefficients of the item once, with:
\begin{equation}
- 2\hat {\bm{x}}_k^\mathrm{T} \hat{\bm{P}}_k^{ - 1}{\bm{x}_k} =  - 2\bm{z}_k^\mathrm{T} {\bm{Q}^{ - 1}}{\bm{C}_k}{\bm{x}_k} - 2\bm{\check {x}}_k^\mathrm{T} \bm{\check {P}}_k^{ - 1}{\bm{x}_k}.
\end{equation}

Finishing (take factor and transpose) to get:
\begin{equation}
\hat { \bm{P}}_k^{ - 1}{{\hat{\bm{x}}}_k} = \bm{C}_k^\mathrm{T} {\bm{Q}^{ - 1}}{\bm{z}_k} + \check{\bm{P}}_k^{ - 1}{{\bm{\check{x}}}_k}.
\end{equation}

Multiply both sides by $\bm{\hat{P}}_k$ and substitute \eqref{eq:kalman-K} to get:
\begin{align}
{{\bm{\hat {x}}}_k} &= {{\hat {\bm{P}}}_k} \bm{C}_k^\mathrm{T} { \bm{Q}^{ - 1}}{\bm{z}_k} + {{\bm{\hat{ P}}}_k}\check {\bm{P}}_k^{ - 1}{{\check {\bm{x}}}_k}\\
&= \bm{K} {\bm{z}_k} + \left( {\bm{I} - \bm{K}{\bm{C}_k}} \right){{\bm{\check {x}}}_k} = {{\check {\bm{x}}}_k} + \bm{K} \left( {\bm{z}_k - {\bm{C}_k}{\bm{\check{x}}_k}} \right).
\end{align}

So we got the expression of the posterior mean. In summary, the above two steps can be summarized into two steps: "Predict" and "Update":

\begin{mdframed}
\begin{enumerate}
\item forecast:
\begin{equation}
\check{\bm{x}}_k = {\bm{A}_k {{\hat{\bm{x}}}_{k - 1}} + {\bm{u}_k}}, \quad \check{\bm{P}}_k = {\bm{A}_k \hat{\bm{P}}_{k-1} { \bm{A}^\mathrm{T}_k} + \bm {R}}.
\end{equation}
\item update:
First calculate $\bm{K}$, which is also known as the Kalman gain.
\begin{equation}
\label{eq:kalman-K-another}
\bm{K} = {{\check {\bm{P}}}_k} \bm{C}_k^\mathrm{T} {\left( {{\bm{C}_k}{{\check { \bm{P}}}_k}\bm{C}_k^\mathrm{T} + {\bm{Q}_k}} \right)^{ - 1}}.
\end{equation}
Then calculate the distribution of posterior probabilities.
\begin{equation}
\begin{array}{l}
\hat {\bm{x}}_k = {{\check {\bm{x}}}_k} + \bm{K} \left( {\bm{z}_k - {\bm{C}_k} {\bm{\check{x}}_k}} \right)\\
{{\bm{\hat {P}}}_k} = \left( {\bm{I} - \bm{K}{\bm{C}_k}} \right) \check{\bm{P} }_k.
\end{array}.
\end{equation}
\end{enumerate}
\end{mdframed}

So far, we have derived the entire process of the classic Kalman filter. In fact, the Kalman filter has several derivations, and we use the form of the maximum a posteriori probability estimate from a probabilistic perspective. We see that in linear Gaussian systems, the Kalman filter constitutes the largest a posteriori probability estimate in the system. Moreover, since the Gaussian distribution still obeys the Gaussian distribution after linear transformation, we have not made any approximation throughout the process. It can be said that the Kalman filter constitutes the optimal unbiased estimation of the linear system.

\subsection{Nonlinear System and EKF}
After understanding the Kalman filter, we must clarify that the motion equations and observation equations in SLAM are usually nonlinear functions, especially the camera model in the visual SLAM, which requires the use of the camera internal parameter model and the pose expressed by Lie algebra. It can't be a linear system. A Gaussian distribution, after nonlinear transformation, is often no longer a Gaussian distribution. Therefore, in a nonlinear system, we must take a certain approximation and approximate a non-Gaussian distribution to a Gaussian distribution.

We hope to extend the results of the Kalman filter into a nonlinear system called the Extended Kalman Filter (EKF). It is common practice to consider the first-order Taylor expansion of the equation of motion and the observation equation near a certain point, leaving only the first-order term, the linear part, and then deriving it according to the linear system. Let the mean and covariance matrix of $k-1$ moment be $\bm{\hat{x}}_{k-1},\bm{\hat{P}}_{k-1}$. At $k$, we move the equations of motion and observations at $\bm{\hat{x}}_{k-1},\bm{\hat{P}}_{k-1}$\ Textbf{linearization} (equivalent to first-order Taylor expansion), with:
\begin{equation}
{\bm{x}_k} \approx f\left( {{{\bm{\hat x}}_{k - 1}},{\bm{u}_k}} \right) + {\left. {\frac{{\partial f}}{{\partial {\bm{x}_{k - 1}}}}} \right|_{{{\bm{\hat x}}_{k - 1}}}}\left( {{\bm{x}_{k - 1}} - {{\bm{\hat x}}_{k - 1}}} \right) + {\bm{w}_k}.
\end{equation}

Note that the partial derivative here is
\begin{equation}
\bm{F} = \left. {\frac{{\partial f}}{{\partial {\bm{x}_{k - 1}}}}} \right|_{{{\bm{\hat x}}_{k - 1}}}.
\end{equation}

Similarly, for the observation equations, there are also:
\begin{equation}
{\bm{z}_k} \approx h\left( {{{\bm{\check x}}_k}} \right) + {\left. {\frac{{\partial h}}{{\partial {\bm{x}_k}}}} \right|_{{{\bm{\check x}}_k}}}\left( {\bm{x}_k - {{\bm{\check x}}_k}} \right) + {\bm{n}_k}.
\end{equation}

Note that the partial derivative here is
\begin{equation}
\bm{H} = \left. {\frac{{\partial h}}{{\partial {\bm{x}_k}}}} \right|_{{{\bm{\check x}}_k}}.
\end{equation}

Then, in the \textbf{prediction} step, according to the equation of motion:
\begin{equation}
P\left( {{\bm{x}_k}|{\bm{x}_0},{\bm{u}_{1:k}},{\bm{z}_{0:k - 1}}} \right)
 = N(f\left( {{{\bm{\hat x}}_{k - 1}},{\bm{u}_k}} \right), \bm{F}\bm{\hat{P}}_{k-1} \bm{F}^\mathrm{T} + \bm{R}_k).
\end{equation}

These derivations and Kalman filters are very similar. For convenience, the mean of the prior and covariance here is recorded.
\begin{equation}
\bm{\check {x}}_k = f\left( {{{\bm{\hat x}}_{k - 1}},{\bm{u}_k}} \right), \quad \bm{\check{P}}_k = \bm{F}\bm{\hat{P}}_{k-1} \bm{F}^\mathrm{T} + \bm{R}_k.
\end{equation}

Then, consider that in the observation we have:
\begin{equation}
P\left( {{\bm{z}_k}|{\bm{x}_k}} \right) = N( h\left( {{{\bm{\check x}}_k}} \right) + \bm{H} \left( {\bm{x}_k - {{\bm{\check x}}_k}} \right), \bm{Q}_k ).
\end{equation}

Finally, based on the initial Bayesian expansion, the posterior probability form of $\bm{x}_k$ can be derived. We skip the middle of the derivation process and only introduce the results. The reader can derive the prediction and update equations of the EKF in the same way as the Kalman filter. In short, we will first define a \textbf{Kalman gain}$\bm{K}_k$:
\begin{equation}
\bm{K}_k = {\bm{\check{P}}_{k}}{\bm{H}^\mathrm{T}}{\left( {\bm{H}{\bm{\check P}_k}{\bm{H}^\mathrm{T}} + {\bm{Q}_k}} \right)^{ - 1}}.
\end{equation}

Based on the Kalman gain, the posterior probability is in the form of
\begin{equation}
{{\bm{\hat x}}_k} = {{\bm{\check x}}_k} + {\bm{K}_k}\left( {{\bm{z}_k} - h\left( {{\bm{\check x}_k}} \right)} \right),{\bm{\hat P}_k} = \left( {\bm{I} - {\bm{K}_k}{\bm{H}}} \right) \bm{\check{P}}_k.
\end{equation}

The Kalman filter gives the evolution of the distribution of state variables after linearization. Under linear systems and Gaussian noise, the Kalman filter gives an unbiased optimal estimate. In the case of nonlinearity of SLAM, it gives the maximum a posteriori estimate (MAP) for a single linear approximation.

\subsection{EKF Discussion}
EKF is known for its compact form and wide application. When we want to estimate an uncertainty in a certain period of time, the first thing we think of is EKF. In the early SLAM, EKF dominated for a long time, and researchers discussed the use of various filters in SLAM, such as IF (Information Filter)\textsuperscript{\cite{Sujan2005} }, IKF\textsuperscript{\cite{Janabi-Sharifi2010}}(Iterated KF), UKF\textsuperscript{\cite{Li2010}}(Unscented KF) and particle filter\textsuperscript{\cite{Sim2007, Lee2011, Gil2010a}} , SWF (Sliding Window \mbox{Filter)\textsuperscript{\cite{Sibley2010}}}, etc. \textsuperscript{\cite{Chen2012}}\footnote{The principle of particle filter is quite different from Kalman filter. }, or use the ideas of divide and conquer to improve the efficiency of EKF\textsuperscript{\cite{Paz2008, Grasa2011}}. To this day, although we recognize that nonlinear optimization has a clear advantage over filters, EKF is still an effective way when computing resources are limited or when the amount to be estimated is relatively simple.

What are the limitations of EKF?

\begin{enumerate}
\item First, the filter method assumes \textbf{Markoviness} to some extent, that is, the state of $k$ is only related to the time of $k-1$, and the state before $k-1$ It has nothing to do with observations (or related to the state of the first few finite moments). This is a bit like thinking about the relationship between two adjacent frames in a visual odometer. If the current frame is indeed related to data from a long time ago (for example, loopback), the filter will be difficult to process.

Non-linear optimization methods tend to use all historical data. It not only considers the relationship between feature points and trajectories in the vicinity, but also considers the state long before, called SLAM (Full-SLAM) in all time. In this sense, the nonlinear optimization method uses more information and of course requires more calculations.

\item Compared with the optimization method introduced in the sixth lecture, the EKF filter only linearizes \textbf{once} at $\bm{\hat{x}}_{k-1}$, and then directly This linearization result calculates the posterior probability. This is equivalent to saying that we think that \textbf{the linearization approximation at this point is still valid at the posterior probability}. In fact, when we are away from the working point, the first-order Taylor expansion does not necessarily approximate the entire function, depending on the nonlinearity of the motion model and the observation model. If they have strong nonlinearities, then the linear approximation is only found in a small range, and it cannot be considered that it can be approximated by linearity at a great distance. This is the EKF's \textbf{non-linearity error} and its main problem.

In the optimization problem, although we also do the first-order (slowest descent) or second-order (Gaussian Newton method or Levin Berg-Marquartt method) approximation, after each iteration, after the state estimation changes, we will Re-expanding Taylor to the new estimate, instead of doing a Taylor expansion at a fixed point like EKF. This makes the optimization method more applicable and can be applied when the state changes greatly. So in general, you can roughly think that \textbf{EKF is just an iteration in optimization}\footnote{More carefully, it is better than an iteration because the linearization of the update step is based on prediction. If you linearize the motion and observation models simultaneously at the time of prediction, it is exactly the same as an iteration. }.

\item From a program implementation point of view, the EKF needs to store the mean and variance of the state quantities and maintain and update them. If the road sign is also put into the state, due to the large number of road signs in the visual SLAM, this amount of storage is considerable and squared with the state quantity (because the covariance matrix is ​​to be stored). Therefore, it is generally accepted that EKF SLAM is not suitable for large scenes.

\item Finally, filter methods such as EKF have no anomaly detection mechanism, which causes the system to diverge easily when there are outliers. In visual SLAM, outliers are common: both feature matching and optical flow methods are easy to track or match to the wrong point. No outlier detection mechanism can make the system very unstable in practice.
\end{enumerate}

Due to these obvious shortcomings of EKF, we generally believe that nonlinear optimization can achieve better results under the same amount of computation \textsuperscript{\cite{Strasdat2012}}. "Better" here means precision and robustness at the same time to achieve better meaning. Let's discuss the backend based on nonlinear optimization. We will mainly introduce graph optimization and demonstrate backend optimization with g2o and Ceres.

\section{BA and graph optimization}
If you have done a visual 3D reconstruction, you should be familiar with this concept. The so-called Bundle Adjustment\footnote{ is also translated into beam adjustment, bundle adjustment, etc., but I feel that there is no Bundle Adjustment in English, so the English name is reserved here. }, refers to the extraction of the optimal 3D model and camera parameters (internal and external parameters) from the visual image. Consider the bundles of light rays emitted from any feature point, which become pixels or detected feature points on the imaging plane of several cameras. If we adjust the camera pose and the spatial position of each feature point so that the light eventually reaches the camera's optical center \textsuperscript{\cite{Triggs2000}}, it is called BA.

We have briefly introduced the principle of BA in the \ref{cpt:5} and \ref{cpt:7}. The focus of this section is to introduce the characteristics of its corresponding graph model structure, and then introduce some common fast Solution.

\subsection{Projection model and BA cost function}
First we review the entire projection process. Starting from the point $\bm{p}$ in a world coordinate system, taking into account the internal and external parameters and distortion of the camera, and finally projecting into pixel coordinates, the following steps are required.

\begin{enumerate}
\item First, convert the world coordinates to camera coordinates. Here you will use the extra camera parameter $(\bm{R}, \bm{t})$:
\begin{equation}
\bm{P}' = \bm{R} \bm{p} + \bm{t} = [X', Y', Z']^\mathrm{T}.
\end{equation}
\item Then, cast $\bm{P}'$ to the normalized plane to get the normalized coordinates:
\begin{equation}
\bm{P}_c = [u_c, v_c, 1]^\mathrm{T} = [X'/Z', Y'/Z', 1]^\mathrm{T}.
\end{equation}
\item Considers the distortion of the normalized coordinates and obtains the original pixel coordinates before dedistortion.
For the time being, only radial distortion is considered:
\begin{equation}
\left\{
\begin{array}{l}
U_c' = {u_c}\left( {1 + {k_1}r_c^2 + {k_2}r_c^4} \right)\\
V_c' = {v_c}\left( {1 + {k_1}r_c^2 + {k_2}r_c^4} \right)
\end{array}
\right. .
\end{equation}
\item Finally, calculate the pixel coordinates based on the internal parameter model:
\begin{equation}
\left\{ \begin{array}{l}
{u_s} = {f_x}u_c' + {c_x}\\
{v_s} = {f_y}v_c' + {c_y}
\end{array} \right.
\end{equation}
\end{enumerate}

This series of calculation processes may seem complicated. We use the process \autoref{fig:calculationflow}~ to visually represent the whole process to help the reader understand. The reader should be able to understand that this process is also the \textbf{observation equation} mentioned earlier. We previously wrote it abstractly as:
\begin{equation}
\bm{z} = h(\bm{x}, \bm{y}).
\end{equation}

\begin{figure}[!htp]
\centering
\includegraphics[width=1.0\textwidth]{backend1/calculationflow}
\caption{Calculation process diagram. The $\bm{p}$ on the left is the 3D coordinate point in the global coordinate system, and the $u_s on the right, v_s$ is the final pixel coordinate of the point on the image plane. $r_c^2=u_c^2 + v_c^2$ in the middle distortion module. }
\label{fig:calculationflow}
\end{figure}

Now we give its detailed parameterization process. Specifically, $\bm{x}$ here refers to the pose of the camera at this time, that is, the external parameter $\bm{R}, \bm{t}$, which corresponds to the Li group as $\bm{T }$, Lie algebra is $\bm{\xi}$. The road sign $\bm{y}$ is the three-dimensional point $\bm{p}$, and the observation data is the pixel coordinates $\bm{z} \buildrel \Delta \over = [u_s, v_s]^T $. Considering the least squares angle, you can write the error about this observation:
\begin{equation}
\bm{e} = \bm{z} - h(\bm{T}, \bm{p}).
\end{equation}

Then, taking into account the observations at other times, we can add a subscript to the error. Let $\bm{z}_{ij}$ be the data generated by observing the road sign $\bm{p}_j$ in the pose $\bm{T}_i$, then the overall \textbf{cost function} (Cost) Function) is
\begin{equation}
\label{eq:BAcostfunction}
\frac{1}{2}\sum_{i=1}^m \sum_{j=1}^n \| \bm{e}_{ij} \|^2 = \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^n \|
\bm{{z}}_{ij} - h(\bm{T}_{i},\bm{p}_j) \|^2 .
\end{equation}

Solving this least squares is equivalent to adjusting the pose and the road sign at the same time, which is called BA. Next, we will gradually explore the solution of the model based on the objective function and the nonlinear optimization content introduced in \ref{cpt:6}.

\subsection{BA's solution}
Observing the observation model $h(\bm{T}, \bm{p})$ in the previous section, it is easy to judge that the function is not a linear function. So we want to optimize it using some of the nonlinear optimizations introduced in the \ref{sec:6.2} section. According to the idea of ​​nonlinear optimization, we should start from a certain initial value and constantly search for the descending direction $\Delta \bm{x}$ to find the optimal solution of the objective function, that is, continuously solve the incremental equation \eqref{eq:minimize-deltax} The increment $\Delta \bm{x}$ in . Although the error term is for a single pose and landmark, on the overall BA objective function, we should define the argument as all variables to be optimized:
\begin{equation}
\bm{x} = [ \bm{T}_1, \ldots, \bm{T}_m, \bm{p}_1, \ldots, \bm{p}_n ]^\mathrm{T}.
\end{equation}

Accordingly, $\Delta \bm{x}$ in the incremental equation is an increment to the overall argument. In this sense, when we give an increment to the argument, the objective function becomes
\begin{equation}
\label{eq:tangentbundleindetail}
\frac{1}{2}\left\Vert f(\bm{x} + \Delta \bm{x}) \right\Vert ^2 \approx \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^n \left\Vert \bm{e}_{ij} + \bm{F}_{ij} \Delta \bm{\xi}_{i} + \bm{E}_{ij} \Delta \bm{p}_j \right\Vert^2 .
\end{equation}

Where $\bm{F}_{ij}$ represents the partial derivative of the entire cost function to the camera pose in the current state, and $\bm{E}_{ij}$ represents the partial derivative of the function to the location of the landmark. We have introduced their specific forms in the ~\ref{sec:BA-vo1}~ section, so we will not start deriving here. Now put the camera pose variables together:
\begin{equation}
\bm{x}_c=[ \bm{\xi}_1, \bm{\xi}_2, \ldots, \bm{\xi}_m ]^\mathrm{T} \in \mathbb{R}^{6m},
\end{equation}
And put the variables of the space point together:
\begin{equation}
\bm{x}_p=[ \bm{p}_1, \bm{p}_2, \ldots , \bm{p}_n ]^\mathrm{T}\in \mathbb{R}^{3n},
\end{equation}
Then, the formula \eqref{eq:tangentbundleindetail} can be simplified as follows:
\begin{equation}
\label{eq:BAleastsquare}
\frac{1}{2}
\left\Vert
f(\bm{x}+ \Delta \bm{x} )
\right\Vert ^2 = 
\frac{1}{2} 
\left\Vert 
\bm{e} + \bm{F}\Delta \bm{x}_c + \bm{E} \Delta \bm{x}_p 
\right \Vert ^2 .
\end{equation}

It should be noted that this formula changes from a sum of many small quadratic terms to a more holistic one. Here the Jacobian matrix $\bm{E}$ and $\bm{F}$ must be the derivative of the overall objective function to the global variable, it will be a large block matrix, and each small block inside, need The "$\bm{F}_{ij}$ and $\bm{E}_{ij}$ "spliced ​​together" for each error term. Then, whether we use the Gauss Newton method or the Levenberg-Marquart method, we will eventually face the incremental linear equation:
\begin{equation}
\bm{H} \Delta \bm{x} = \bm{g}.
\end{equation}

According to the knowledge of \ref{cpt:6}, we know that the main difference between the Gauss Newton method and the Levenberg-Marquart method is that $\bm{H}$ is $\bm{J }^\mathrm{T}\bm{J}$ is still in the form of $\bm{J}^\mathrm{T}\bm{J}+ \lambda \bm{I}$. Since we classify variables into poses and spatial points, the Jacobian matrix can be divided into blocks.
\begin{equation}
\bm{J}=[\bm{F} \ \bm{E}].
\end{equation}

Then, taking the Gauss-Newton method as an example, the $\bm{H}$ matrix is
\begin{equation}\label{eq:HessianMatrix}
\bm{H} = \bm{J}^\mathrm{T}\bm{J} =
\begin{bmatrix}
         \bm{F}^\mathrm{T}\bm{F}   &   \bm{F}^\mathrm{T}\bm{E}   \\ 
         \bm{E}^\mathrm{T}\bm{F}   &   \bm{E}^\mathrm{T}\bm{E}
\end{bmatrix} .
\end{equation}

Of course, we also need to calculate this matrix in the Levenberg-Marquart method. It's not hard to find that because all the optimization variables are considered, the dimensions of this linear equation will be very large, including all camera poses and landmark points. Especially in visual SLAM, an image will present hundreds of feature points, greatly increasing the scale of this linear equation. If you calculate the incremental equation directly by inverting $\bm{H}$, since the matrix inversion is an operation of $O(n^3)$, \textsuperscript{\cite{Sueli2003}}, this is very expensive. Computing resources. Fortunately, the $\bm{H}$ matrix here has a certain special structure. With this special structure, we can speed up the solution process.

\subsection{Sparseness and marginalization}
An important development of the 21st century visual SLAM is the recognition of the sparse structure of the matrix $\bm{H}$, and found that the structure can naturally and explicitly use graph optimization to represent \textsuperscript{\cite{Kummerle2011, Polok2013}}. This section will discuss the matrix sparse structure in detail.

The sparsity of the $\bm{H}$ matrix is ​​caused by the Jacobian matrix $\bm{J}(\bm{x})$. Consider one of these cost functions, $\bm{e}_{ij}$. Note that this error term only describes the $\bm{p}_j$ in $\bm{T}_i$, which only involves the $i$ camera pose and the $j$ landmark point. The derivative of the remaining variables is 0. Therefore, the Jacobian matrix corresponding to the error term has the following form:
\begin{equation}
\bm{J}_{ij}(\bm{x}) = \left(
\bm{0}_{2 \times 6},...
\bm{0}_{2 \times 6},
\frac{\partial \bm{e}_{ij}}{\partial \bm{T}_i},
\bm{0}_{2 \times 6},...
\bm{0}_{2 \times 3},...
\bm{0}_{2 \times 3},
\frac{\partial \bm{e}_{ij}}{ \partial \bm{p}_j},
\bm{0}_{2 \times 3},...
\bm{0}_{2 \times 3} 
\right) .
\end{equation}

Where $\bm{0}_{2 \times 6}$ represents the $\bm{0}$ matrix with dimensions $2 \times 6$, and the same $\bm{0}_{2 \times 3}$ same. The bias term for the camera pose is ${\partial \bm{e}_{ij}}/{\partial \bm{\xi}_i}$ dimension is $2 \times 6$, partial guide to the landmark point The ${\partial \bm{e}_{ij}}/{\partial \bm{p}_j}$ dimension is $2 \times 3$. The Jacobian matrix of this error term is zero except that the two are non-zero blocks. This embodies the fact that the error term is independent of other road signs and tracks. From the perspective of graph optimization, this observation edge is only related to two vertices. So, what effect does it have on the incremental equation? Why does the $\bm{H}$ matrix produce sparsity?

Taking \autoref{fig:sparse}~ as an example, we set $\bm{J}_{ij}$ to have only non-zero blocks at $i,j$, then its contribution to $\bm{H}$ For $\bm{J}_{ij}^\mathrm{T} \bm{J}_{ij}$, it has the sparse form drawn on the graph. The $\bm{J}_{ij}^\mathrm{T} \bm{J}_{ij}$ matrix also has only 4 non-zero blocks, located at $(i,i), (i,j) , (j,i), (j,j)$. For the overall $\bm{H}$, there are:
\begin{equation}
\bm{H} = \sum_{i,j} \bm{J}_{ij}^\mathrm{T} \bm{J}_{ij},
\end{equation}
Note that $i$ takes values ​​in all camera poses, and $j$ takes values ​​in all landmark points. We divide $\bm{H}$ into blocks:
\begin{equation}
\label{eq:H-blocks}
\bm{H} = \left[ {\begin{array}{*{20}{c}}
	{{\bm{H}_{11}}}&{{\bm{H}_{12}}}\\
	{{\bm{H}_{21}}}&{{\bm{H}_{22}}}
	\end{array}} \right] .
\end{equation}

\begin{figure}[!htp]
\centering
\includegraphics[width=1.0\textwidth]{backend1/sparse.pdf}
\caption{When an error term $\bm{J}$ is sparse, its contribution to $\bm{H}$ is also sparse. }
\label{fig:sparse}
\end{figure}

Here $\bm{H}_{11}$ is only related to the camera pose, and $\bm{H}_{22}$ is only related to the landmark points. When we traverse $i,j$, the following facts are always true:
\begin{enumerate}
\item No matter how $i, j$ changes, $\bm{H}_{11}$ is a diagonal matrix, with only non-zero blocks at $\bm{H}_{i,i}$.
\item Similarly, $\bm{H}_{22}$ is also a diagonal array with only non-zero blocks at $\bm{H}_{j,j}$.
\item For $\bm{H}_{12}$ and $\bm{H}_{21}$, they may be sparse or dense, depending on the specific observations.
\end{enumerate}

This shows the sparse structure of $\bm{H}$. Later, in solving the linear equation, it is also necessary to use its sparse structure. Perhaps the reader has not yet grasped the meaning of this, and we give an example to illustrate its situation. Suppose there are 2 camera poses ($C_1, C_2$) and 6 road signs ($P_1, P_2, P_3, P_4, P_5, P_6$) in a scene. The variables for these cameras and point clouds are $\bm{T}_i, i = 1,2$ and $\bm{p}_j, j = 1,\cdots, 6$. The camera $C_1$ observed the road signs $P_1, P_2, P_3, P_4$, and the camera $C_2$ observed the road signs $P_3, P_4, P_5, P_6$. We draw this process as a schematic \autoref{fig:simplegraph}~. Cameras and road signs are represented by circular nodes. If the $i$ camera is able to observe the $j$ point, we will connect an edge to their corresponding node.

\begin{figure}[!htp]
\centering
\includegraphics[width=0.8\textwidth]{backend1/simplegraph.pdf}
\caption{A diagram of the composition of points and edges. The figure shows that the camera $C_1$ has observed the signposts $P_1, P_2, P_3, P_4$, and the camera $C_2$ has seen $P_3$ to $P_6$. }
\label{fig:simplegraph}
\end{figure}

Can be launched, the BA target function under the scene should be
\begin{equation}
\frac{1}{2}\left( \left\Vert \bm{e}_{11} \right\Vert^2 + 
\left\Vert \bm{e}_{12} \right\Vert^2 + 
\left\Vert \bm{e}_{13} \right\Vert^2 + 
\left\Vert \bm{e}_{14} \right\Vert^2 + 
\left\Vert \bm{e}_{23} \right\Vert^2 + 
\left\Vert \bm{e}_{24} \right\Vert^2 + 
\left\Vert \bm{e}_{25} \right\Vert^2 + 
\left\Vert \bm{e}_{26} \right\Vert^2 
\right).
\end{equation}

Here $\bm{e}_{ij}$ uses the previously defined cost function, ie, \eqref{eq:BAcostfunction}. Take $\bm{e}_{11}$ as an example. It describes the $P_1$ seen in $C_1$, regardless of other camera poses and road signs. Let $\bm{J}_{11}$ be the Jacobian matrix for $\bm{e}_{11}$, it’s not hard to see that $\bm{e}_{11}$ is for the camera variable $\ The partial derivatives of bm{\xi}_2$ and the landmarks $\bm{p}_2, \cdots, \bm{p}_6$ are all zero. We put all variables as $\bm{x} = \left( \bm{\xi}_1, \bm{\xi}_2, \bm{p}_1, \cdots, \bm{p}_2 \right) The order of ^\mathrm{T}$ is placed, then:
\begin{equation}
\bm{J}_{11} = \frac{\partial \bm{e}_{11}}{\partial \bm{x}}
= \left(
\frac{\partial \bm{e}_{11}}{\partial \bm{\xi}_1},
\bm{0}_{2\times 6},
\frac{\partial \bm{e}_{11}}{\partial \bm{p}_1},
\bm{0}_{2\times 3},
\bm{0}_{2\times 3},
\bm{0}_{2\times 3},
\bm{0}_{2\times 3},
\bm{0}_{2\times 3}
\right).
\end{equation}

In order to facilitate the representation of sparsity, we use a square with a color to indicate that the matrix has a value in the square, and the remaining areas without color indicate that the matrix has a value of zero at that point. Then the above $\bm{J}_{11}$ can be expressed as \autoref{fig:jaobianrow}~. Similarly, other Jacobian matrices will have similar sparse patterns.

\begin{figure}[!t]
\centering
\includegraphics[width=0.8\textwidth]{backend1/Jacobianrow.pdf}
The non-zero block distribution of the \caption{$\bm{J}_{11}$ matrix. The upper mark indicates the variable corresponding to the column of the matrix. Since the camera parameter dimension is larger than the point cloud parameter dimension, the matrix block corresponding to $C_1$ is wider than the matrix block corresponding to $P_1$. }
\label{fig:jaobianrow}
\end{figure}

In order to get the Jacobian matrix corresponding to the objective function, we can classify these $\bm{J}_{ij}$ as vectors in a certain order, then the overall Jacobian matrix and the corresponding $\bm{H}$ matrix The sparse situation is as shown in \autoref{fig:simplematrix}~.

\begin{figure}[!t]
\centering
\includegraphics[width=1\textwidth]{backend1/simplematrix.pdf}
\caption{Sparseness of the Jacobian matrix (left) and the sparsity of the $\bm{H}$ matrix (right). The filled squares indicate that the matrix has values ​​at the corresponding matrix block, and the remaining parts with no color represent the matrix. The value there is always 0. }
\label{fig:simplematrix}
\end{figure}

Perhaps you have noticed that \autoref{fig:simplegraph}~ corresponds to \textbf{adjacency matrix}\footnote{the so-called adjacency matrix is ​​a matrix whose $i, j$ elements describe Whether there is an edge between nodes $i$ and $j$. If this edge exists, set this element to 1, otherwise set to 0. } and the $\bm{H}$ matrix in the above figure, except for the diagonal elements, have the same structure. In fact, it is. The above $\bm{H}$ matrix has a total of $8\times 8$ matrix blocks. For matrix blocks in the $\bm{H}$ matrix that are not diagonal, if the matrix block is non-zero, then There will be an edge between the variables corresponding to their position in the figure, we can clearly see this from \autoref{fig:matrixandgraph}~. Therefore, the non-zero matrix block of the non-diagonal portion of the $\bm{H}$ matrix can be understood as a relationship between its corresponding two variables, or can be called a constraint. Therefore, we find that there is a clear connection between the graph optimization structure and the sparseness of the incremental equation.

\begin{figure}[!htp]
\centering
\includegraphics[width=1\textwidth]{backend1/matrixandgraph.pdf}
\caption{$\bm{H}$ The correspondence between non-zero matrix blocks in the matrix and the edges in the graph. For example, the red matrix block on the right side of the $\bm{H}$ matrix on the left shows that there is an edge between the corresponding variables $C_2$ and $P_6$ in the right image. $\bm{e}_{26} $. }
\label{fig:matrixandgraph}
\end{figure}

Now consider the more general case, if we have $m$ camera pose, $n$ landmark points. Since there are usually more road signs than cameras, there is $n \gg m$. From the above reasoning, the actual $\bm{H}$ matrix will be as shown in \autoref{fig:BigHmatrix}~. Its upper left corner block is very small, while the diagonal corner block in the lower right corner occupies a lot of places. In addition to this, the non-diagonal parts are distributed with scattered observation data. Because it is shaped like an arrow, it is also called an arrow-like matrix \textsuperscript{\cite{Barfoot2016}}. At the same time, it is also very similar to a scorpion, so I also call it the 镐 matrix matrix \footnote{ This is a joke, please do not write in the formal academic paper. }.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.7\textwidth]{backend1/BigHmatrix.pdf}
\caption{Generally $\bm{H}$ matrix. }
\label{fig:BigHmatrix}
\end{figure}

For $\bm{H}$ with this sparse structure, what is the difference between the linear equation $\bm{H} \Delta \bm{x}= \bm{g}$? There are several ways to use the sparse acceleration calculation of $\bm{H}$ in reality. This section introduces one of the most commonly used methods in visual SLAM: Schur elimination. Also known as Marginalization in SLAM research.

Looking closely at \autoref{fig:BigHmatrix}, we can easily see that this matrix can be divided into 4 blocks, which is consistent with the formula \eqref{eq:H-blocks}. The upper left corner is a diagonal block matrix, and the dimensions of each diagonal block element are the same as the dimensions of the camera pose, and are a diagonal block matrix. The lower right corner is also a diagonal block matrix, and the dimension of each diagonal block is the dimension of the road sign. The structure of the non-diagonal block is related to the specific observation data. We first divide the matrix into regions according to the way shown in \autoref{fig:MatrixSegmentation}. It is not difficult for the reader to find that the four regions correspond to the four matrix blocks in the formula \eqref{eq:HessianMatrix}. For the convenience of subsequent analysis, we remember that these 4 blocks are $\bm{B}, \bm{E}, \bm{E}^\mathrm{T}, \bm{C}$.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\textwidth]{backend1/MatrixSegmentation.pdf}
\caption{$\bm{H}$ The division of the matrix. }
\label{fig:MatrixSegmentation}
\end{figure}

Thus, the corresponding linear system of equations can also be changed from $\bm{H\Delta x} = \bm{g}$ to the following form:
\begin{equation}
\label{eq:linearequations}
 \left[ \begin{matrix}
\bm{B}   &   \bm{E} \\
\bm{E^\mathrm{T}} &   \bm{C}
\end{matrix}\right] 
\left[ \begin{array}{l}
\Delta \bm{x}_c \\
\Delta \bm{x}_p 
\end{array} \right] = 
\left[ \begin{array}{l}
\bm{v} \\
\bm{w} 
\end{array} \right].
\end{equation}

Where $\bm{B}$ is a diagonal block matrix, the dimensions of each diagonal block are the same as the dimensions of the camera parameters, and the number of diagonal blocks is the number of camera variables. Since the number of road signs is much larger than the number of camera variables, $\bm{C}$ is often much larger than $\bm{B}$. Each landmark in 3D space is three-dimensional, so the $\bm{C}$ matrix is ​​a diagonal block matrix, and each block is a $3 \times 3$ matrix. The difficulty of inverting the diagonal block matrix is ​​much less difficult than the inverse of the general matrix, because we only need to invert the diagonal matrix blocks separately. Considering this property, we perform Gaussian elimination on the linear equations. The goal is to eliminate the non-diagonal part of the upper right corner, $\bm{E}$, to get:
\begin{equation}\label{eq:guasselimination}
\left[ \begin{matrix}
\bm{I}   &    -\bm{EC^{-1}} \\
\bm{0}	 &	  \bm{I}
\end{matrix}\right]
\left[ \begin{matrix}
\bm{B}   &   \bm{E} \\
\bm{E^\mathrm{T}} &   \bm{C}
\end{matrix}\right] 
\left[ \begin{array}{l}
\Delta \bm{x}_c \\
\Delta \bm{x}_p 
\end{array} \right] = 
\left[ \begin{matrix}
\bm{I}   &    -\bm{EC^{-1}}  \\
\bm{0}	 &	  \bm{I}
\end{matrix}
\right]
\left[ \begin{array}{l}
\bm{v} \\
\bm{w} 
\end{array} \right]  .
\end{equation}

Finishing, get:
\begin{equation}
\left[ \begin{matrix}
\bm{B} - \bm{E}\bm{C}^{-1}\bm{E}^\mathrm{T}	& 	\bm{0} \\
\bm{E}^\mathrm{T}							& 	\bm{C}
\end{matrix} \right]
\left[ \begin{array}{l}
\Delta \bm{x}_c \\
\Delta \bm{x}_p 
\end{array} \right] = 
\left[\begin{array}{l}
\bm{v} - \bm{E}\bm{C}^{-1}\bm{w}  \\
\bm{w}
\end{array}\right].
\end{equation}

After the elimination, the first line of the equation becomes an item unrelated to $\Delta \bm{x}_p$. Take it out alone and get the incremental equation for the pose part:
\begin{equation}\label{eq:marginalization}
\left[ 
\bm{B} - \bm{E}\bm{C}^{-1}\bm{E}^\mathrm{T}
\right]
\Delta \bm{x}_c  = 
\bm{v} - \bm{E}\bm{C}^{-1}\bm{w} .
\end{equation}

The dimensions of this linear equation are the same as the $\bm{B}$ matrix. Our approach is to solve this equation first, then substitute the solved $\Delta \bm{x}_c$ into the original equation, and then solve $\Delta \bm{x}_p$. This process is called \textbf{Marginalization}\textsuperscript{\cite{Sibley2010}}, or \textbf{Schur Elimination}. Compared to the direct solution of linear equations, its advantages are:

\begin{enumerate}
\item In the elimination process, since $\bm{C}$ is a diagonal block, $\bm{C}^{-1}$ is easy to solve.
After \item solves $\Delta \bm{x}_c$, the incremental equation for the road sign portion is $\Delta \bm{x}_p = \bm{C}^{-1} (\bm{w} - \bm{E}^\mathrm{T} \Delta \bm{x}_c)$ is given. This still uses the features of $\bm{C}^{-1}$ that are easy to solve.
\end{enumerate}

Thus, the main computational amount of marginalization lies in the solution \eqref{eq:marginalization}. There is not much we can say about this equation. It is just an ordinary linear equation and no special structure can be utilized. Let's write the coefficient of this equation as $\bm{S}$. How is it sparse? \autoref{fig:marginalization} shows a $\bm{S}$ instance after the Schur elimination, and it can be seen that its sparsity is irregular.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\textwidth]{backend1/marginalization.pdf}
\caption{The sparse state of the $\bm{S}$ matrix after the Schur elimination of the $\bm{H}$ matrix. }
\label{fig:marginalization}
\end{figure}

As mentioned earlier, the non-zero elements at the non-diagonal blocks of the $\bm{H}$ matrix correspond to the association of the camera and the landmark. So, does the sparsity of $\bm{S}$ after Schur elimination have physical meaning? The answer is yes. Here we do not prove that the non-zero matrix block on the off-diagonal line of the $\bm{S}$ matrix indicates that there are common observations between the two camera variables. It is called Co-visibility. Conversely, if the block is zero, it means that the two cameras are not observed together. For example, the sparse matrix shown by \autoref{fig:marginalizationanalysis}~, the top left $4 \times 4$ matrix blocks can indicate that the corresponding camera variables $C_1, C_2, C_3, C_4$ have common observations.

\begin{figure}[!htp]
\centering
\includegraphics[width=0.8\textwidth]{backend1/marginalizationanalysis.pdf}
\caption{ Take the $4 \times 4$ matrix block in the $\bm{S}$ matrix as an example. The matrix block in this area is $\bm{S}_{14}, \bm{S}_{24 }$ is not zero, indicating that there is a common observation point between the camera $C_4$ and the camera $C_1$, $C_2$; and $S_{34}$ is zero, indicating that there is no common observation between $C_3$ and $C_4$. Road sign. }
\label{fig:marginalizationanalysis}
\end{figure}
\clearpage
Thus, the sparse structure of the $\bm{S}$ matrix depends on the actual observations, and we cannot predict in advance. In practice, for example, the Local Mapping link in ORB-SLAM\textsuperscript{\cite{Mur-Artal2015}} deliberately selects frames with common observations as key frames when doing BA, in which case Schur eliminates The resulting $\bm{S}$ is a dense matrix. However, since this module is not executed in real time, this is acceptable. But in other methods, such as DSO\textsuperscript{\cite{Engel2016}}, OKVIS\textsuperscript{\cite{Leutenegger2015}}, etc., they use the sliding window method (Sliding Window). This type of method requires a BA for each frame to prevent the accumulation of errors, so they must also use some techniques to maintain the sparsity of the $\bm{S}$ matrix. Readers who want to be able to go deeper into this piece can refer to their papers. I won't talk about these details in detail here.

From a probabilistic point of view, we call this step marginal because we actually turn the problem of $(\Delta \bm{x}_c, \Delta \bm{x}_p)$ into a fixed one. $\Delta \bm{x}_p$, find $\Delta \bm{x}_c$, and then ask for $\Delta \bm{x}_p$. This step is equivalent to doing a conditional probability expansion:
\begin{equation}
P( \bm{x}_c, \bm{x}_p ) = P(\bm{x}_c | \bm{x}_p ) P( \bm{x}_p ) ,
\end{equation}

The result is the edge distribution for $\bm{x}_p$, so it is called marginalization. In the marginalization process we talked about earlier, we actually marginalized all the landmark points. According to the actual situation, we can also choose a part to be marginalized. At the same time, Schur elimination is only one way to achieve marginalization, and can also be marginalized using Cholesky decomposition.

The reader may continue to ask, after the Schur elimination, we also need to solve the linear equations \eqref{eq:marginalization}. Is there any trick to solve it? Unfortunately, this part is a traditional matrix numerical solution, usually calculated by decomposition. Regardless of which solution is used, we recommend Schur elimination using the sparsity of $\bm{H}$. Not only does this increase speed, but also because the conditional cost of the $\bm{S}$ matrix after the elimination is often smaller than the previous $\bm{H}$ matrix. Schur elimination does not mean that all road signs are eliminated. The elimination of camera variables is also the means used in SLAM.

%\subsection{Sparse linear equations solution}
% After the Schur elimination, we also need to solve the linear equations \eqref{eq:marginalization}. We abbreviate this formula into the formula \eqref{eq:reducedBundleAdjustment}. Where $\bm{S}$ is a semi-positive symmetric matrix with a dimension of $mc \times mc$, and $\bm{p}$ is a vector with a dimension of $mc \times 1$.
%
%\begin{equation}\label{eq:reducedBundleAdjustment}
%\bm{S}\Delta \bm{x}_c = \bm{p}
%\end{equation}
%
% readers may think that this is very simple. If you reverse or violate $\bm{S}$, you can get the solution of this equation \eqref{eq:reducedBundleAdjustment}. This is theoretically possible, but if you think carefully We will also find that although the $\bm{S}$ matrix we get here has greatly reduced the dimension compared to the previous $\bm{H}$, the inversion (or violation) is still a calculation. Very large calculations. In fact, not only SLAM, but also the engineering calculations in other areas will rarely be solved by the idea of ​​inversion (or pseudo-reverse).
%
% Because the methods for solving linear equations are varied, they are not taken out here. Usually, in general, the way to solve linear equations is LU decomposition, but here, since our $\bm{S}$ matrix is ​​a symmetric semi-definite matrix, we can do it faster. Among SLAM or SFM, common methods include Cholesky decomposition or $LDL^T$ decomposition. Cholesky is used to decompose the positive definite matrix, and $LDL^T$ is used to decompose the symmetric matrix. Therefore, the choice between the two is mainly to look at the characteristics of the $\bm{S}$ matrix. Since we usually make the $\bm{S}$ matrix positive in SLAM, we will focus on the Cholesky decomposition method used in the case of positive definite matrices. You may wonder why the decomposition of the matrix accelerates the solution of this linear equation, so before we do that, let's review the solution to the linear equations for the triangular matrix:
%
% assumes that the $\bm{L}$ matrix is ​​a lower triangular matrix with dimensions nn \times n$ and the elements on the diagonal are non-zero. $\bm{b}$ is a vector with dimensions $n \times 1$, then the linear equations $\bm{Lx=b}$ expands to get the following form:
%$$\begin{array}{*{20}{c}}
%\begin{array}{l}
%{L_{11}}{x_1}\\
%{L_{21}}{x_1} + {L_{22}}{x_2}\\
%\vdots \\
%\vdots \\
%{L_{n1}}{x_1} + {L_{n2}}{x_2} +  \cdots  \cdots {L_{nn}}{x_n}
%\end{array}&\begin{array}{l}
%= {b_1}\\
%= {b_2}\\
%\vdots \\
%\vdots \\
%= {b_n}
%\end{array}
%\end{array}	$$
%
% At this point, we can find $x_1$ directly from the first line, then substitute the second line ${L_{21}}{x_1} + {L_{22}}{x_2} $ to find $x_2$, in turn The analogy can get the last $x_n$. The whole process finds $x_n$ from $x_1$, which is called \textbf{forward substitution}. Its pseudo code can be expressed as follows:
%
%\begin{algorithm}[h]
% \caption{(forward substitution method)}
% \label{alg:forwardsubstitution}
% \begin{algorithmic}[1]
% \REQUIRE
% $\bm{L}$: The triangle under the real number, the diagonal element is non-zero;
% $\bm{b}$: real number vector;
% $n$: The dimension of $\bm{L}$;
% \FOR{$i=1$ to $n$ }
%
% \STATE $x_i = (b_i - \sum_{k=0}^{i-1}{L_{ik}x_k})/L_{ii}$;
%
% \ENDFOR
%
% \RETURN
% $\bm{x}$
% \end{algorithmic}
%\end{algorithm}
%
% Of course, when $\bm{L}$ is an upper triangular matrix, the corresponding substitution order is to first find the last item $x_n$, and then enter the last to find $x_1$. This process is called \textbf{back substitution. The corresponding pseudo code is as follows:
%
%\begin{algorithm}[h]
% \caption{(reverse substitution method)}
% \label{alg:backsubstitution}
% \begin{algorithmic}[1]
% \REQUIRE
% $\bm{L}$: The triangle under the real number, the diagonal element is non-zero;
% $\bm{b}$: real number vector;
% $n$: The dimension of $\bm{L}$;
% \FOR{$i=n$ to $1$ }
%
% \STATE $x_i = (b_i - \sum_{k=i}^{n}{L_{ik}x_k})/L_{ii}$;
%
% \ENDFOR
%
% \RETURN
% $\bm{x}$
% \end{algorithmic}
%\end{algorithm}
%
% is based on such excellent characteristics of the triangular matrix. Decomposing it into a triangular matrix is ​​one of the commonly used solving methods when solving linear equations. The Cholesky decomposition we are going to introduce is a form of decomposing a positive definite matrix $\bm{S}$ into $\bm{S} = \bm{L}\bm{L^T}$, when $\bm{L} When the matrix diagonal element is a positive number, the decomposition result is unique. The Cholesky decomposition is also very simple, starting with the diagonal elements and recursively reorganizing each column below the diagonal elements:
%
%\begin{algorithm}[h]
% \caption{(Cholesky decomposition)}
% \label{alg:CholeskyDecomposition}
% \begin{algorithmic}[1]
% \REQUIRE
% $\bm{S}$: real positive definite matrix;
% $n$: The dimension of the positive definite matrix $\bm{S}$;
% \FOR{$j=1$ to $n$}
%
% \STATE $\bm{L}_{jj} = \sqrt{\bm{S}_{jj} - \sum_{k=1}^{j-1}{L_{jk}^2}}$ ;
%
% \FOR{$i=j+1$ to $n$}
% \STATE $\bm{L}_{ij} = \frac{1}{\bm{L}_{jj}}(\bm{S}_{ij} - \sum_{k=1}^{ J-1}{\bm{L}_{ik}\bm{L}_{jk}})$;
% \ENDFOR
%
% \ENDFOR
% \RETURN
% $\bm{L}$
% \end{algorithmic}
%\end{algorithm}
%
% It is easy to see that the complexity of the algorithm is $O(n^3)$, where $n$ is the dimension of the matrix. After the Cholesky decomposition of \eqref{eq:reducedBundleAdjustment}, the linear equations we need to solve become $\bm{LL^T}\Delta \bm{x}_c = \bm{p}$. To take full advantage of the convenience of triangular matrix solving, we denote the vector $\bm{L^T} \Delta \bm{x}_c$ as $\bm{y}$. In this way, we can first solve $\bm{Ly=b}$ to get $\bm{y}$, and then use the convenience of the triangle matrix solution to find $\bm{L^T} \Delta \ Bm{x}_c = \bm{y}$ to get the final $\Delta \bm{x}_c $. So we can easily conclude that the process of solving linear equations using Cholesky decomposition is as follows:
%
%\begin{algorithm}[h]
% \caption{(Cholesky decomposition solves linear equations)}
% \label{alg:CholeskyLinearsolver}
% \begin{algorithmic}[1]
% \REQUIRE
% $\bm{S}$: real positive definite matrix
% $n$: dimension of positive definite matrix $\bm{S}$
% $\bm{b}$: real number vector with dimension n$times\1$
% \STATE uses (\ref{alg:CholeskyDecomposition}) to perform Cholesky decomposition on the matrix $\bm{S}$ to obtain its corresponding lower triangular matrix $\bm{L}$;
%
% \STATE uses the forward substitution method (\ref{alg:forwardsubstitution}) to solve the linear equations $\bm{Ly = b}$ and get the vector $\bm{y}$;
%
% \STATE uses the inverse substitution method (\ref{alg:backsubstitution}) to solve the linear equations $\bm{L^Tx = y}$ and get the vector $\bm{x}$;
%
% \RETURN
% $\bm{x}$
% \end{algorithmic}
%\end{algorithm}
%
% For semi-positive matrices, the usual practice is to add a smaller diagonal matrix and then make it reversible. Of course, we also need to consider a kind of situation, that is, when the $\bm{S}$ matrix is ​​a sparse matrix, this will involve solving the problem of sparse matrix linear equations. Sparseness provides great convenience to SLAM, not only from the speed of the algorithm, but also from the memory space occupied by the algorithm. There are many kinds of storage of sparse matrices, the simplest of which is to save only the ranks of non-zero elements and their own values, and finally put them in a Hash table for management. However, the data structure of the sparse matrix will be customized according to the algorithm.
%
% is good at these low-level mathematical content, many existing algorithm libraries have been done quite well, such as CHOLMOD\cite{chen2008algorithm}, CSparse\cite{davis2005csparse}, such efficient Cholesky decomposition algorithm library has been used in a wide range of industries. In the world of circles and academia, even the famous software MATLAB is called. It also has an algorithmic library such as Suitsparse\cite{davis2014suitesparse} that integrates efficient algorithms for various decompositions of sparse matrices, so most people can ignore these theories. For details, it is advisable to choose to be familiar with the calls to these algorithm libraries.
%
% However, in order to facilitate the reader's familiarity with the frontier theoretical research of various SLAMs, the author still has to briefly describe the sparse matrix. When studying the decomposition operation of a sparse matrix, there is a concept called \textbf{fill-in}. This concept refers to the fact that in the position where the sparse matrix is ​​originally 0, some of the positions become elements with values ​​after they are decomposed. For example, if we directly perform Cholesky decomposition on the arrow type matrix, we will see the case of \autoref{fig:ArrowMatrix}. We can see that after the Cholesky decomposition, the arrow matrix is ​​almost completely filled with \textbf{.
%
%\begin{figure}[!htp]
% \centering
% \includegraphics[width=0.8\textwidth]{backend1/arrowMatrix.pdf}
% \caption{arrow matrix (left) and its corresponding Cholesky decomposition matrix (right), Cholesky decomposition turns the original sparse matrix into a dense matrix}
% \label{fig:ArrowMatrix}
%\end{figure}
%
% Since our Cholesky decomposition of the matrix is ​​to solve the linear equations $\bm{Sx=p}$, we can change the order of the elements in the vector $\bm{x}$. This change only needs to be $ The location of non-zero members in \bm{S}$ does not require recalculation of $\bm{S}$. For example, for the linear system $\bm{Sx=p}$ corresponding to the arrow matrix of \autoref{fig:ArrowMatrix}, we reverse the order of all variables in the entire $\bm{x}$, then the corresponding The $\bm{S}$ matrix and its Cholesky decomposition will appear as \autoref{fig:ArrowMatrixPermute}. Cholesky is still a sparse matrix after decomposition, and even the sparse pattern is exactly the same as the original matrix.
%
%\begin{figure}[!htp]
% \centering
% \includegraphics[width=0.8\textwidth]{backend1/arrowMatrixPermute.pdf}
% \caption{Change the order arrow matrix (left) and its corresponding Cholesky decomposition matrix (right), Cholesky is still sparse matrix after decomposition, and has the same sparse form as the original matrix. }
% \label{fig:ArrowMatrixPermute}
%\end{figure}
%
% is obvious, so that after sorting, it does not affect the solution of the linear equations at all, even when we do the backward or reverse substitution method, because the decomposition maintains the sparsity, it can save a lot of multiplication and addition operations and thus improve the calculation. effectiveness. At the same time, it is more attractive that we can still save the Cholesky-decomposed matrix with the original sparse matrix data structure. We don't need to open up new storage space, and we can continuously calculate and cover it directly on the original data structure. . If it is pre-sorting \eqref{fig:ArrowMatrix}, it is too bad. We need not only more memory space to save the matrix, but also need to replace the data structure of this matrix (using a hash table to deal with sparse matrices). The data structure to preserve dense matrices would be a complex and inefficient method). Therefore, in order to solve the linear equations of the sparse matrix, in order to be more efficient, we should first use the \textbf{ordering] operation to avoid excessive filling as much as possible.
%
% As for this kind of algorithm for reducing the sorting of padding, readers can refer to \cite{agarwal2012variable} due to its variety and variety. One of the most influential is the EMD algorithm (Exact Minimum Degree), which continuously selects nodes with \textbf{Degree} for the corresponding graph of the matrix to obtain the order corresponding to the matrix. However, it is worth noting that this sorting algorithm is a \textbf{NPDiff} problem (NP-complete, which is a problem that cannot be guaranteed in polynomial time). Because of this, I also need to remind readers that not all sorts can make the entire linear system faster. Therefore, in order to truly ensure the acceleration, it is recommended that the reader still run the comparison of the running time of the two to make a choice.
%
%\subsection{QR解解解Bundle Adjustment }
The solution to %Bundle Adjustment is not limited to the one we introduced above. For example, we will briefly introduce a class of methods to avoid solving the $\bm{H}$ matrix and solve the least squares directly by QR decomposition. Equation \eqref{eq:BAleastsquare}.
%
The %QR decomposition is an operation that decomposes the matrix into a unit orthogonal matrix $\bm{Q}$ and an upper triangular matrix $\bm{R}$, ie $\bm{J=QR}$. QR分解的方法也有很多，例如给定旋转(Given Rotation)，Householder 变换，Gram-Schmidt正交化等，这里不一一介绍，具体可以参考\cite{golub2012matrix}。 QR分解当中一个非常重要的定理，就是当$\bm{J}$的维度是$m \times n, (m \ge n)$，且列满秩矩阵的时候，矩阵$\bm{J}$的QR分解为如下形式：
%
%\begin{displaymath}
%\bm{J=QR} = \left( {{{\bm{Q}}_1},{{\bm{Q}}_2}} \right)\left( {\begin{array}{*{20}{c}}
%		{{\bm{R}_1}}\\
%		 \bm{0}
%		\end{array}} \right) = \bm{Q}_1 \bm{R}_1
%\end{displaymath}
%
%此时$\bm{J=Q_1 R_1}$是该分解的\textbf{紧凑形式}(thin QR decomposition)，其中 $\bm{Q_1} \in \bm{\mathbb{R}^{m \times n}}$，由一系列单位正交的列向量组成(即$\bm{Q_1^TQ_1 = I_n}$)；同时$\bm{R_1}$是维度为$n \times n$的上三角矩阵，而且对角线元素均大于0。
%
%由于$\bm{Q}$是单位正交矩阵，我们有${\left\| \bm{Q^Ty} \right\|_2} = {\left\| \bm{y} \right\|_2}$，所以我们不难推出：
%
%\begin{align*}
%\left\| \bm{Jx + f} \right\|^2 &= 
%\left\| (\bm{Q_1},\bm{Q_2})(\begin{array}{*{20}{c}}
%		\bm{R}_1\\
%		\bm{0}
%		\end{array})\bm{x} + \bm{f} \right\|^2 \\ &= 
%\left\| \left( \begin{array}{*{20}{c}}
%			\bm{Q}_1^T\\
%			\bm{Q}_2^T
%			\end{array} \right)(\bm{Q_1},\bm{Q_2})(\begin{array}{*{20}{c}}
%		\bm{R}_1\\
%		\bm{0}
%		\end{array})\bm{x} + \left( \begin{array}{*{20}{c}}
%			\bm{Q}_1\\
%			\bm{Q}_2
%			\end{array} \right)\bm{f} \right\|^2 \\ &= 
%\left\| \bm{R_1 x} + \bm{Q}_1^T\bm{f} \right\|^2 + \left\| \bm{Q}_2^T\bm{f} \right\|^2
%\end{align*}
%
%这样，我们不难看出，由于$\bm{R_1}$是上三角矩阵，该最小二乘的最优解就是用前面提到的逆向代入法求解方程组：
%
%\begin{displaymath}
%\bm{R_1 x} = -\bm{Q_1^T f}
%\end{displaymath}
%
%该方法也同样需要讲究使用前面的排序手段来保持矩阵$\bm{R}_1$的稀疏性，但由于该分解方法远远没有Schur消元后进行Cholesky分解的方法快速，现实中很少采纳。但值得一提的是，QR分解却是比Cholesky分解数值上更加稳定的方法。
%
%线性系统的稳定性是实战中不可忽略的分析部分，因为由于处理器精度的限制(例如双精度浮点数只有16位有效数字，则精度的数量级是$10^{-16}$)，有的低端处理器又只能让你使用单精度浮点数，因此任何情况下我们对矩阵的计算都存在截断误差。这种截断误差对矩阵的影响，可以说是对矩阵的一种\textbf{扰动}，我们并不希望这类扰动对最后的求解结果带来太大影响。具体的分析需要用到矩阵的\textbf{条件数}(Condition number)，这又是一些复杂的数学内容，我推荐希望深入的读者参考书籍\cite{golub2012matrix}。但需要我们记住的是，相比前面提到的Cholesky分解而言，QR分解是个速度慢但是数值上更稳定的选择。

\subsection{Robust kernel function}
In the previous BA problem, we minimize the sum of the squares of the two norms of the error term as the objective function. Although this method is very intuitive, there is a serious problem: If the data given by an error item is wrong for reasons such as mismatching, what will happen? We add an edge that should not be added to the graph. However, the optimization algorithm does not recognize that this is an erroneous data. It treats all data as errors. This data, in the opinion of the algorithm, is observed with a small probability of seeing the data. At this point, there will be a very large margin in the graph optimization, and its gradient is also large, meaning that adjusting the variables associated with it will cause the objective function to drop more. Therefore, the algorithm will attempt to prioritize the estimates of the nodes to which this edge is connected, so that they conform to the unreasonable requirements of this edge. Since the error on this side is really large, it tends to smooth out the effects of other correct edges, allowing the optimization algorithm to focus on adjusting an incorrect value. This is obviously not what we want to see.

The reason for this problem is that the two norms grow too fast when the error is large. Then there is the existence of a nuclear function. The kernel function guarantees that the error on each side will not be too large to cover up the other edges. The specific way is to replace the two norm metric of the original error with a function that does not grow so fast, while at the same time guaranteeing its own smooth nature (otherwise it can't be derived!). Because they make the whole optimization result more robust, they are also called Robust Kernel.

There are many types of robust kernel functions, such as the most commonly used Huber kernel:
\begin{equation}
H\left( e \right) = 
\left\{ 
\begin{array}{ll}
\frac{1}{2}{e^2} &\quad \text{当} |e| \leqslant \delta, \\
\delta \left( {\left| e \right| - \frac{1}{2}\delta } \right) &\quad \text{其他}
\end{array} \right.
\end{equation}

We see that when the error $e$ is greater than a certain threshold of $\delta$, the function growth changes from a quadratic form to a form, which is equivalent to limiting the maximum value of the gradient. At the same time, the Huber kernel function is smooth and can be easily derived. \autoref{fig:huber}~ shows the comparison between the Huber kernel function and the quadratic function. It can be seen that the Huber kernel function grows significantly lower than the quadratic function when the error is large.

\begin{figure}[!htp]
\centering
\includegraphics[width=0.5\textwidth]{backend1/huberloss}
\caption{Huber kernel function. }
\label{fig:huber}
\end{figure}

In addition to the Huber core, there are Cauchy kernels, Tukey cores, etc. Readers can see which kernel functions are provided by both g2o and Ceres.

\subsection{小结}
In this section we focus on sparsity issues in BA. However, in practice, most software libraries have implemented detailed operations for us, and what we need to do is mainly to construct the Bundle Adjustment problem, set the Schur elimination, and then call the dense or sparse matrix solver to optimize the variables. If you want to know more about BA, you can read the \cite{Triggs2000} reference on the basis of reading this section.

In the next two sections, we will use the two libraries of Ceres and g2o to do the Bundle Adjustment. In order to reflect their differences, we will use a public data set BAL\textsubscript{\cite{bundleadjustmentinlarge}} and use the shared read and write code.

\section{Practice: Ceres BA}
\subsection{BALDataset}
We used the BAL data set for the demonstration of BA. The BAL data set provides several scenes, and the camera and landmark information in each scene is given by a text file. In this example, use the file problem-16-22106-pre.txt as an example. The file stores the information of the BA problem in a row. See the detailed format. We read the contents of the file using the BALProblem class defined in common.h and then solve it with Ceres and g2o respectively.

It should be noted that the BAL data set has some special features:
\begin{enumerate}
    \item The camera internal parameter model of  BAL is given by the focal length $f$ and the distortion parameters $k_1, k_2$. $f$ is similar to the $f_x$ and $f_y$ we mentioned. Since the photo pixels are basically square, in many practical situations $f_x$ is very close to $f_y$, and it is not a bad idea to use the same value. In addition, there is no $c_x, c_y$ in this model, because the stored data has already removed these two values.
    \item The BAL data is projected with the projection plane behind the camera's optical center, so we need to multiply the coefficient by $-1$ after projection, as calculated by our previous model. However, most datasets still use the projection plane in front of the optical center, and we should read the format specification carefully before using the dataset.
\end{enumerate}

After reading the data with the BALProblem class, we can call the Normalize function to normalize the original data, or add noise to the data through the Perturb function. Normalization refers to zeroing the center of all landmark points and then scaling them to a suitable scale. This will make the values ​​more stable during the optimization process, preventing BA problems that are large or have large offsets in extreme cases.

Readers are advised to read the other interfaces of the BALProblem class. Since these codes are only responsible for reading and writing peripheral functions such as data, we don't give them in the text to save space. After solving the BA, we can also use the function of this class to write the result to a ply file (a point cloud file format), and then use the meshlab software to view. Meshlab can be installed by apt-get, and the installation method will not be described here.

\subsection{Ceres BA Writing}
In the bundle\_adjustment\_ceres.cpp file, we implemented the process of Ceres solving BA. The key to using Ceres is to define the projection error model, which is given in SnavelyReprojectionError.h:

\begin{lstlisting}[language=c++, caption=slambook2/ch9/SnavelyReprojectionError.cpp（片段）]
class SnavelyReprojectionError {
public:
	SnavelyReprojectionError(double observation_x, double observation_y) : observed_x(observation_x),
	observed_y(observation_y) {}
	
	template<typename T>
	bool operator()(const T *const camera,
		const T *const point,
		T *residuals) const {
		// camera[0,1,2] are the angle-axis rotation
		T predictions[2];
		CamProjectionWithDistortion(camera, point, predictions);
		residuals[0] = predictions[0] - T(observed_x);
		residuals[1] = predictions[1] - T(observed_y);
		
		return true;
	}
	
	// camera : 9 dims array
	// [0-2] : angle-axis rotation
	// [3-5] : translation
	// [6-8] : camera parameter, [6] focal length, [7-8] second and forth order radial distortion
	// point : 3D location.
	// predictions : 2D predictions with center of the image plane.
	template<typename T>
	static inline bool CamProjectionWithDistortion(const T *camera, const T *point, T *predictions) {
		// Rodrigues' formula
		T p[3];
		AngleAxisRotatePoint(camera, point, p);
		// camera[3,4,5] are the translation
		p[0] += camera[3];
		p[1] += camera[4];
		p[2] += camera[5];
		
		// Compute the center fo distortion
		T xp = -p[0] / p[2];
		T yp = -p[1] / p[2];
		
		// Apply second and fourth order radial distortion
		const T &l1 = camera[7];
		const T &l2 = camera[8];
		
		T r2 = xp * xp + yp * yp;
		T distortion = T(1.0) + r2 * (l1 + l2 * r2);
		
		const T &focal = camera[6];
		predictions[0] = focal * distortion * xp;
		predictions[1] = focal * distortion * yp;
		
		return true;
	}
	
	static ceres::CostFunction *Create(const double observed_x, const double observed_y) {
		return (new ceres::AutoDiffCostFunction<SnavelyReprojectionError, 2, 9, 3>(
			new SnavelyReprojectionError(observed_x, observed_y)));
	}
	
private:
	double observed_x;
	double observed_y;
};
\end{lstlisting}
The parentheses operator of this class implements the interface for Ceres calculation error, and the actual calculation is in the CamProjectionWithDistortion function. Note that in Ceres, we must store the optimization variables as a double array. Now each camera has a total of 6-dimensional pose, 1D focal length and 2D distortion parameters, which are described by a total of 9-dimensional parameters. We must also store them in this order in actual storage. The static function Create of this class acts as an external call interface and directly returns a Ceres cost function that can be automatically derived. We just call the Create function and put the cost function into ceres::Problem.

Next we implement the part of BA building and solving:：

\begin{lstlisting}[language=c++, caption=slambook2/ch9/SnavelyReprojectionError.cpp（片段）]
void SolveBA(BALProblem &bal_problem) {
	const int point_block_size = bal_problem.point_block_size();
	const int camera_block_size = bal_problem.camera_block_size();
	double *points = bal_problem.mutable_points();
	double *cameras = bal_problem.mutable_cameras();
	
	// Observations is 2 * num_observations long array observations
	// [u_1, u_2, ... u_n], where each u_i is two dimensional, the x
	// and y position of the observation.
	const double *observations = bal_problem.observations();
	ceres::Problem problem;
	
	for (int i = 0; i < bal_problem.num_observations(); ++i) {
		ceres::CostFunction *cost_function;
		
		// Each Residual block takes a point and a camera as input
		// and outputs a 2 dimensional Residual
		cost_function = 
			SnavelyReprojectionError::Create(observations[2 * i + 0], observations[2 * i + 1]);
		
		// If enabled use Huber's loss function.
		ceres::LossFunction *loss_function = new ceres::HuberLoss(1.0);
		
		// Each observation corresponds to a pair of a camera and a point
		// which are identified by camera_index()[i] and point_index()[i]
		// respectively.
		double *camera = cameras + camera_block_size * bal_problem.camera_index()[i];
		double *point = points + point_block_size * bal_problem.point_index()[i];
		
		problem.AddResidualBlock(cost_function, loss_function, camera, point);
	}

// Observations is 2 * num_observations long array observations
// [u_1, u_2, ... u_n], where each u_i is two dimensional, the x
// and y position of the observation.
Const double *observations = bal_problem.observations();
Ceres::Problem problem;

	std::cout << "Solving ceres BA ... " << endl;
	ceres::Solver::Options options;
	options.linear_solver_type = ceres::LinearSolverType::SPARSE_SCHUR;
	options.minimizer_progress_to_stdout = true;
	ceres::Solver::Summary summary;
	ceres::Solve(options, &problem, &summary);
	std::cout << summary.FullReport() << "\n";
}
\end{lstlisting}
Visible problem building part is quite simple. If you want to add another cost function, the whole process will not change much. Finally, in ceres::Solver::Options, we can set the solution method. The use of SPARSE\_SCHUR will allow Ceres to actually solve the same process as we described earlier, that is, Schur marginalization of the road sign portion to solve this problem in an accelerated manner. However, in Ceres we can't control which parts of the variable are marginalized, which is automatically found and calculated by the Ceres solver.

Ceres' BA solution output is as follows:
\begin{lstlisting}[language=sh,caption=terminal output]
./build/bundle_adjustment_ceres problem-16-22106-pre.txt
Header: 16 22106 83718bal problem file loaded...
bal problem have 16 cameras and 22106 points. 
Forming 83718 observations. 
Solving ceres BA ... 
iter      cost      cost_change  |gradient|   |step|    tr_ratio  tr_radius  ls_iter  iter_time  total_time
0  1.842900e+07    0.00e+00    2.04e+06   0.00e+00   0.00e+00  1.00e+04        0    6.10e-02    2.24e-01
1  1.449093e+06    1.70e+07    1.75e+06   2.16e+03   1.84e+00  3.00e+04        1    1.79e-01    4.03e-01
2  5.848543e+04    1.39e+06    1.30e+06   1.55e+03   1.87e+00  9.00e+04        1    1.56e-01    5.59e-01
3  1.581483e+04    4.27e+04    4.98e+05   4.98e+02   1.29e+00  2.70e+05        1    1.51e-01    7.10e-01
......
\end{lstlisting}
The overall error should continue to decrease as the number of iterations increases. Finally, we will optimize the pre-optimized and optimized point cloud output as initial.ply and final.ply, and use meshlab to open these two point clouds directly. The resulting graph is shown in \autoref{fig:g2o-BA}.

\section{Practice: g2o solves BA}
Let's consider how to solve this BA problem using g2o. As before, g2o uses a graph model to describe the structure of the problem, so we use nodes to represent the camera and road signs, and then use edges to represent the observations between them. We still use custom points and edges, just cover some key functions. For cameras and road signs, we can define the following structure and use the override keyword to represent the coverage of the base class virtual function:
\begin{lstlisting}[language=c++,caption=slambook2/ch9/bundle_adjustment_g2o.cpp(fragment)]
/// 姿态和内参的结构
struct PoseAndIntrinsics {
	PoseAndIntrinsics() {}
	
	/// set from given data address
	explicit PoseAndIntrinsics(double *data_addr) {
		rotation = SO3d::exp(Vector3d(data_addr[0], data_addr[1], data_addr[2]));
		translation = Vector3d(data_addr[3], data_addr[4], data_addr[5]);
		focal = data_addr[6];
		k1 = data_addr[7];
		k2 = data_addr[8];
	}
	
	/// 将估计值放入内存
	void set_to(double *data_addr) {
		auto r = rotation.log();
		for (int i = 0; i < 3; ++i) data_addr[i] = r[i];
		for (int i = 0; i < 3; ++i) data_addr[i + 3] = translation[i];
		data_addr[6] = focal;
		data_addr[7] = k1;
		data_addr[8] = k2;
	}
	
	SO3d rotation;
	Vector3d translation = Vector3d::Zero();
	double focal = 0;
	double k1 = 0, k2 = 0;
};

/// 位姿加相机内参的顶点，9维，前三维为so3，接下去为t, f, k1, k2
class VertexPoseAndIntrinsics : public g2o::BaseVertex<9, PoseAndIntrinsics> {
public:
	EIGEN_MAKE_ALIGNED_OPERATOR_NEW;
	
	VertexPoseAndIntrinsics() {}
	
	virtual void setToOriginImpl() override {
		_estimate = PoseAndIntrinsics();
	}
	
	virtual void oplusImpl(const double *update) override {
		_estimate.rotation = SO3d::exp(Vector3d(update[0], update[1], update[2])) * _estimate.rotation;
		_estimate.translation += Vector3d(update[3], update[4], update[5]);
		_estimate.focal += update[6];
		_estimate.k1 += update[7];
		_estimate.k2 += update[8];
	}
	
	/// 根据估计值投影一个点
	Vector2d project(const Vector3d &point) {
		Vector3d pc = _estimate.rotation * point + _estimate.translation;
		pc = -pc / pc[2];
		double r2 = pc.squaredNorm();
		double distortion = 1.0 + r2 * (_estimate.k1 + _estimate.k2 * r2);
		return Vector2d(_estimate.focal * distortion * pc[0],
		_estimate.focal * distortion * pc[1]);
	}
	
	virtual bool read(istream &in) {}
	
	virtual bool write(ostream &out) const {}
};

class VertexPoint : public g2o::BaseVertex<3, Vector3d> {
public:
	EIGEN_MAKE_ALIGNED_OPERATOR_NEW;
	
	VertexPoint() {}
	
	virtual void setToOriginImpl() override {
		_estimate = Vector3d(0, 0, 0);
	}
	
	virtual void oplusImpl(const double *update) override {
		_estimate += Vector3d(update[0], update[1], update[2]);
	}
	
	virtual bool read(istream &in) {}
	
	virtual bool write(ostream &out) const {}
};

class EdgeProjection :
public g2o::BaseBinaryEdge<2, Vector2d, VertexPoseAndIntrinsics, VertexPoint> {
public:
	EIGEN_MAKE_ALIGNED_OPERATOR_NEW;
	
	virtual void computeError() override {
		auto v0 = (VertexPoseAndIntrinsics *) _vertices[0];
		auto v1 = (VertexPoint *) _vertices[1];
		auto proj = v0->project(v1->estimate());
		_error = proj - _measurement;
	}
	
	// use numeric derivatives
	virtual bool read(istream &in) {}
	
	virtual bool write(ostream &out) const {}
};
\end{lstlisting}

We define the rotation, translation, focus, and distortion parameters in the same camera vertex, and then define the observation edge between the camera and the landmark. Here we do not implement the edge Jacobian calculation function, so g2o will automatically provide a numerical calculation of Jacobian. Finally, according to the data in the BAL, the g2o optimization problem can be built up:
\begin{lstlisting}[language=c++,caption=slambook2/ch9/bundle_adjustment_g2o.cpp(fragment)]
void SolveBA(BALProblem &bal_problem) {
	const int point_block_size = bal_problem.point_block_size();
	const int camera_block_size = bal_problem.camera_block_size();
	double *points = bal_problem.mutable_points();
	double *cameras = bal_problem.mutable_cameras();
	
	// pose dimension 9, landmark is 3
	typedef g2o::BlockSolver<g2o::BlockSolverTraits<9, 3>> BlockSolverType;
	typedef g2o::LinearSolverCSparse<BlockSolverType::PoseMatrixType> LinearSolverType;
	// use LM
	auto solver = new g2o::OptimizationAlgorithmLevenberg(
	g2o::make_unique<BlockSolverType>(g2o::make_unique<LinearSolverType>()));
	g2o::SparseOptimizer optimizer;
	optimizer.setAlgorithm(solver);
	optimizer.setVerbose(true);
	
	/// build g2o problem
	const double *observations = bal_problem.observations();
	// vertex
	vector<VertexPoseAndIntrinsics *> vertex_pose_intrinsics;
	vector<VertexPoint *> vertex_points;
	for (int i = 0; i < bal_problem.num_cameras(); ++i) {
		VertexPoseAndIntrinsics *v = new VertexPoseAndIntrinsics();
		double *camera = cameras + camera_block_size * i;
		v->setId(i);
		v->setEstimate(PoseAndIntrinsics(camera));
		optimizer.addVertex(v);
		vertex_pose_intrinsics.push_back(v);
	}
	for (int i = 0; i < bal_problem.num_points(); ++i) {
		VertexPoint *v = new VertexPoint();
		double *point = points + point_block_size * i;
		v->setId(i + bal_problem.num_cameras());
		v->setEstimate(Vector3d(point[0], point[1], point[2]));
		// g2o在BA中需要手动设置待Marg的顶点
		v->setMarginalized(true);
		optimizer.addVertex(v);
		vertex_points.push_back(v);
	}
	
	// edge
	for (int i = 0; i < bal_problem.num_observations(); ++i) {
		EdgeProjection *edge = new EdgeProjection;
		edge->setVertex(0, vertex_pose_intrinsics[bal_problem.camera_index()[i]]);
		edge->setVertex(1, vertex_points[bal_problem.point_index()[i]]);
		edge->setMeasurement(Vector2d(observations[2 * i + 0], observations[2 * i + 1]));
		edge->setInformation(Matrix2d::Identity());
		edge->setRobustKernel(new g2o::RobustKernelHuber());
		optimizer.addEdge(edge);
	}
	
	optimizer.initializeOptimization();
	optimizer.optimize(40);
	
	// set to bal problem
	for (int i = 0; i < bal_problem.num_cameras(); ++i) {
		double *camera = cameras + camera_block_size * i;
		auto vertex = vertex_pose_intrinsics[i];
		auto estimate = vertex->estimate();
		estimate.set_to(camera);
	}
	for (int i = 0; i < bal_problem.num_points(); ++i) {
		double *point = points + point_block_size * i;
		auto vertex = vertex_points[i];
		for (int k = 0; k < 3; ++k) point[k] = vertex->estimate()[k];
	}
}
\end{lstlisting}

The above defines the nodes and edges used in this question. Next, we need to generate some nodes and edges based on the actual data in the BALProblem class, and give them to g2o for optimization. It's worth noting that in order to take full advantage of the sparsity in BA, you need to set the setMarginalized property in the roadmap to true here. The main fragments of the code are as follows:
\begin{lstlisting}[language=c++,caption=slambook2/ch9/bundle_adjustment_g2o(fragment)]
void SolveBA(BALProblem &bal_problem) {
	const int point_block_size = bal_problem.point_block_size();
	const int camera_block_size = bal_problem.camera_block_size();
	double *points = bal_problem.mutable_points();
	double *cameras = bal_problem.mutable_cameras();
	
	// pose dimension 9, landmark is 3
	typedef g2o::BlockSolver<g2o::BlockSolverTraits<9, 3>> BlockSolverType;
	typedef g2o::LinearSolverCSparse<BlockSolverType::PoseMatrixType> LinearSolverType;
	// use LM
	auto solver = new g2o::OptimizationAlgorithmLevenberg(
	g2o::make_unique<BlockSolverType>(g2o::make_unique<LinearSolverType>()));
	g2o::SparseOptimizer optimizer;
	optimizer.setAlgorithm(solver);
	optimizer.setVerbose(true);
	
	/// build g2o problem
	const double *observations = bal_problem.observations();
	// vertex
	vector<VertexPoseAndIntrinsics *> vertex_pose_intrinsics;
	vector<VertexPoint *> vertex_points;
	for (int i = 0; i < bal_problem.num_cameras(); ++i) {
		VertexPoseAndIntrinsics *v = new VertexPoseAndIntrinsics();
		double *camera = cameras + camera_block_size * i;
		v->setId(i);
		v->setEstimate(PoseAndIntrinsics(camera));
		optimizer.addVertex(v);
		vertex_pose_intrinsics.push_back(v);
	}
	for (int i = 0; i < bal_problem.num_points(); ++i) {
		VertexPoint *v = new VertexPoint();
		double *point = points + point_block_size * i;
		v->setId(i + bal_problem.num_cameras());
		v->setEstimate(Vector3d(point[0], point[1], point[2]));
		// g2o在BA中需要手动设置待Marg的顶点
		v->setMarginalized(true);
		optimizer.addVertex(v);
		vertex_points.push_back(v);
	}
	
	// edge
	for (int i = 0; i < bal_problem.num_observations(); ++i) {
		EdgeProjection *edge = new EdgeProjection;
		edge->setVertex(0, vertex_pose_intrinsics[bal_problem.camera_index()[i]]);
		edge->setVertex(1, vertex_points[bal_problem.point_index()[i]]);
		edge->setMeasurement(Vector2d(observations[2 * i + 0], observations[2 * i + 1]));
		edge->setInformation(Matrix2d::Identity());
		edge->setRobustKernel(new g2o::RobustKernelHuber());
		optimizer.addEdge(edge);
	}
	
	optimizer.initializeOptimization();
	optimizer.optimize(40);
	
	// set to bal problem
	for (int i = 0; i < bal_problem.num_cameras(); ++i) {
		double *camera = cameras + camera_block_size * i;
		auto vertex = vertex_pose_intrinsics[i];
		auto estimate = vertex->estimate();
		estimate.set_to(camera);
	}
	for (int i = 0; i < bal_problem.num_points(); ++i) {
		double *point = points + point_block_size * i;
		auto vertex = vertex_points[i];
		for (int k = 0; k < 3; ++k) point[k] = vertex->estimate()[k];
	}
}
\end{lstlisting}
One big difference between g2o and Ceres is that when using sparse optimization, g2o must manually set which vertices are marginalized vertices, otherwise a runtime error will be reported (the reader can try to comment out the line v->setMarginalized(true)). The rest of the place and the Ceres experiment are similar, we will not introduce more. The g2o experiment also outputs a point cloud before and after optimization for comparison.

\begin{figure}[!htp]
\centering
\includegraphics[width=1.0\textwidth]{backend1/g2o-ba}
\caption{Visualization point cloud before and after optimization. The left side is the initial value before optimization, and the right side is optimized. }
\label{fig:g2o-BA}
\end{figure}

\section{小结}
This lecture explores the problem of state estimation and graph optimization in depth. We see that SLAM can be seen as a state estimation problem in the classical model. If we assume Markov properties and only consider the current state, we get a filter model represented by EKF. If not, we can also choose to consider all motions and observations, which constitute a least squares problem. In the case of only observation equations, this problem is called BA and can be solved using nonlinear optimization methods. We have carefully discussed the sparsity problem in the solution process and pointed out the connection between the problem and the graph optimization. Finally, we demonstrate how to use the g2o and Ceres libraries to solve the same optimization problem, giving the reader an intuitive understanding of BA.

\section*{ Exercises}
\begin{enumerate}
    \item The  proof \eqref{eq:kalman-K-another} is established. Hint: You might use the SMW (Sherman-Morrison-Woodbury) formula, reference \cite{Sherman1950, Barfoot2016}.
    \item Compare the values ​​of the optimized objective function of g2o and Ceres. Point out why the two have the same effect in Meshlab but the values ​​are different.
    \item Perform a Schur elimination on some of the point clouds in Ceres to see what the difference is.
    \item proves that the $\bm{S}$ matrix is ​​a semi-positive matrix.
    \item Read the documentation \cite{Kummerle2011} and see how g2o handles kernel functions. How does it relate to the Loss function in Ceres?
    \item[\optional] In both examples, we optimized the camera pose, camera coordinates and landmarks with $f, k_1, k_2$ as parameters. Consider optimizing with the full camera model described in Lecture 5, ie, consider at least $f_x, f_y, p_1, p_2, $ $k_1, k_2$. Modify the current Ceres and g2o programs to complete the experiment.
\end{enumerate}
