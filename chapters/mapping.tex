656%! Mode :: "TeX: UTF-8"
\chapter{Build map}
\begin{mdframed}
\textbf{main target}
\begin{enumerate}[labelindent=0em, leftmargin=1.5em]
\item Understand the principle of dense depth estimation in monocular SLAM.
\item Experimentally understand the process of monocular dense reconstruction.
\item Learn about several map formats in RGB-D reconstruction.
\end{enumerate}
\end{mdframed}

In this lecture we start to introduce the algorithm of the mapping part. In the front-end and back-end, we focus on the problem of simultaneously estimating the camera motion trajectory and the spatial position of feature points. However, when SLAM is actually used, in addition to positioning the camera body, there are many other requirements. For example, consider SLAM placed on a robot, then we would like the map to be used for positioning, navigation, obstacle avoidance, and interaction. The feature point map obviously cannot meet all these needs. Therefore, in this lecture, we will discuss various forms of maps in more detail, and point out the current flaws in visual SLAM maps.

\newpage
\section{Overview}
Mapping should be one of the two goals of SLAM-because SLAM is called simultaneous positioning and mapping. But until now, we have discussed positioning issues, including positioning by feature points, direct method, and back-end optimization. So, does this suggest that mapping is not so important in SLAM, so we didn't discuss it until this lecture?

the answer is negative. In fact, in the classic SLAM model, what we call a map is a collection of all landmark points. Once the location of the landmarks is determined, it can be said that we have completed the mapping. Therefore, the visual odometer mentioned above is good, as is the Bundle Adjustment. In fact, the positions of the landmark points are modeled and optimized. From this perspective, we have discussed the problem of mapping. So why bother with a separate picture?

This is because people have different needs for building maps. As a low-level technology, SLAM is often used to provide information for upper-level applications. If the upper layer is a robot, then the developers at the application layer may want to use SLAM to do global positioning and let the robot navigate in the map-for example, the sweeper needs to complete the sweeping work and wants to calculate a path that covers the entire map. Or, if the upper layer is an augmented reality device, then the developer may wish to superimpose the virtual object on the real object, and in particular, may also need to handle the occlusion relationship between the virtual object and the real object.

We found that the requirements for "positioning" at the application level are similar, and they want SLAM to provide the spatial pose information of the camera or the subject with the camera. For maps, there are many different needs. From the perspective of visual SLAM, "mapping" serves "positioning"; but at the application level, "mapping" obviously carries many other requirements. Regarding the usefulness of the map, we can roughly summarize it as follows:

\begin{enumerate}
\item \textbf{targeting}. Positioning is a basic function of maps. In the previous visual odometer section, we discussed how to use local maps to achieve positioning. In the loop detection part, we also saw that as long as there is global descriptor information, we can also determine the position of the robot through loop detection. Furthermore, we also hope to save the map so that the robot can still locate in the map after the next boot, so that only the model of the map needs to be modeled once, instead of re-doing a complete SLAM every time the robot is started.
\item \textbf{navigation}. Navigation refers to the process by which a robot can plan a path in a map, find a path between any two map points, and then control its movement to a target point. In the process, we need to know at least \textbf{where in the map is not passable and which is passable}. This is beyond the capabilities of the sparse feature point map, we must have another map form. We will say later that this must be at least a \textbf{dense} map.
\item \textbf{Avoidance}. Obstacle avoidance is also a problem that robots often encounter. It is similar to navigation, but focuses more on the handling of local, dynamic obstacles. Similarly, with only feature points, we cannot determine whether a feature point is an obstacle, so we need a \textbf{dense} map.
\item \textbf{rebuild}. Sometimes, we want to use SLAM to obtain the reconstruction effect of the surrounding environment. This kind of map is mainly used to show people, so we want it to look more comfortable and beautiful. Alternatively, we can also use the map for communication so that others can remotely view the 3D objects or scenes we have reconstructed-such as 3D video calls or online shopping. This map is also \textbf{dense}, and we also have some requirements for its appearance. We may not be satisfied with the reconstruction of dense point clouds, but prefer to be able to construct textured planes, just like 3D scenes in video games.
\item \textbf{interactive}. Interaction mainly refers to the interaction between people and the map. For example, in augmented reality, we would place virtual objects in the room and interact with these virtual objects-for example, I would click on a virtual web browser on the wall to watch the video, or Throw objects on the wall, hoping they have (virtual) physical collisions. On the other hand, there will be interactions with people and maps in robot applications. For example, the robot may receive the command "take the newspaper on the table". In addition to the environment map, the robot also needs to know which map is the "table", what is called "above" and what is called "above" newspaper". This requires robots to have a higher level of understanding of maps-also known as semantic maps.
\end{enumerate}

\autoref{fig: maps} ~ vividly explains the relationship between the various map types and uses discussed above. Our previous discussion has basically focused on the "sparse roadmap map" section, and we have not discussed dense maps. The so-called dense map is relative to the sparse map. The sparse map only models the part of interest, that is, the feature points (signpost points) that have been said for a long time. A dense map is a model that \textbf{all} has seen. For the same table, a sparse map might model only the four corners of the table, while a dense map models the entire desktop. Although from a positioning perspective, a map with only four corners can also be used to locate the camera, but because we cannot infer the spatial structure between these points from the four corners, we cannot use only four corners to complete the navigation. , Obstacle avoidance, and other tasks that require dense maps to complete.

\begin{figure}[! ht]
\centering
\includegraphics[width=1.0 \textwidth]{mapping/maps.pdf}
\caption{A schematic of various maps. The three example maps are from the literature \cite{Mur-Artal2015, Labbe2014, Salas-Moreno2013}. }
\label{fig: maps}
\end{figure}

As can be seen from the discussion above, dense maps occupy a very important place. So the remaining question is: Can dense maps be created with visual SLAM? If so, how to build it?

\section{Monocular dense reconstruction}
\subsection{Stereovision}
The problem of dense reconstruction of visual SLAM is the first important topic of this lecture. Cameras have long been considered as angle-only sensors. The pixels in a single image can only provide the angle between the object and the imaging plane of the camera and the brightness collected by the object, but cannot provide the distance of the object. In dense reconstruction, we need to know the distance of each pixel (or most pixels). There are roughly the following solutions:

\begin{enumerate}
\item Use a monocular camera to measure the distance of pixels by triangulating after moving the camera.
\item Use a binocular camera to calculate the distance between pixels using the parallax of left and right (the principle of multi-eye is the same).
\item Use RGB-D camera to get pixel distance directly.
\end{enumerate}

The first two methods are called Stereo Vision, and the moving single purpose is also called moving view stereo (Moving View Stereo). Compared to the depth directly measured by RGB-D, the depth acquisition of monocular and binocular is often "laborous"-we need to spend a lot of calculations and finally get some unreliable \footnote{officially called Fragile. } Depth estimation. Of course, RGB-D also has some range, application range, and lighting limitations, but compared to monocular and binocular results, dense reconstruction using RGB-D is often a more common choice. The advantage of monocular and binocular is that it can still estimate depth information through stereo vision in outdoor and large scene applications where RGB-D cannot be applied well.

Having said that, in this section we will lead the reader through a single-purpose dense estimation and experience why it is laborious and unpleasant. Let's start with the simplest case: how to estimate the depth of an image based on a given video sequence over a period of time. In other words, instead of considering SLAM, let's consider a slightly simpler mapping problem.

Suppose there is a video sequence, and we get the trajectory of each frame through some magic (of course it is also likely to be estimated by the visual odometry front end). Now we take the first image as the reference frame and calculate the depth (or distance) of each pixel in the reference frame. First, please remember how we completed the process in the feature points section:

\begin{enumerate}
\item First, we extract features from the image and calculate the matches between the features based on the descriptor. In other words, through the feature, we tracked a certain spatial point and knew its position between the various images.
\item Then, because we can't use only one image to determine the location of a feature point, we must estimate its depth from observations at different perspectives, the principle is the triangulation described earlier.
\end{enumerate}

In dense depth map estimation, the difference is that we cannot treat each pixel as a feature point calculation descriptor. Therefore, matching becomes a very important part of the dense depth estimation problem: how to determine where a certain pixel of the first image appears in other images? This requires \textbf{polar search} and \textbf{block matching technology} \textsuperscript{\cite{Pizzoli2014}}. Then, when we know the position of a pixel in each picture, we can use triangulation to determine its depth like a feature point. The difference is that here we will use many triangulations to converge the depth estimation, not just one. We hope that the depth estimation will gradually converge to a stable value from a very uncertain quantity as the measurement increases. This is \textbf{depth filter technology}. So, the following content will mainly focus on this topic.

\subsection{Epipolar search and block matching}

Let's first explore the geometric relationship of observing the same point from different perspectives. This is very much like the epipolar geometry discussed in section ~ \ref{sec: epipolar-geometry} ~. See \autoref{fig: epipolar-line-search} ~. The camera on the left observes a pixel $\bm{p}_1 $. Since this is a monocular camera, we have no way of knowing its depth, so assuming that this depth may be within a certain area, let's say that it is between a certain minimum and infinity: $(d_ \mathrm{min}, + \infty) $. Therefore, the spatial points corresponding to this pixel are distributed on a certain line segment (ray in this example). From another perspective (right camera), the projection of this line segment also forms a line on the image plane, which we know is called \textbf{polar line}. This epipolar line can also be determined when the motion between the two cameras is known. Conversely, if the motion is unknown, the epipolar line cannot be determined. So the question is: which point on the polar line is the $\bm{p}_1 $point we just saw?

\begin{figure}[! htp]
\centering
\includegraphics[width=.8 \textwidth]{mapping/epipolar-line-search.pdf}
\caption{Epipolar search diagram. }
\label{fig: epipolar-line-search}
\end{figure}

Repeatedly, in the feature point method, we found the position of $\bm{p}_2 $through feature matching. However, now we have no descriptors, so we can only search for points that are similar to $\bm{p}_1 $. To be more specific, we may walk from one end to the other of the epipolar line in the second image and compare the similarity of each pixel with $\bm{p}_1 $one by one. From the perspective of directly comparing pixels, this approach is quite similar to the direct method.

In the discussion of the direct method, we know that comparing the brightness value of a single pixel is not necessarily stable and reliable. One very obvious thing is: in case there are many similar points on the pole line to $\bm{p}_1 $, how do we determine which one is true? This seems to return to the question we mentioned in loop detection: how to determine the similarity of two images (or two points)? Loop detection is solved by the bag of words, but because there are no features here, we have to find another way.

An intuitive idea is: Since the brightness of a single pixel is indistinguishable, is it possible to compare pixel blocks? We take a small block of $w \times w $around $\bm{p}_1 $, and then take a lot of small blocks of the same size for comparison on the polar line, which can improve the discrimination to a certain extent . This is called \textbf{block matching}. Note that in this process, this comparison is only meaningful if it assumes that the gray value of the entire small block is constant between different images. So the assumption of the algorithm has changed from the invariance of the gray level of the pixel to the invariance of the gray level of the image block-to a certain extent, it becomes stronger.

OK, now we have taken the small pieces around $\bm{p}_1 $, and we have taken many small pieces on the polar line. May wish to write the small blocks around $\bm{p}_1 $as $\bm{A} \in \mathbb{R} ^{w \times w} $, and write down the $n $small blocks on the pole Into $\bm{B}_i, i=1, \cdots, n $. So, how to calculate the difference between small blocks? There are several different calculation methods:

\begin{enumerate}
\item SAD (Sum of Absolute Difference). As the name suggests, the absolute value of the difference between two small blocks is summed:
\begin{equation}
S (\bm{A}, \bm{B})_{\mathrm{SAD}}=\sum_{i, j} | \bm{A} (i, j)-\bm{B} (i, j) |.
\end{equation}
\item SSD. The SSD here does not refer to the solid-state drive that everyone is familiar with, but the meaning of Sum of Squared Distance:
\begin{equation}
S (\bm{A}, \bm{B})_{\mathrm{SSD}}=\sum_{i, j} \left (\bm (A) (i, j)-\bm{B} ( i, j) \right) ^ 2.
\end{equation}
\item NCC (Normalized Cross Correlation). This method is more complicated than the first two, it calculates the correlation between two small blocks:
\begin{equation}
S (\bm{A}, \bm{B})_{\mathrm{NCC}}=\frac{{\sum \limits_{i, j}{\bm{A} (i, j) \bm{ B} (i, j)}}}{{\sqrt{\sum \limits_{i, j}{\bm{A}{{(i, j)} ^ 2} \sum \limits_{i, j}{\bm{B}{{(i, j)} ^ 2}}}}}}.
\end{equation}
Note that because correlation is used here, a correlation close to 0 indicates that the two images are not similar, and a close to 1 indicates similarity. The first two distances are reversed. A value close to 0 indicates similarity, while a large value indicates dissimilarity.
\end{enumerate}

As in many cases we have encountered, these calculations often have a contradiction between accuracy and efficiency. Good-precision methods often require complex calculations, while simple fast algorithms often do not work well. This requires us to make trade-offs in actual engineering. In addition, in addition to these simple versions, we can \textbf{remove the average of each small block first}, called SSD with average, NCC with average, and so on. After removing the mean value, we allow situations like "the small block $\bm{B} $is brighter than $\bm{A} $as a whole, but still very similar" \footnote{whole overall may be brightened by ambient light Or the camera exposure parameter is increased. }, So it's more reliable than before. If the reader is interested in more block matching measurement methods, it is recommended to read the article \cite{stereo-matching-website, Hirschmuller2007} as supplementary material.

Now, we have calculated the similarity measure between $\bm{A} $and each $\bm{B}_i $on the epipolar line. For the sake of description, suppose we use NCC, then we will get an NCC distribution along the epipolar line. The shape of this distribution depends heavily on how the image itself looks like \autoref{fig: matching-score} ~. In the case of a long search distance, we usually get a non-convex function: there are many peaks in this distribution, but there must be only one true corresponding point. In this case, we tend to use the probability distribution to describe the depth value rather than a single value to describe the depth. So, our question turns to how the estimated depth distribution will change when we continuously perform epipolar search on different images-this is the so-called \textbf{depth filter}.

\begin{figure}[! htp]
\centering
\includegraphics[width=.75 \textwidth]{mapping/matching-score.pdf}
\caption{The distribution of matching scores along the distance, the image is from the literature \cite{Vogiatzis2011}. }
\label{fig: matching-score}
\end{figure}

\subsection{Gaussian depth filter}
The estimation of the depth of a pixel can itself be modeled as a state estimation problem. Therefore, there are two solutions to the problem: filters and nonlinear optimization. Although the nonlinear optimization effect is better, in the case of SLAM, where real-time requirements are strong, considering that the front end has occupied a lot of calculations, the filter method is usually used in the construction of the map. This is also the purpose of the depth filter discussed in this section.

There are several different approaches to the distribution of depth assumptions. First, under relatively simple assumptions, we can assume that the depth value obeys the Gaussian distribution, and get a Kalman-like method (but it is actually just a normalized product, which we will see later). On the other hand, in the literature such as \cite{Vogiatzis2011, Forster2014}, the uniform-Gaussian mixed distribution hypothesis is also adopted, and another form of more complex depth filter is derived. Based on the simple and easy-to-use principle, we first introduce and demonstrate the depth filter under the assumption of Gaussian distribution, and then use the uniform-Gaussian mixed distribution filter as an exercise.

Let the depth $d $of a pixel obey:
\begin{equation}
P(d)=N(\mu, \sigma^2).
\end{equation}

And whenever new data comes in, we will observe its depth. Similarly, suppose this observation is also a Gaussian distribution:
\begin{equation}
P(d_{\mathrm{obs}})=N(\mu_{\mathrm{obs}}, \sigma_{\mathrm{obs}}^2 ).
\end{equation}

So, our question is how to use the observed information to update the original $d $distribution. This is exactly an information fusion problem. According to Appendix A, we understand that the product of two Gaussian distributions is still a Gaussian distribution. Let the distribution of $d $after fusion be $N (\mu_{\mathrm{fuse}}, \sigma_{\mathrm{fuse}} ^ 2) $, then according to the product of the Gaussian distribution, there are:
\begin{equation}
{\mu_{\mathrm{fuse}}}=\frac{{\sigma_{\mathrm{obs}}^2\mu  + {\sigma ^2}{\mu_{\mathrm{obs}}}}}{{{\sigma ^2} + \sigma_{\mathrm{obs}}^2}},\quad \sigma_{\mathrm{fuse}}^2=\frac{{{\sigma ^2}\sigma_{\mathrm{obs}}^2}}{{{\sigma ^2} + \sigma_{\mathrm{obs}}^2}}.
\end{equation}

Since we only have observation equations and no equations of motion, we only use the information fusion part for the depth here, without the need to predict and update like the full Kalman. You can see that the fused equation is relatively easy to understand, but the question still remains: how to determine the distribution of the depth we observe? That is, how to calculate $\mu_{\mathrm{obs}}, \sigma_{\mathrm{obs}} $?

Regarding $\mu_{\mathrm{obs}}, \sigma_{\mathrm{obs}} $, there are some different processing methods. For example, the literature \cite{Engel2013} considers the sum of geometric uncertainty and photometric uncertainty, while \cite{Vogiatzis2011} only considers geometric uncertainty. For the time being, we only consider the uncertainty caused by geometric relations. Now, suppose we have determined the projection position of a pixel of the reference frame in the current frame through epipolar search and block matching. So, how big is this location's uncertainty about depth?

Take \autoref{fig: uncertainty-mapping} ~ as an example. Considering an epipolar search, we found the $\bm{p}_2 $point corresponding to $\bm{p}_1 $, and observed the depth value of $\bm{p}_1 $. The corresponding three-dimensional point of $p_1$ is $\bm{P} $. Thus, you can remember that $\bm{O}_1 \bm{P} $is $\bm{p} $, $\bm{O}_1 \bm{O}_2 $is the camera's translation $\bm{t} $, $\bm{O}_2 \bm{P} $is recorded as $\bm{a} $. Also, note the lower two corners of this triangle as $\alpha, \beta $. Now, consider that there is a pixel size error on the epipolar line $l_2 $, so that the angle of $\beta $becomes $\beta'$, and $\bm{p}_2 $also becomes $\bm{p }_2'$, and note that the corner above is $\gamma $. What we want to ask is, how much difference does this one pixel error cause $\bm{p}'$and $\bm{p} $?

\begin{figure}[! ht]
\centering
\includegraphics[width=.84 \textwidth]{mapping/uncertainty.pdf}
\caption{Uncertainty analysis. }
\label{fig: uncertainty-mapping}
\end{figure}

This is a typical geometric problem. Let's list the geometric relationships between these quantities. Obviously:
\begin{equation}
\begin{array}{l}
\bm{a}=\bm{p} - \bm{t} \\
\alpha =\arccos \left\langle {\bm{p}, \bm{t}} \right\rangle \\
\beta =\arccos \left\langle {\bm{a}, - \bm{t}} \right\rangle .
\end{array}
\end{equation}

Perturbing a pixel for $\bm{p}_2 $will cause a change in $\beta $to become $\beta'$. According to the geometric relationship, there are:
\begin{equation}
\begin{array}{l}
\beta'=\arccos \left\langle {\bm{O}_2 \bm{p}_2', -\bm{t}} \right\rangle \\
\gamma =\pi  - \alpha  - \beta'.
\end{array}
\end{equation}

Therefore, from the sine theorem, the size of $\bm{p}'$can be obtained:
\begin{equation}
\| \bm{p}' \|=\| \bm{t} \| \frac{{\sin \beta'}}{{\sin \gamma }}.
\end{equation}

From this, we determined the depth uncertainty caused by the uncertainty of a single pixel. If you think that the block search of the epipolar search has only one pixel error, then you can set:
\begin{equation}
\sigma_{\mathrm{obs}}=\| \bm{p} \|-\| \bm{p}'\|.
\end{equation}

Of course, if the uncertainty of the epipolar search is greater than one pixel, we can also amplify this uncertainty by following this derivation. The following deep data fusion has been introduced earlier. In actual engineering, when the uncertainty is less than a certain threshold, it can be considered that the depth data has converged.

In summary, we give a complete process for estimating the dense depth:
\begin{mdframed}
\begin{enumerate}
\item Assume that the depth of all pixels satisfies an initial Gaussian distribution.
\item When new data is generated, the position of the projection point is determined by epipolar search and block matching.
\item Calculates the depth and uncertainty after triangulation based on geometric relationships.
\item Fuse the current observation into the previous estimate. If it converges, stop the calculation, otherwise return to step 2.
\end{enumerate}
\end{mdframed}

These steps constitute a set of feasible depth estimation methods. Please note that the depth value mentioned here is the length of $O_1 P $. It is slightly different from the "depth" we mentioned in the pinhole camera model-the depth in the pinhole camera refers to the $z $value of the pixel . We will demonstrate the results of the algorithm in the practice section.

\section{Practice: Monocular Dense Reconstruction}
The example program in this section will use the test data set of REMODE \textsuperscript{\cite{Handa2012, Pizzoli2014}}. It provides a monocular overhead image collected by a drone, a total of 200, and provides the true pose of each image. Let us consider, on the basis of these data, estimate the depth value corresponding to each pixel of the first frame image, that is, perform monocular dense reconstruction.

First, the reader is requested to download the data used by the sample program from ~ \url{http://rpg.ifi.uzh.ch/datasets/remode_test_data.zip} ~. You can use a web browser or download tool to download it. After decompression, all images from 0 to 200 will be found in test \_data/Images, and a text file will be found in the test \_data directory, which records the pose corresponding to each image:
\begin{lstlisting}
scene_000.png 1.086410 4.766730 -1.449960 0.789455 0.051299 -0.000779 0.611661
scene_001.png 1.086390 4.766370 -1.449530 0.789180 0.051881 -0.001131 0.611966
scene_002.png 1.086120 4.765520 -1.449090 0.788982 0.052159 -0.000735 0.612198
...
\end{lstlisting}

\autoref{fig: remode-dataset} ~ shows images at several moments. It can be seen that the scene is mainly composed of the ground, the table and the debris on the table. If the depth estimate is roughly correct, then we can at least see the difference between the depth value of the table and the ground. Below, we write a dense depth estimation program as explained earlier. For easy understanding, the program is written in C style and placed in a single file. This program is a bit long. In the book, we will focus on several important functions. The rest of the content is asked to read the source code of GitHub.

\begin{figure}[! ht]
\centering
\includegraphics[width=1.0 \textwidth]{mapping/remode-dataset.pdf}
\caption{Dataset icon. }
\label{fig: remode-dataset}
\end{figure}

\begin{lstlisting}[language=c ++, caption=slambook/ch13/dense \_monocular/dense \_mapping.cpp (fragment)]
#include <iostream>
#include <vector>
#include <fstream>
using namespace std; 
#include <boost/timer.hpp>

// for sophus 
#include <sophus/se3.h>
using Sophus::SE3;

// for eigen 
#include <Eigen/Core>
#include <Eigen/Geometry>
using namespace Eigen;

#include <opencv2/core/core.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/imgproc/imgproc.hpp>

using namespace cv;

/ **********************************************
* This program demonstrates dense depth estimation of a monocular camera with a known trajectory
* Use epipolar search + NCC matching method, corresponding to section 13.2 of this book
* Please note that this program is not perfect, you can improve it completely.
*********************************************** /

// ------------------------------------------------------------------
// parameters 
const int boarder=20; 	// 边缘宽度
const int width=640;  	// 宽度
const int height=480;  	// 高度
const double fx=481.2f;	// 相机内参
const double fy=-480.0f;
const double cx=319.5f;
const double cy=239.5f;
const int ncc_window_size=2; // window width taken by NCC
const int ncc_area=(2 * ncc_window_size + 1) * (2 * ncc_window_size + 1); // NCC window area
const double min_cov=0.1; // convergence judgment: minimum variance
const double max_cov=10; // divergence decision: maximum variance

// ------------------------------------------------ ------------------
// important functions
// read data from the REMODE data set
bool readDatasetFiles (
const string & path,
vector <string> & color_image_files,
vector <SE3> & poses
);

// Update the depth estimate based on the new image
bool update (
const Mat & ref,
const Mat & curr,
const SE3 & T_C_R,
Mat & depth,
Mat & depth_cov
);

// epipolar search
bool epipolarSearch (
const Mat & ref,
const Mat & curr,
const SE3 & T_C_R,
const Vector2d & pt_ref,
const double & depth_mu,
const double & depth_cov,
Vector2d & pt_curr
);

// Update the depth filter
bool updateDepthFilter (
const Vector2d & pt_ref,
const Vector2d & pt_curr,
const SE3 & T_C_R,
Mat & depth,
Mat & depth_cov
);

// Calculate NCC score
double NCC (const Mat & ref, const Mat & curr, const Vector2d & pt_ref, const Vector2d & pt_curr);

// Bilinear grayscale interpolation
inline double getBilinearInterpolatedValue (const Mat & img, const Vector2d & pt){
uchar * d=& img.data[int (pt (1,0)) * img.step + int (pt (0,0))];
double xx=pt (0,0)-floor (pt (0,0));
double yy=pt (1,0)-floor (pt (1,0));
return ((1-xx) * (1-yy) * double (d[0]) +
xx * (1-yy) * double (d[1]) +
(1-xx) * yy * double (d[img.step]) +
xx * yy * double (d[img.step + 1]))/255.0;
}

// ------------------------------------------------ ------------------
// some gadgets
// show estimated depth map
bool plotDepth (const Mat & depth);

// pixel to camera coordinate system
inline Vector3d px2cam (const Vector2d px){
return Vector3d (
(px (0,0)-cx)/fx,
(px (1,0)-cy)/fy,
		1
);
}

// Camera coordinate system to pixels
inline Vector2d cam2px (const Vector3d p_cam){
return Vector2d (
p_cam (0,0) * fx/p_cam (2,0) + cx,
p_cam (1,0) * fy/p_cam (2,0) + cy
);
}

// detect if a point is inside the image border
inline bool inside (const Vector2d & pt){
return pt (0,0)>=boarder && pt (1,0)>=boarder
&& pt (0,0) + boarder <width && pt (1,0) + boarder <= height;
}

int main (int argc, char ** argv)
{
if (argc!=2)
{
cout << "Usage: dense_mapping path_to_test_dataset" << endl;
return -1;
}

// read data from the dataset
vector <string> color_image_files;
vector <SE3> poses_TWC;
bool ret=readDatasetFiles (argv[1], color_image_files, poses_TWC);
if (ret == false)
{
cout << "Reading image files failed!" << endl;
return -1;
}
cout << "read total" << color_image_files.size () << "files." << endl;

// first picture
Mat ref=imread (color_image_files[0], 0); // gray-scale image
SE3 pose_ref_TWC=poses_TWC[0];
double init_depth=3.0; // initial depth value
double init_cov2=3.0; // initial value of variance
Mat depth (height, width, CV_64F, init_depth); // depth map
Mat depth_cov (height, width, CV_64F, init_cov2); // depth map variance
Ranch
for (int index=1; index <color_image_files.size (); index ++)
{
cout << "*** loop" << index << "***" << endl;
Mat curr=imread (color_image_files[index], 0);
if (curr.data == nullptr) continue;
SE3 pose_curr_TWC=poses_TWC[index];
SE3 pose_T_C_R=pose_curr_TWC.inverse () * pose_ref_TWC;
// Coordinate conversion relationship: T_C_W * T_W_R=T_C_R
update (ref, curr, pose_T_C_R, depth, depth_cov);
plotDepth (depth);
imshow ("image", curr);
waitKey (1);
}

Ranch
return 0;
}

// Update the entire depth map
bool update (const Mat & ref, const Mat & curr, const SE3 & T_C_R, Mat & depth, Mat & depth_cov)
{
#pragma omp parallel for
for (int x=boarder; x <width-boarder; x ++)
#pragma omp parallel for
for (int y=boarder; y <height-boarder; y ++)
{
// traverse each pixel
if (depth_cov.ptr <double> (y)[x] <min_cov
|| depth_cov.ptr <double> (y)[x]> max_cov) // depth has converged or diverged
continue;
// search for a match on (x, y) on the epipolar line
Vector2d pt_curr;
bool ret=epipolarSearch (
ref,
curr,
T_C_R,
Vector2d (x, y),
depth.ptr <double> (y)[x],
sqrt (depth_cov.ptr <double> (y)[x]),
pt_curr
);
Ranch
if (ret == false) // match failed
continue;
Ranch
// uncomment this to show matches
// showEpipolarMatch (ref, curr, Vector2d (x, y), pt_curr);
Ranch
// Match successfully, update depth map
updateDepthFilter (Vector2d (x, y), pt_curr, T_C_R, depth, depth_cov);
}
}

// epipolar search
// For methods, see sections 13.2 and 13.3 of this book
bool epipolarSearch (
const Mat & ref, const Mat & curr,
const SE3 & T_C_R, const Vector2d & pt_ref,
const double & depth_mu, const double & depth_cov,
Vector2d & pt_curr
)
{
Vector3d f_ref=px2cam (pt_ref);
f_ref.normalize ();
Vector3d P_ref=f_ref * depth_mu; // P vector of reference frame
Ranch
Vector2d px_mean_curr=cam2px (T_C_R * P_ref); // pixels projected by mean depth
double d_min=depth_mu-3 * depth_cov, d_max=depth_mu + 3 * depth_cov;
if (d_min <0.1) d_min=0.1;
Vector2d px_min_curr=cam2px (T_C_R * (f_ref * d_min)); // pixels projected at minimum depth
Vector2d px_max_curr=cam2px (T_C_R * (f_ref * d_max)); // pixels projected at maximum depth
Ranch
Vector2d epipolar_line=px_max_curr-px_min_curr; // polar line (line segment form)
Vector2d epipolar_direction=epipolar_line; // polar line direction
epipolar_direction.normalize ();
double half_length=0.5 * epipolar_line.norm (); // half length of the polar line segment
if (half_length> 100) half_length=100; // we don't want to search too many things
Ranch
// Uncomment this sentence to show the epipolar line (segment)
// showEpipolarLine (ref, curr, pt_ref, px_min_curr, px_max_curr);
Ranch
// Search on the epipolar line, centering on the average depth point, taking half length on each side
double best_ncc=-1.0;
Vector2d best_px_curr;
for (double l=-half_length; l <= half_length; l +=0.7) // l +=sqrt (2)/2
{
Vector2d px_curr=px_mean_curr + l * epipolar_direction; // point to be matched
if (! inside (px_curr))
continue;
// Calculate the NCC of the point to be matched and the reference frame
double ncc=NCC (ref, curr, pt_ref, px_curr);
if (ncc> best_ncc)
{
best_ncc=ncc;
best_px_curr=px_curr;
}
}
if (best_ncc <0.85f) // only believe that NCC has a high match
return false;
pt_curr=best_px_curr;
return true;
}

double NCC (
const Mat & ref, const Mat & curr,
const Vector2d & pt_ref, const Vector2d & pt_curr
)
{
// zero mean − normalized cross-correlation
// first calculate the mean
double mean_ref=0, mean_curr=0;
vector <double> values_ref, values_curr; // mean of reference frame and current frame
for (int x=-ncc_window_size; x <= ncc_window_size; x ++)
for (int y=-ncc_window_size; y <= ncc_window_size; y ++)
{
double value_ref=double (ref.ptr <uchar> (int (y + pt_ref (1,0)))[int (x + pt_ref (0,0))])/255.0;
mean_ref +=value_ref;
Ranch
double value_curr=getBilinearInterpolatedValue (curr, pt_curr + Vector2d (x, y));
			mean_curr += value_curr;
Ranch
			values_ref.push_back(value_ref);
			values_curr.push_back(value_curr);
		}
Ranch
	mean_ref /= ncc_area;
	mean_curr /= ncc_area;
Ranch
	// 计算 Zero mean NCC
	double numerator=0, denominator1=0, denominator2=0;
	for ( int i=0; i<values_ref.size(); i++ )
	{
		double n=(values_ref[i]-mean_ref) * (values_curr[i]-mean_curr);
		numerator += n;
		denominator1 += (values_ref[i]-mean_ref)*(values_ref[i]-mean_ref);
		denominator2 += (values_curr[i]-mean_curr)*(values_curr[i]-mean_curr);
	}
	return numerator/sqrt( denominator1*denominator2+1e-10 );   // 防止分母出现零
}

bool updateDepthFilter(
	const Vector2d& pt_ref, 
	const Vector2d& pt_curr, 
	const SE3& T_C_R,
	Mat& depth, 
	Mat& depth_cov
)
{
	// 用三角化计算深度
	SE3 T_R_C=T_C_R.inverse();
	Vector3d f_ref=px2cam( pt_ref );
	f_ref.normalize();
	Vector3d f_curr=px2cam( pt_curr );
	f_curr.normalize();
Ranch
	// 方程参照本书第 7 讲三角化一节
	Vector3d t=T_R_C.translation();
	Vector3d f2=T_R_C.rotation_matrix() * f_curr; 
	Vector2d b=Vector2d ( t.dot ( f_ref ), t.dot ( f2 ) );
	double A[4];
	A[0]=f_ref.dot ( f_ref );
	A[2]=f_ref.dot ( f2 );
	A[1]=-A[2];
	A[3]=- f2.dot ( f2 );
	double d=A[0]*A[3]-A[1]*A[2];
	Vector2d lambdavec=
		Vector2d (  A[3] * b ( 0,0 ) - A[1] * b ( 1,0 ),
			-A[2] * b ( 0,0 ) + A[0] * b ( 1,0 )) /d;
	Vector3d xm=lambdavec ( 0,0 ) * f_ref;
	Vector3d xn=t + lambdavec ( 1,0 ) * f2;
	Vector3d d_esti=( xm+xn )/2.0;  // 三角化算得的深度向量
	double depth_estimation=d_esti.norm();   // 深度值
Ranch
	// 计算不确定性（以一个像素为误差）
	Vector3d p=f_ref*depth_estimation;
	Vector3d a=p - t; 
	double t_norm=t.norm();
	double a_norm=a.norm();
	double alpha=acos( f_ref.dot(t)/t_norm );
	double beta=acos( -a.dot(t)/(a_norm*t_norm));
	double beta_prime=beta + atan(1/fx);
	double gamma=M_PI - alpha - beta_prime;
	double p_prime=t_norm * sin(beta_prime)/sin(gamma);
	double d_cov=p_prime - depth_estimation; 
	double d_cov2=d_cov*d_cov;
Ranch
	// 高斯融合
	double mu=depth.ptr<double>( int(pt_ref(1,0)) )[ int(pt_ref(0,0)) ];
	double sigma2=depth_cov.ptr<double>( int(pt_ref(1,0)) )[ int(pt_ref(0,0)) ];
Ranch
	double mu_fuse=(d_cov2*mu+sigma2*depth_estimation)/( sigma2+d_cov2);
	double sigma_fuse2=( sigma2 * d_cov2 )/( sigma2 + d_cov2 );
Ranch
	depth.ptr<double>( int(pt_ref(1,0)) )[ int(pt_ref(0,0)) ]=mu_fuse; 
	depth_cov.ptr<double>( int(pt_ref(1,0)) )[ int(pt_ref(0,0)) ]=sigma_fuse2;
Ranch
	return true;
}

// 其他次要的函数略
\end{lstlisting}

If the reader understands the previous section, it is not difficult to read the source code here. Nevertheless, we explain a few key functions:

\begin{enumerate}
	\item The main function is very simple. It is only responsible for reading the image from the data set, and then handing it to the update function to update the depth map.
	\item update function, we traverse each pixel of the reference frame, first find the epipolar match in the current frame, if it can match, then use the result of epipolar match to update the depth map estimation.
	\item The principle of epipolar search is roughly the same as that described in the previous section, but some details have been added to the implementation: because the depth value is assumed to follow a Gaussian distribution, we use the mean as the center, and take $ \pm 3 \sigma $ as the radius , Then look for the epipolar projection in the current frame. Then, iterate through the pixels on this polar line (the step size is an approximate value of $ \sqrt{2} /2$0.7), and find the highest NCC point as the matching point. If the highest NCC is also lower than the threshold (here take 0.85), the match is considered to have failed.
	\item The calculation of NCC uses the de-averaging method, that is, for the image block $ \bm{A}, \bm{B} $, take:
	\begin{equation}
		\mathrm{NCC}_{z} (\bm{A}, \bm{B}) = \frac{{\sum\limits_{i,j} {\left( {\bm{A}(i,j) - \bm{\bar{ A}}(i,j)} \right)\left( {\bm{B}(i,j) - \bm{\bar {B}}(i,j)} \right)} }}{{\sqrt {\sum\limits_{i,j} {{{\left( {\bm{A}(i,j) - \bm{\bar {A}}(i,j)} \right)}^2}} \sum\limits_{i,j} {{{\left( {\bm{B}(i,j) - \bm{\bar {B}}(i,j)} \right)}^2}} } }}.
	\end{equation}
	\item The calculation method of triangulation is consistent with Section 7.5, and the calculation of uncertainty is consistent with the Gaussian fusion method and the previous section.
	\end{enumerate}

Although the program is a bit long, I believe the reader can understand it according to the above tips. Let's look at its actual operation effect.

\subsection *{Experimental result}
After compiling this program, run with the data set directory as a parameter.\footnote{Please note that dense depth estimation is time consuming. If your computer is older, please wait patiently for a while. }:
\begin{lstlisting}
$ build/dense_mapping ~/dataset/test_data 
read total 202 files.
*** loop 1 ***
*** loop 2 ***
...
\end{lstlisting}

\clearpage
The information output by the program is relatively simple, showing only the number of iterations, the current image and the depth map. Regarding the depth map, we show the result of multiplying the depth value by 0.4-that is, the depth of the pure white point (the value is 1.0) is about 2.5 meters. The darker the color, the smaller the depth value, which is the closer the object is to us . If you actually run the program, you should find that the depth estimation is a dynamic process-a process that gradually converges from a less certain initial value to a stable value. Our initial value uses a distribution with a mean and variance of 3.0. Of course, you can also modify the initial distribution to see how it affects the results.

From \autoref{fig: snapshot} ~, it can be found that when the number of iterations exceeds a certain value, the depth map stabilizes and no new data is changed. Looking at the depth map after stabilization, we find that the difference between the floor and the table can be roughly seen, while the depth of objects on the table is close to the table. The whole estimate is mostly correct, but there are also a large number of incorrect estimates. They appear as inconsistencies in the depth map with the surrounding data and are over- or under-estimated. In addition, the location at the edge, because it is seen less frequently during exercise, has not been accurately estimated. In summary, we think that most of this depth map is correct, but it does not achieve the expected results. We will analyze the reasons for these situations in the next section and discuss what can be improved.

\begin{figure}[! ht]
\centering
\includegraphics[width=.9 \textwidth]{mapping/snapshot.pdf}
\caption{Screenshot while the demo program is running. The two graphs are the results of 10 and 30 iterations, respectively. }
\label{fig: snapshot}
\end{figure}

\clearpage
\section{Experimental Analysis and Discussion}
In the previous section we demonstrated a dense map of a mobile monocular camera, estimating the depth of each pixel of the reference frame. Our code is relatively simple and straightforward, without using many tricks, so common situations occur in practical engineering-simple is often not the most effective.

Due to the complexity of real data, programs that can work in the actual environment often require careful consideration and a lot of engineering skills, which makes every practical code extremely complex-they are difficult to explain to beginners, so We had to use a less efficient, but relatively readable and writeable implementation. Of course, we can put forward several suggestions for improving the demo program, but we do not intend to present the modified (very complicated) program directly to the reader.

Below we make a preliminary analysis of the results of the experiments in the previous section. We will analyze the results of the demonstration experiments from the perspectives of computer vision and filters.

\subsection{The problem with pixel gradients}
Observing the depth image, we will find an obvious fact. The correctness of the block matching depends on whether the image blocks are distinguishable. Obviously, if the image block is only black or white and lacks valid information, then in the NCC calculation, we are likely to mismatch it with a surrounding pixel. The reader is invited to observe the printer surface in the demo program. Because it is uniform white, it is very easy to cause mismatch, so the depth information of the printer surface is mostly incorrect-the space surface of the sample program has a streak-like depth estimate that should not be. According to our intuitive imagination, The printer surface must be smooth.

There is a problem involved here, which we have seen once in the direct method. When performing block matching (and NCC calculations), we must assume that the small block is unchanged, and then compare the small block with other small blocks. At this time, the small blocks with \textbf{obvious gradient} will have good discrimination, and it is not easy to cause mismatch. For \textbf{pixels with inconspicuous gradients}, since there is no discriminativeness in block matching, it will be difficult to effectively estimate its depth. Conversely, where the pixel gradient is obvious, the depth information we get is relatively accurate, such as magazines on the desktop, phones, and other objects with obvious \textbf{texture}. Therefore, the demo program reflects a very common problem in stereo vision: \textbf{dependence on object texture}. This problem is also extremely common in binocular vision, which shows that the reconstruction quality of stereo vision is very dependent on the environment texture.

Our demo program deliberately used well-textured environments, such as checkerboard-like floors, wood-grained tabletops, etc., and thus got a seemingly good result. However, in practice, places with uniform brightness such as walls and smooth objects will often appear, affecting our estimation of its depth. In a way, the problem is \textbf{cannot be improved and solved on the existing algorithm flow}-if we still only care about the neighborhood (small block) around a pixel.

Further discussing the pixel gradient problem, we will also find the connection between the pixel gradient and the epipolar line. The article \cite{Engel2013} discussed their relationship in detail, but it is also intuitively reflected in our demo program.

Taking \autoref{fig: epipolar-gradient} ~ as an example, we will give two more extreme cases: the pixel gradient is parallel to the polar line direction, and the pixel gradient is perpendicular to the polar line direction. Let's look at the vertical situation first. In the vertical example, even if the small blocks have obvious gradients, when we do block matching along the epipolar line, we will find that the matching degrees are all the same, so no effective match can be obtained. Conversely, in the parallel example, we can accurately determine where the highest matching point appears. In practice, the gradient and the epipolar line are likely to be in between: they are neither completely vertical nor completely parallel. At this time, we say that when the angle between the pixel gradient and the epipolar line is large, the uncertainty of the epipolar line matching is large; and when the angle is small, the uncertainty of the matching becomes small. In the demo program, we uniformly treat these situations as a pixel error, which is actually not fine enough. Considering the relationship between epipolar lines and pixel gradients, a more accurate uncertainty model should be used. Specific adjustments and improvements are left as exercises.

\begin{figure}[! htp]
\centering
\includegraphics[width=1.0 \textwidth]{mapping/epipolar-gradient.pdf}
\caption{The relationship between pixel gradient and epipolar line. }
\label{fig: epipolar-gradient}
\end{figure}

\subsection{inverse depth}
From another perspective, we may ask: Is it appropriate to assume pixel depth as a Gaussian distribution? This relates to a parameterization problem (Parameterization).

\clearpage
In the previous content, we often described a point by its three world coordinates $ x, y, z $, which is a parameterized form. We think that the three quantities $ x, y, z $ are random, and they obey the (three-dimensional) Gaussian distribution. However, this lecture uses the image coordinates $ u, v $ and the depth value $ d $ to describe a certain spatial point (that is, a densely constructed map). We think that $ u, v $ are not moving, and $ d $ follows a (one-dimensional) Gaussian distribution, which is another form of parameterization. Then we have to ask: Is there any difference between the two parameterization forms? Can we also assume that $ u, v $ obey the Gaussian distribution and form another parametric form?

Different parameterized forms actually describe the same quantity, that is, a certain three-dimensional space point. Considering that when we see a point on the camera, its image coordinates $ u, v $ are relatively certain. The uncertainty of \footnote{$ u, v $ depends on the resolution of the image. }, And the depth value $ d $ is very uncertain. At this time, if the world coordinates $ x, y, z $ are used to describe this point, then depending on the current pose of the camera, there may be a clear correlation between the three quantities $ x, y, z $. Reflected in the covariance matrix, the non-diagonal elements are not zero. And if you parameterize a point with $ u, v, d $, then its $ u, v $, and $ d $ are at least approximately independent, and we can even think that $ u, v $ is also independent-so it The covariance matrix is ​​approximated by a diagonal matrix, which is more concise.

Inverse depth is a widely used parameterization technique \textsuperscript{\cite{Montiel2006, Civera2008}} that has appeared in SLAM research in recent years. In the demo program, we assume that the depth value satisfies a Gaussian distribution: $ d \sim N (\mu, \sigma ^ 2) $. But isn't this reasonable? Does the depth really approximate a Gaussian distribution? If you think about it, there are some problems with the normal distribution of depth:

\begin{enumerate}
\item What we actually want to express is: the depth of this scene is about 5 \textasciitilde 10 meters, there may be some more distant points, but the distance is definitely not less than the focal length of the camera (or the depth is not considered to be less than 0). This distribution does not form a symmetrical shape like the Gaussian distribution. Its tail may be slightly longer, while the negative range is zero.
\item In some outdoor applications, there may be points that are very far away or even infinitely far away. It is difficult to cover these points in our initial values, and describing them with a Gaussian distribution will have some numerical computational difficulties.
\end{enumerate}

Therefore, the inverse depth came into being. People have found in simulations that it is more efficient to assume that the inverse of the depth, which is \textbf{inverse depth}, is Gaussian distribution \textsuperscript{\cite{Civera2008}}. Subsequently, in practical applications, the inverse depth also has better numerical stability, which has gradually become a general technique that exists in standard practices in existing SLAM schemes \textsuperscript{\cite{Forster2014, Engel2014, Mur- Artal2015}}.

It is not complicated to change the demo program from positive depth to reverse depth. As long as in the derivation of the previous depth, change $ d $ to the inverse depth $ d ^{-1} $. We also leave this change as an exercise for the reader to complete.
\subsection{Transform between images}
Before block matching, it is also a common preprocessing method to do an image-to-image transformation. This is because we assume that the image patches remain the same as the camera moves, and this assumption holds when the camera is panned (the sample data set is basically this example), but when the camera undergoes a significant rotation, it is It is difficult to continue. In particular, when the camera is rotated around the light center, an image with lower black and white may become an image with upper black and lower white, causing the correlation to directly become negative (although still the same block).

To prevent this, we usually need to take into account the motion between the reference frame and the current frame before block matching. According to the camera model, a pixel $ \bm{P}_R $ on the reference frame has the following relationship with the real three-dimensional point world coordinate $ \bm{P}_W $:
\begin{equation}
d_R {\bm{P}_R}=\bm{K} \left( {{\bm{R}_{\mathrm{RW}}}{\bm{P}_W} + {\bm{t}_{\mathrm{RW}}}} \right).
\end{equation}

Similarly, for the current frame, there is also a projection of $ \bm{P}_W $ on it, which is recorded as $ \bm{P}_C $:
\begin{equation}
d_C {\bm{P}_C}=\bm{K} \left( {{\bm{R}_{\mathrm{CW}}}{\bm{P}_W} + {\bm{t}_{\mathrm{CW}}}} \right).
\end{equation}

Substituting and eliminating $ \bm{P}_W $, we get the pixel relationship between the two images:
\begin{equation}
d_C {\bm{P}_C}=d_R \bm{K} \bm{R}_{\mathrm{CW}} \bm{R}_{\mathrm{RW}}^\mathrm{T} \bm{K}^{-1} \bm{P}_R + \bm{K} \bm{t}_{\mathrm{CW}} - \bm{K} \bm{R}_{\mathrm{CW}} \bm{R}_{\mathrm{RW}}^\mathrm{T} \bm{K} \bm{t}_{\mathrm{RW}}.
\end{equation}

% Since the depths of $ d_C and d_R $ are unknown, we temporarily consider them to be the same, so this formula constitutes an affine transformation from image coordinates to image coordinates. Therefore, we can transform the current frame (or reference frame) and then perform block matching to obtain a better effect on rotation. There are some slightly different affine transformation methods, such as solving the relative derivative of image coordinates in \cite{Klein2007} to calculate affine, which is also feasible.
When $ d_R, \bm{P}_R $ is known, the projection position of $ \bm{P}_C $ can be calculated. At this time, by giving $ \bm{P}_R $ two additional components, $ \mathrm{d} u, \mathrm{d} v $, you can get the increase of $ \bm{P}_C $ The amount $ \mathrm{d} u_c, \mathrm{d} v_c $. In this way, an affine transformation is formed by calculating a linear relationship between the reference frame and the current frame image coordinate transformation in the local range:
\begin{equation}
\left[ \begin{array}{l}
\mathrm{d}u_c\\
\mathrm{d}v_c
\end{array} \right]=\left[ {\begin{array}{*{20}{c}}
	{\frac{{\mathrm{d}u_c}}{{\mathrm{d}u}}}&{\frac{{\mathrm{d}u_c}}{{\mathrm{d}v}}}\\
	{\frac{{\mathrm{d}v_c}}{{\mathrm{d}u}}}&{\frac{{\mathrm{d}v_c}}{{\mathrm{d}v}}}
	\end{array}} \right]\left[ \begin{array}{l}
\mathrm{d}u\\
\mathrm{d}v
\end{array} \right]
\end{equation}

According to the affine transformation matrix, we can transform the pixels of the current frame (or reference frame) and then perform block matching in order to obtain a better effect on rotation.

\subsection{Parallelization: A Question of Efficiency}
We have also seen in experiments that the estimation of dense depth maps is very time-consuming, because the points we need to estimate have suddenly changed from hundreds of feature points to hundreds of thousands of pixels, even now mainstream CPUs have It is impossible to calculate such a huge amount in real time. However, this problem also has another property: the depth estimates of these hundreds of thousands of pixels are independent of each other! This makes parallelization useful.

In the sample program, we traverse all pixels in a double loop and search them epipolarly one by one. When we use the CPU, the process is serial: the next pixel must be calculated after the previous pixel has been calculated. However, in fact, it is not necessary for the next pixel to wait for the calculation of the previous pixel to complete, because there is no obvious relationship between them, so we can use multiple threads to calculate each pixel separately and then unify the results. In theory, if we have 300,000 threads, the calculation time for this problem is the same as calculating one pixel.

GPU's parallel computing architecture is very suitable for such problems. Therefore, in single-dual and dual-purpose dense reconstruction, it is often seen to use GPU for parallel acceleration. Of course, this book is not intended to involve GPU programming, so we only point out the possibility of using GPU acceleration here, and the specific practice is left to the reader for verification. According to some similar work, using GPU's dense depth estimation can be real-time on mainstream GPUs.

\subsection{Other improvements}
In fact, we can propose many improvements to this routine, such as:

\begin{enumerate}
\item Now each pixel is calculated completely independently, there may be situations where the pixel depth is small and the edge is large. We can assume that the adjacent depth changes in the depth map will not be too large, thus adding a spatial regular term to the depth estimation. This approach makes the resulting depth map smoother.
\item We do not explicitly deal with the case of Outlier. In fact, due to various factors such as occlusion, lighting, motion blur, it is impossible to maintain a successful match for each pixel. In the practice of the demo program, as long as the NCC is greater than a certain value, a successful match is considered to have occurred, and no mismatch is considered.
Ranch
\hspace{2em} There are several ways to handle mismatches. For example, the depth filter under the uniform-Gaussian mixture distribution proposed by the literature \cite{Vogiatzis2011}, which explicitly distinguishes the inner point from the outer point and performs probabilistic modeling, can better deal with the outer point data. However, this type of filter theory is more complicated. I don't want to cover too much in this book, and readers can read the original paper.
\end{enumerate}

As can be seen from the above discussion, there are many possible improvements. If we improve each step carefully, in the end, we hope to get a good dense construction plan. However, as we discussed, there are some problems \textbf{there are theoretical difficulties}, such as the dependence on the environment texture, and the correlation between the pixel gradient and the epipolar direction (and the parallel case). These issues \textbf{difficult to solve by tweaking the code implementation}. So, so far, although binocular and mobile mono can build dense maps, we usually think that they are too dependent on the environment texture and lighting and are not reliable enough.

\section{RGB-D Dense Construction}
In addition to using monocular and binocular for dense reconstruction, the RGB-D camera is a better choice in scope. The depth estimation problem discussed in detail in the previous lecture can be obtained completely by hardware measurement in the sensor in the RGB-D camera, without consuming a large amount of computing resources to estimate. In addition, the structured light or flying time principle of RGB-D ensures the independence of depth data on texture. Even when facing a solid-colored object, as long as it can reflect light, we can measure its depth. This is also a great advantage of RGB-D sensors.

It is relatively easy to make dense maps using RGB-D. However, depending on the map format, there are several different mainstream mapping methods. The most intuitive and simple method is to convert the RGB-D data into a Point Cloud based on the estimated camera pose, and then stitch them together to get a Point Cloud Map composed of discrete points. ). On this basis, if we have further requirements on the appearance, and we want to estimate the surface of the object, we can use triangle mesh (Surfel) to construct the map. On the other hand, if you want to know the obstacle information of the map and navigate on the map, you can also create an Occupancy Map through Voxel.

We seem to have introduced many new concepts. Readers are requested not to worry, we will slowly introduce them one by one. For those who are suitable for experiments, we will provide several demo programs as usual. Since there is not much theoretical knowledge involved in RGB-D mapping, the following sections will be introduced directly in the practical part. GPU mapping is beyond the scope of this book, we will briefly explain its principle without making a demonstration.

\subsection{Practice: Point Cloud Map}
First, let's explain the simplest point cloud map. The so-called point cloud is a map represented by a set of discrete points. The most basic point contains the three-dimensional coordinates of $ x, y, z $, and can also carry the color information of $ r, g, b $. Since RGB-D cameras provide color and depth maps, it is easy to calculate RGB-D point clouds based on camera internal parameters. If the pose of the camera is obtained by some means, as long as the point clouds are directly added, the global point cloud can be obtained. In the ~ \ref{sec: join-point-cloud} ~ section of this book, an example of splicing point clouds through internal and external parameters of the camera was given. However, that example is mainly for the reader to understand the internal and external parameters of the camera, and in the actual mapping, we will also add some filtering to the point cloud to obtain better visual effects. In this program, we mainly use two types of filters: out-point removal filters, and down-sampling filters. The code of the sample program is as follows (because part of the code is the same as before, we mainly look at the changed part):
\begin{lstlisting}[language=c++,caption=slambook/ch13/dense\_RGBD/pointcloud\_mapping.cpp（片段）]
int main( int argc, char** argv )
{
	// 图像读取部分略
	// 定义点云使用的格式：这里用的是XYZRGB
	typedef pcl::PointXYZRGB PointT; 
	typedef pcl::PointCloud<PointT> PointCloud;
Ranch
	// 新建一个点云
	PointCloud::Ptr pointCloud( new PointCloud ); 
	for ( int i=0; i<5; i++ )
	{
		PointCloud::Ptr current( new PointCloud );
		cout<<"转换图像中: "<<i+1<<endl; 
		cv::Mat color=colorImgs[i]; 
		cv::Mat depth=depthImgs[i];
		Eigen::Isometry3d T=poses[i];
		for ( int v=0; v<color.rows; v++ )
		for ( int u=0; u<color.cols; u++ )
		{
			unsigned int d=depth.ptr<unsigned short> ( v )[u]; // 深度值
			if ( d==0 ) continue; // 为 0 表示没有测量到
			if ( d >= 7000 ) continue; // 深度太大时不稳定，去掉
			Eigen::Vector3d point; 
			point[2]=double(d)/depthScale; 
			point[0]=(u-cx)*point[2]/fx;
			point[1]=(v-cy)*point[2]/fy; 
			Eigen::Vector3d pointWorld=T*point;
Ranch
			PointT p ;
			p.x=pointWorld[0];
			p.y=pointWorld[1];
			p.z=pointWorld[2];
			p.b=color.data[ v*color.step+u*color.channels() ];
			p.g=color.data[ v*color.step+u*color.channels()+1 ];
			p.r=color.data[ v*color.step+u*color.channels()+2 ];
			current->points.push_back( p );
		}
		// depth filter and statistical removal 
		PointCloud::Ptr tmp ( new PointCloud );
		pcl::StatisticalOutlierRemoval<PointT> statistical_filter;
		statistical_filter.setMeanK(50);
		statistical_filter.setStddevMulThresh(1.0);
		statistical_filter.setInputCloud(current);
		statistical_filter.filter( *tmp );
		(*pointCloud) += *tmp;
	}
Ranch
	pointCloud->is_dense=false;
	cout<<"点云共有"<<pointCloud->size()<<"个点."<<endl;
Ranch
	// voxel filter 
	pcl::VoxelGrid<PointT> voxel_filter; 
	voxel_filter.setLeafSize( 0.01, 0.01, 0.01 );       // resolution 
	PointCloud::Ptr tmp ( new PointCloud );
	voxel_filter.setInputCloud( pointCloud );
	voxel_filter.filter( *tmp );
	tmp->swap( *pointCloud );
Ranch
	cout<<"滤波之后，点云共有"<<pointCloud->size()<<"个点."<<endl;
Ranch
	pcl::io::savePCDFileBinary("map.pcd", *pointCloud );
	return 0;
}
\end{lstlisting}

\clearpage
Our thinking has not changed much. The main differences are:

\begin{enumerate}
\item When generating a point cloud per frame, remove points with too large or invalid depth values. This is mainly because the effective range of Kinect is taken into account, and the depth value after the range will have a larger error.
\item Uses statistical filter methods to remove outliers. This filter counts the distribution of distance values ​​between each point and the $ N $ points closest to it, and removes points with an excessively large mean distance. In this way, we keep those "sticky" points and remove isolated noise points.
\item Finally, the Voxel Filter is used for downsampling. Due to the overlap of visual fields in multiple perspectives, there will be a large number of very close points in the overlapping area. This will take up a lot of memory space in vain. Voxel filtering ensures that there is only one point in a certain size cube (or voxel), which is equivalent to downsampling the three-dimensional space, which can save a lot of storage space.
\end{enumerate}

\autoref{fig: pcd-filter} ~ shows the comparison chart before and after filtering. On the left is a point cloud map generated by the lecture 5 program, and on the right is a filtered point cloud map. Looking at the white box, you can see that there are many isolated points caused by noise before filtering. After removing the statistical outliers, we eliminated these noises and made the entire map cleaner. On the other hand, in the voxel filter, we adjusted the resolution to 0.01, which means there is one point per cubic centimeter. This is a relatively high resolution, so we can't feel the difference in the map in the screenshot, but you can see that the number of points has been significantly reduced from the program output (from 900,000 points to 440,000 points, which is half removed about).

\begin{figure}[! ht]
\centering
\includegraphics[width=1.0 \textwidth]{mapping/pcd-filter.pdf}
\caption{Comparison chart before and after filtering (you may need to zoom in to see it clearly, or try it yourself on your computer). }
\label{fig: pcd-filter}
\end{figure}

The point cloud map provides us with a more basic visual map that allows us to get an overview of what the environment looks like. It is stored in three dimensions, allowing us to quickly browse all corners of the scene and even roam through the scene. A big advantage of point clouds is that they can be efficiently generated directly from RGB-D images without additional processing. Its filtering operation is also very intuitive, and the processing efficiency is acceptable.

However, the use of point clouds to express maps is still very basic, and we may wish to see if the point cloud map can meet the needs of maps mentioned earlier.
\enlargethispage{-3pt}
\begin{enumerate}
\item Positioning requirements: Depends on how the front-end visual odometer is processed. If it is a feature point-based visual odometer, since the feature point information is not stored in the point cloud, it cannot be used for a feature point-based positioning method. If the front end is the ICP of the point cloud, then you can consider performing ICP on the local point cloud to the global point cloud to estimate the pose. However, this requires better accuracy of the global point cloud. In our way of processing point clouds, the point cloud itself is not optimized, so it is not enough.
\item Requirements for navigation and obstacle avoidance: It cannot be used directly for navigation and obstacle avoidance. A pure point cloud cannot represent the information of "there is an obstacle", nor can we make a query in the point cloud of "whether an arbitrary space point is occupied." However, it can be processed on the basis of point clouds to obtain a map form more suitable for navigation and obstacle avoidance.
\item Visualization and interaction: Has basic visualization and interaction capabilities. We can see what the scene looks like and we can walk around in the scene. From a visual point of view, because the point cloud contains only discrete points and no object surface information (such as normals), it is not in line with people's visualization habits. For example, an object in a point cloud map is the same when viewed from the front and from the back, and you can see what is behind it through the object: these are not in line with our daily experience, because we have no information on the surface of the object.
\end{enumerate}

In summary, when we say that a point cloud map is "basic" or "primary", it means that it is closer to the raw data read by the sensor. It has some basic functions, but it is usually used for debugging and basic display, and it is not convenient to use it directly for applications. If we want maps to have more advanced features, point cloud maps are a good starting point. For example, for the navigation function, we can start from a point cloud and build an Occupancy Grid for the navigation algorithm to query whether a point can pass. For another example, the Poisson reconstruction \textsuperscript{\cite{Kazhdan2006}} method commonly used in SfM can reconstruct the object mesh map from the basic point cloud to obtain the surface information of the object. In addition to Poisson reconstruction, Surfel is also a way to express the surface of an object. Using the surface element as the basic unit of the map, it can create a beautiful visual map \textsuperscript{\cite{Stuckler2014}}.

\autoref{fig: poisson-surfel} ~ shows an example of Poisson reconstruction and Surfel. You can see that their visual effects are significantly better than pure point cloud mapping, and they can all be constructed from point clouds. Most of the map forms converted from point clouds are provided in the PCL library, and interested readers can further explore the contents of the PCL library. This book, as an introductory material, does not introduce every map form in detail.

\begin{figure}[! htp]
\centering
\includegraphics[width=1.0 \textwidth]{mapping/poisson-surfel.pdf}
\caption{Schematic representation of Poisson reconstruction and Surfel. }
\label{fig: poisson-surfel}
\end{figure}

\subsection{octree map}
The following introduces a map form that is more commonly used in navigation and has better compression performance: \textbf{octree map}.

In the point cloud map, although we have a three-dimensional structure and voxel filtering to adjust the resolution, the point cloud has several obvious defects:

\begin{itemize}
\item Point cloud maps are usually large, so pcd files are also large. An image with $ 640 $ pixels $ \times480 $ pixels will generate 300,000 space points and requires a lot of storage space. Even after some filtering, the pcd file is very large. And the annoyance is that its "big" is not necessary. Point cloud maps provide a lot of unnecessary detail. We don't particularly care about the folds on the carpet and the shadows in the dark. Putting them on the map is a waste of space. Because of the space taken up, unless we reduce the resolution, we cannot model a larger environment with limited memory. However, reducing the resolution will cause the map quality to decrease. Is there any way to compress and store the map and discard some duplicate information?
\item Point cloud maps cannot handle moving objects. Because we only have "add points" in our approach, but not "remove them when they disappear". In the actual environment, the ubiquity of moving objects makes point cloud maps impractical.
\end{itemize}

What we are going to introduce next is a flexible, compressed map form that can be updated at any time: Octo-map \textsuperscript{\cite{Hornung2013}}.

We know that it is a common practice to model three-dimensional space into many small squares (or voxels). If we cut each side of a small square into two pieces on average, the small square would become eight small squares of the same size. This step can be repeated until the final block size reaches the highest accuracy for modeling. In this process, the matter of "dividing a small square into eight of the same size" is regarded as "expanding from a node into eight children", then the entire process of subdividing from the largest space to the smallest space is An Octo-tree.

As shown in \autoref{fig: octomap} ~, the left side shows a large cube that is continuously divided evenly into eight pieces until it becomes the smallest one. Therefore, the entire large block can be regarded as the root node, and the smallest block can be regarded as the "leaf node". Therefore, in the octree, when we go up one layer from the next layer node, the volume of the map can be expanded by eight times. Let's do a simple calculation: if the block size of the leaf node is 1 cm $ ^ 3 $, when we limit the octree to 10 layers, the total volume that can be modeled is about $ 8 ^{10} \text{ cm} ^ 3=1,073 \text{m} ^ 3 $, which is enough to model a house. Because volume and depth have an exponential relationship, when we use larger depths, the modeled volume will grow very quickly.

\begin{figure}[! ht]
\centering
\includegraphics[width=1.0 \textwidth]{mapping/octomap.pdf}
\caption{Octave schematic. }
\label{fig: octomap}
\end{figure}

The reader may be wondering, in the voxel filter of the point cloud, don't we also limit only one point in a voxel? Why do we say that point clouds occupy space, while octrees save space? This is because, in an octree, we store information in a node if it is occupied. However, the difference is that \textbf{no need to expand this node} when all child nodes of a block are occupied or not occupied. For example, when the map is blank at the beginning, we only need a root node instead of a complete tree. When adding information to the map, most of the octree nodes do not need to be expanded to the leaf level because the actual objects are often connected together and the empty places are often connected together. Therefore, octrees save a lot of storage space than point clouds.

\newpage
Earlier, the nodes of the octree stored information about whether it was occupied. From the point cloud point of view, we can naturally use 0 to indicate blank and 1 to occupy. This 0-1 representation can be stored in one bit to save space, but it seems a bit too simple. Due to the influence of noise, we may see that a point is 0 for a while and 1 for a while; or 0 for most of the time and 1 for a small part of the time; An "unknown" state. Can you describe this in more detail? We will choose to use \textbf{probability} to express whether a node is occupied or not. For example, use a floating point number $ x \in[0,1] $. This $ x $ starts with 0.5. If it is continuously observed to be occupied, then let this value increase continuously; conversely, if it is continuously observed to be blank, then let it continue to decrease.

In this way, we dynamically model the obstacle information in the map. However, there is a small problem with the current method: if you let $ x $ keep increasing or decreasing, it may run outside the $[0,1] $ range, causing inconvenience in processing. So instead of directly describing a node's occupation with probability, we use log-odds to describe it. Let $ y \in \mathbb{R} $ be the logarithm of probability and $ x $ be the probability of 0 \textasciitilde1, then the transformation between them is described by logit transformation:
\begin{equation}
y=\mathrm{logit}(x)=\log \left( \frac{x}{1-x} \right).
\end{equation}

Its inverse transform is
\begin{equation}
x=\mathrm{logit}^{-1}(y)=\frac{\exp(y)}{\exp(y)+1}.
\end{equation}

\enlargethispage{4pt}
It can be seen that when $ y $ changes from $-\infty $ to $ + \infty $, $ x $ changes from 0 to 1. When $ y $ takes 0, $ x $ takes 0.5. Therefore, we might as well store $ y $ to indicate whether the node is occupied. When "occupancy" is continuously observed, let $ y $ increase by one value; otherwise, let $ y $ decrease by one value. When querying probability, use inverse logit transformation to convert $ y $ to probability. In mathematical form, let a node be $ n $ and the observation data be $ z $. Then the logarithm of the probability of a node from the beginning to the time of $ t $ is $ L (n | z_{1: t}) $, and the time of $ t + 1 $ is
\clearpage
\begin{equation}
L(n|z_{1:t+1})=L(n|z_{1:t-1}) + L(n|z_{t}).
\end{equation}

It would be a little more complicated if written in probabilistic form instead of log-probabilistic form:
\begin{equation}
P (n | z_{1: T})=\left[1+ \frac{1-P (n | z_T)}{P (n | z_T)} \frac{1-P (n | z_{1: T-1})}{P (n | z_{1: T-1})} \frac{P (n)}{1-P (n)} \right] ^{-1}.
\end{equation}

With log probability, we can update the entire octree map based on RGB-D data. Suppose we observe a pixel with a depth of $ d $ in an RGB-D image, this illustrates one thing: we \textbf{observed an occupation data at the spatial point corresponding to the depth value, and from the camera light The line from which the heart starts to this point should be} (otherwise it will be blocked). With this information, the octree map can be updated well, and the structure of the motion can be processed.

\subsection{Practice: Octree Map}
The following demonstrates the mapping process of octomap through a program. First, readers are asked to install the octomap library: \url{https://github.com/OctoMap/octomap}. The Octomap library mainly contains octomap maps and octovis (a visualization program), both of which are cmake projects. It mainly depends on doxygen and can be installed by the following command:
\begin{lstlisting}
sudo apt-get install doxygen
\end{lstlisting}

Let's directly demonstrate how to generate an octree map from the previous 5 images, and then draw it.
\begin{lstlisting}[language=c++,caption=slambook/ch13/dense\_RGBD/octomap\_mapping.cpp（片段）]
#include <iostream>
#include <fstream>
using namespace std;

#include <opencv2/core/core.hpp>
#include <opencv2/highgui/highgui.hpp>

#include <octomap/octomap.h>    // for octomap 

#include <Eigen/Geometry> 
#include <boost/format.hpp>  // for formating strings

int main( int argc, char** argv )
{
	// 图像和位姿读取部分略
	cout<<"正在将图像转换为 Octomap ..."<<endl;
Ranch
	// octomap tree 
	octomap::OcTree tree( 0.05 ); // 参数为分辨率
Ranch
	for ( int i=0; i<5; i++ )
	{
		cout<<"转换图像中: "<<i+1<<endl; 
		cv::Mat color=colorImgs[i]; 
		cv::Mat depth=depthImgs[i];
		Eigen::Isometry3d T=poses[i];
Ranch
		octomap::Pointcloud cloud;  // the point cloud in octomap 
Ranch
		for ( int v=0; v<color.rows; v++ )
			for ( int u=0; u<color.cols; u++ )
			{
				unsigned int d=depth.ptr<unsigned short> ( v )[u]; // 深度值
				if ( d==0 ) continue; // 为0表示没有测量到
				if ( d >= 7000 ) continue; // 深度太大时不稳定，去掉
				Eigen::Vector3d point; 
				point[2]=double(d)/depthScale; 
				point[0]=(u-cx)*point[2]/fx;
				point[1]=(v-cy)*point[2]/fy; 
				Eigen::Vector3d pointWorld=T*point;
				// 将世界坐标系的点放入点云
				cloud.push_back( pointWorld[0], pointWorld[1], pointWorld[2] ); 
			}
Ranch
		// 将点云存入八叉树地图，给定原点，这样可以计算投射线
		tree.insertPointCloud( cloud, octomap::point3d( T(0,3), T(1,3), T(2,3) ) );     
	}
Ranch
	// 更新中间节点的占据信息并写入磁盘
	tree.updateInnerOccupancy();
	cout<<"saving octomap ... "<<endl;
	tree.writeBinary( "octomap.bt" );
	return 0;
}
\end{lstlisting}

We used octomap :: OcTree to build the entire map. In fact, octomap provides many kinds of octrees: there are maps and occupancy information. You can also define which variables each node needs to carry. For simplicity, we use the most basic octree map without color information.

Ocotmap provides a point cloud structure inside. It is slightly simpler than PCL's point cloud, and only carries the spatial location information of points. Based on the RGB-D image and camera pose information, we first turn the coordinates of the points to world coordinates, then put them into the point cloud of the octomap, and finally hand them to the octree map. After that, octomap will update the internal occupation probability based on the projection information introduced previously, and finally save it as a compressed octree map. We save the generated map as octomap.bt file. When compiling octovis before, we actually installed a visualization program, octovis. Now, call it to open the map file, and you can see what the map actually looks like.

\autoref{fig: octomap-result} ~ shows the results of the map we built. Since we did not add color information to the map, it will be gray when the map is opened at first. Press "1" to color according to the height information. Readers can familiarize themselves with the octovis interface, including viewing, rotating, and zooming maps.

\begin{figure}[! htp]
\centering
\includegraphics[width=1.0 \textwidth]{mapping/octomap-result.pdf}
\caption{Octree display results at different resolutions. }
\label{fig: octomap-result}
\end{figure}

On the right is the depth limit bar of the octree. Here you can adjust the resolution of the map. Since the default depth we use when constructing is 16 layers, here we show 16 layers, which is the highest resolution, that is, the side length of each small block is 0.05 meters. When we reduce the depth by one layer, the leaf nodes of the octree are raised by one layer, and the side length of each small block is doubled to 0.1 meters. As you can see, we can easily adjust the map resolution to suit different occasions.

Octomap also has some places to explore. For example, we can easily query the occupation probability of any point to design a method for navigating in the map \textsuperscript{\cite{Burri2015}}. Readers can also compare the file sizes of point cloud maps and octree maps. The disk file of the point cloud map generated in the previous section is about 6.9MB, while the octomap is only 56KB, not even one percent of the point cloud map, which can effectively model larger scenes.

\section{\textsuperscript{\ttfamily *} TSDF maps and Fusion series}

At the end of this lecture, we introduce a research direction that is very similar to SLAM but slightly different: real-time 3D reconstruction. This section involves GPU programming, and does not provide reference examples, so it is an optional reading material.

In the previous map model, \textbf{focused on positioning}. Map stitching is placed in the SLAM framework as a subsequent processing step. The reason this framework becomes mainstream is that the positioning algorithm can meet the real-time requirements, and the map processing can be processed at key frames without the need for real-time response. Positioning is usually lightweight, especially when using sparse features or sparse direct methods; correspondingly, the representation and storage of maps are heavyweight. Their scale and computational requirements are large and not conducive to real-time processing. Especially dense maps can often only be calculated at the key frame level.

However, in the current practice, we have not optimized dense maps. For example, when the same chair was observed in both images, we only superimposed the two point clouds based on the pose of the two images to generate a map. Because pose estimation is usually with errors, this direct stitching is often not accurate enough, for example, the point clouds of the same chair cannot be superimposed perfectly. At this time, two ghost images of the chair appear in the map-this phenomenon is sometimes called "ghost image".

This phenomenon is obviously not what we want. We want the reconstruction result to be smooth and complete, which is in line with our understanding of the map. Under this kind of thinking, a kind of practice that takes "building a picture" as the main body and locates the secondary position, that is, the real-time 3D reconstruction to be introduced in this section. As the main goal of 3D reconstruction is to reconstruct accurate maps, GPUs need to be used for acceleration, and even very advanced GPUs or multiple GPUs are required for parallel acceleration, usually requiring heavy computing equipment. In contrast, SLAM is moving towards lightweight and miniaturization. Some solutions even abandon the construction and loopback detection parts, and only retain the visual odometer. And real-time reconstruction is moving towards the reconstruction of large-scale, large-scale dynamic scenes.

Since the emergence of RGB-D sensors, real-time reconstruction using RGB-D images has formed an important development direction. Kinect Fusion \textsuperscript{\cite{Newcombe2011}}, Dynamic Fusion \textsuperscript{\cite{Newcombe2015}} , Elastic Fusion \textsuperscript{\cite{Whelan2015}}, Fusion4D \textsuperscript{\cite{Dou2016}}, Volume Deform \textsuperscript{\cite{Innmann2016}}, and more. Among them, Kinect Fusion has completed the basic model reconstruction, but it is limited to small scenes; the subsequent work is to expand it to large, sports, and even deformed scenes. We see them as rebuilding a large class of work in real time, but due to the large variety, it is impossible to discuss in detail how each works. \autoref{fig: fusions} ~ shows part of the reconstruction results. You can see that these modeling results are very fine, much more delicate than just stitching point clouds.

\begin{figure}[! htp]
\centering
\includegraphics[width=0.9 \textwidth]{mapping/fusions.pdf}
\caption{Various real-time 3D reconstruction models. (A) Kinect Fusion; (b) Dynamic Fusion; (c) Volume Deform; (d) Fusion4D; (e) Elastic Fusion. }
\label{fig: fusions}
\end{figure}

Let's take the classic TSDF map as an example. TSDF is the abbreviation of Truncated Signed Distance Function, which may be translated as \textbf{Truncation Signed Distance Function}. Although it seems inappropriate to call "functions" "maps", we will temporarily call them TSDF maps, TSDF reconstructions, etc. until there is no better translation, as long as there is no deviation in understanding.

Similar to octrees, TSDF maps are also a web format (or, in other words, square) maps, as shown in \autoref{fig: tsdf}. First select the three-dimensional space to be modeled, such as $3 \times3 \times3 \text{m} ^ 3 $, divide this space into many small blocks according to a certain resolution, and store the information inside each small block. The difference is that TSDF maps are stored entirely in video memory instead of memory. Using the parallel nature of the GPU, we can compute and update each voxel in parallel, instead of having to serialize as the CPU traverses the memory area.

\begin{figure}[! t]
\centering
\includegraphics[width=1.0 \textwidth]{mapping/tsdf.pdf}
\caption{TSDF schematic. }
\label{fig: tsdf}
\end{figure}

In each TSDF voxel, the distance between the small block and the nearest object surface is stored. If the patch is in front of the surface of the object, it has a positive value; conversely, if the patch is behind the surface, it is negative. Because the surface of the object is usually a very thin layer, the values ​​that are too large and too small are taken as $1 $and $-1 $, so that the distance after truncation is obtained, which is called TSDF. Then by definition, the place where the TSDF is 0 is the surface itself—or, because of the numerical error, the place where the TSDF changes from negative to positive is the surface itself. In the lower part of \autoref{fig: tsdf} ~, we see that a surface similar to a human face appears where TSDF changes the symbol.

\newpage
\enlargethispage{4pt}
TSDF also has two issues, "positioning" and "mapping", which are very similar to SLAM, but the specific form is slightly different from the previous lectures. Here, the positioning problem mainly refers to how to compare the current RGB-D image with the TSDF map in the GPU to estimate the camera pose. The mapping problem is how to update the TSDF map based on the estimated camera pose. Traditionally, we will also perform bilateral Bayesian filtering on the RGB-D image to remove noise from the depth map.

The positioning of TSDF is similar to the ICP introduced earlier, but due to the parallelization of the GPU, we can perform ICP calculations on the entire depth map and TSDF map without having to calculate feature points first, as in traditional visual odometry. At the same time, because TSDF has no color information, it means that we can \textbf{use only the depth map} and not use the color map to complete the pose estimation, which rids the visual mileage calculation method's dependence on lighting and texture to some extent , Making RGB-D reconstruction more robust \footnote{But then again, it is more dependent on depth maps. }. On the other hand, the mapping part is also a process of updating the values ​​in TSDF in parallel, making the estimated surface smoother and more reliable. Since we don't introduce too much GPU-related content, the specific method will not be elaborated. Please refer to relevant literature for interested readers.

\section{Summary}
This lecture introduces some common map types, especially dense map forms. We see that dense maps can be constructed based on monocular or binocular, while RGB-D maps are often easier and more stable. The maps in this lecture focus on metric maps, and the topological map form is relatively different from SLAM research, so it is not discussed in detail.

\section *{EXERCISE}
\begin{enumerate}
\item comprehension (13.6).
\item Change the dense depth estimation in this tutorial to semi-dense. You can filter out the obvious gradients first.
\item[\optional] Change the monocular dense reconstruction code demonstrated in this lecture from positive depth to inverse depth, and add affine transformation. Has your experiment improved?
\item Can you demonstrate how to do navigation or path planning in an octree?
\item Research Literature \cite{Newcombe2011} explores how TSDF maps perform pose estimation and update. How is it similar to the positioning mapping algorithm we talked about before?
\item[\optional] Investigate the principle and implementation of a uniform-Gaussian hybrid filter.
\end{enumerate}
