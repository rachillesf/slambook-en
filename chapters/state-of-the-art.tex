%! Mode :: "TeX: UTF-8"
\chapter{SLAM: present and future}
\begin{mdframed}
\textbf{main target}
\begin{enumerate}[labelindent = 0em, leftmargin = 1.5em]
\item Learn about the classic SLAM implementation.
\item Through experiments, compare the similarities and differences of various SLAM schemes.
\item Explore the future direction of SLAM.
\end{enumerate}
\end{mdframed}

The working principle of each module in a SLAM system was introduced earlier, which is the result of many years of work by researchers. At present, in addition to these theoretical frameworks, we have also accumulated many excellent open source SLAM solutions. However, since most of their implementations are complex and not suitable for beginners, we have introduced them at the end of this book. I believe that the reader should understand the basic principles by reading the previous content.

\newpage
\section{current open source solution}
This lecture is the summary of the entire book. We will take the reader to see how much the existing SLAM scheme can do. In particular, we focus on solutions that provide open source implementations. In the field of SLAM research, it is not easy to see open source solutions. Often, the content of the theory in the paper is only 20%, and the other 80% is written in the code, which is not mentioned in the paper. It is the selfless dedication of these researchers that has promoted the rapid advancement of the entire SLAM industry, giving subsequent researchers a higher starting point. Before we start SLAM, we should have a deep understanding of similar solutions, and then carry out our own research, so that it will be more meaningful.

The first half of this lecture will take the reader through the current visual SLAM solution and comment on its historical status and advantages and disadvantages. \autoref{table: opensource-slam} ~ enumerates some common open source SLAM schemes, readers can choose the schemes of interest for research and experiment. Due to space limitations, we have selected only a few representative programs, which is certainly not comprehensive. In the second half, we will explore some possible future development directions and present some current research results.

{
\small
\begin{table}[! h]
\caption{Commonly used open source SLAM solutions}
\label{table: opensource-slam}
\begin{tabu}{@{} c | c | X @{}}
\toprule
Scheme name & sensor form & address \\
\midrule
MonoSLAM & Monocular & \url{https://github.com/hanmekim/SceneLib2} \\
PTAM & Monocular & \url{http://www.robots.ox.ac.uk/~gk/PTAM/} \\
ORB-SLAM & Monocular-based & \url{http://webdiis.unizar.es/~raulmur/orbslam/} \\
LSD-SLAM & Monocular-based & \url{http://vision.in.tum.de/research/vslam/lsdslam} \smallskip \\
SVO & Monocular & \url{https://github.com/uzh-rpg/rpg_svo} \\
DTAM & RGB-D & \url{https://github.com/anuranbaka/OpenDTAM} \\
DVO & RGB-D & \url{https://github.com/tum-vision/dvo_slam} \\
DSO & Monocular & \url{https://github.com/JakobEngel/dso} \\
RTAB-MAP & Binocular/RGB-D & \url{https://github.com/introlab/rtabmap} \\
RGBD-SLAM-V2 & RGB-D & \url{https://github.com/felixendres/rgbdslam_v2} \\
Elastic Fusion & RGB-D & \url{https://github.com/mp3guy/ElasticFusion} \\
Hector SLAM & Laser & \url{http://wiki.ros.org/hector_slam} \\
GMapping & Laser & \url{http://wiki.ros.org/gmapping} \\
OKVIS & Multi-head + IMU & \url{https://github.com/ethz-asl/okvis} \\
ROVIO & Monocular + IMU & \url{https://github.com/ethz-asl/rovio} \\
\bottomrule
\end{tabu}
\end{table}
}

\smallskip
\subsection{MonoSLAM}
When it comes to visual SLAM, the first thing that many researchers think of is A. J. Davison's monocular SLAM job \textsuperscript{\cite{Davison2007, Davison2003}}. Professor Davison is a pioneer in the field of visual SLAM research. The MonoSLAM he proposed in 2007 is the first real-time monocular visual SLAM system \textsuperscript{\cite{Davison2007}}, which is considered to be the birthplace of many jobs \footnote{this It is a continuation of his doctoral work. He is also working on miniaturization and low power of SLAM. }. MonoSLAM uses extended Kalman filtering as the back end and tracks very sparse feature points on the front end. Because EKF occupies a clear dominant position in early SLAM, MonoSLAM is also based on EKF. The current state of the camera and all landmark points are used as state quantities to update its mean and covariance.

\autoref{fig: mono-slam} ~ shows what MonoSLAM looks like at runtime. It can be seen that the monocular camera tracks very sparse feature points in an image (and uses active tracking technology). In EKF, the position of each feature point follows a Gaussian distribution, so we can express its mean and uncertainty in the form of an ellipsoid. In the right half of the figure, we can find some small balls distributed in space. The longer they appear in a certain direction, the more uncertain their position in that direction. We can imagine that if a feature point converges, we should be able to see it change from a long ellipsoid (very uncertain in the direction of camera $ Z $) to a small point.

\begin{figure}[! htp]
\centering
\includegraphics[width = 1.0 \textwidth]{past-and-future/mono-slam.pdf}
\caption{MonoSLAM runtime screenshot. Left: Representation of tracking feature points in the image; Right: Representation of feature points in 3D space. }
\label{fig: mono-slam}
\end{figure}

Although this approach seems to have many disadvantages today, it was already a milestone work at that time, because before that the visual SLAM system could not basically run online. It could only rely on a robot to carry a camera to collect data and then perform offline positioning. And mapping. The advances in computer performance and the processing of images in a sparse manner add up to a SLAM system that can run online. From a modern point of view, MonoSLAM has situations such as narrow application scenarios, limited number of road signs, and sparse feature points are very easy to lose. The development of MonoSLAM has also been stopped and replaced by more advanced theory and programming tools. But this does not prevent us from understanding and respecting the work of our predecessors.

\subsection{PTAM}

In 2007, Klein and others proposed PTAM (Parallel Tracking and Mapping) \textsuperscript{\cite{Klein2007}}, which is also an important event in the development of visual SLAM. The significance of PTAM lies in the following two points:
\clearpage
\begin{enumerate}
\item PTAM proposes and implements the parallelization of the tracking and mapping process. We now know that the tracking part needs to respond to the image data in real time, and the optimization of the map does not need to be calculated in real time. Back-end optimization can be performed slowly in the background, and then thread synchronization can be performed when necessary. This is the first time that the concept of front-end and back-end is distinguished in visual SLAM, which led to the design of many later visual SLAM systems (most of the SLAM we see now are divided into front-end and back-end).
\item PTAM is the first solution to use non-linear optimization instead of using traditional filters as the back end. It introduces a keyframe mechanism: instead of processing each image in detail, we string together several key images and then optimize their trajectories and maps. Most of the early SLAMs used EKF filters or their variants, as well as particle filters. After PTAM, visual SLAM research gradually turned to the back end dominated by nonlinear optimization. Because people did not realize the sparseness of back-end optimization before, they felt that the optimized back-end cannot process such large-scale data in real time, and PTAM is a significant counter-example.

\hspace{2em} PTAM is also an augmented reality software that demonstrates cool AR effects (as shown in \autoref{fig: ptam}). Based on the camera pose estimated by PTAM, we can place a virtual object on a virtual plane, which looks just like in a real scene.
\end{enumerate}

\begin{figure}[! ht]
\centering
\includegraphics[width = 1.0 \textwidth]{past-and-future/ptam}
\caption{PTAM demo screenshot. It can provide real-time positioning and mapping, and can also overlay virtual objects on the virtual plane. }
\label{fig: ptam}
\end{figure}

\clearpage
However, from a modern perspective, PTAM is also considered to be one of the early SLAM work combined with AR. Similar to many early work, there are obvious flaws: small scenes, tracking is easy to lose, etc. These were corrected in subsequent plans.

\subsection{ORB-SLAM}

After introducing several historical solutions, let's look at some modern SLAM systems. ORB-SLAM is a very famous \textsuperscript{\cite{Mur-Artal2015}} among the successors of PTAM (see \autoref{fig: orb-slam}). It was proposed in 2015, and it is one of the very complete and very easy to use systems in modern SLAM systems (if not the most complete and easy to use). ORB-SLAM represents a peak of mainstream feature point SLAM. Compared with previous work, ORB-SLAM has the following obvious advantages:

\begin{figure}[! ht]
\centering
\includegraphics[width = 1.0 \textwidth]{past-and-future/orb-slam}
\caption{ORB-SLAM Run screenshot. The left side is the image and the feature points tracked, and the right side is the camera trajectory and the modeled feature point map. Below is its signature three-threaded structure. }
\label{fig: orb-slam}
\end{figure}

\begin{enumerate}
\item supports three modes: monocular, binocular, and RGB-D. This makes it possible to test it on ORB-SLAM, no matter what kind of common sensor we have, it has good versatility.
\item The entire system is calculated around ORB features, including visual odometers and ORB dictionaries for loop detection. It shows that the ORB feature is an excellent compromise between efficiency and accuracy of current computing platforms. ORB is not as time-consuming as SIFT or SURF, and can be calculated in real time on the CPU; compared with simple corner features such as Harris corners, it also has good rotation and scaling invariance. In addition, ORB provides descriptors that enable us to perform loop detection and relocation during large-scale movements.
\item ORB loop detection is its highlight. The excellent loop detection algorithm ensures that ORB-SLAM effectively prevents accumulated errors and can be quickly recovered after being lost. This is because many existing SLAM systems are not perfect. For this reason, ORB-SLAM must load a large ORB dictionary \mbox{file} \footnote{before running. Currently the open source version of ORB-SLAM uses a dictionary in text format, which can be speeded up by changing to a binary format dictionary. }.
\item ORB-SLAM innovatively uses three threads to complete SLAM: a tracking thread that tracks feature points in real time, an optimization thread for local bundle adjustment (Co-visibility Graph, commonly known as \textbf{小 图}), and Loopback detection and optimization thread (Essential Graph commonly known as \textbf{大 图}). Among them, the Tracking thread is responsible for extracting ORB feature points for each new image, and comparing with the nearest key frame, calculating the position of the feature points and roughly estimating the camera pose. The small graph thread solves a Bundle Adjustment problem, which includes feature points and camera poses in local space. This thread is responsible for solving more detailed camera poses and feature point spatial positions. However, with only the first two threads, only a better visual odometer has been completed. The third thread, the big picture thread, performs loop detection on the global map and key frames to eliminate accumulated errors. Because there are too many map points in the global map, the optimization of this thread does not include map points, but only pose maps composed of camera poses.
Hell
\hspace{2em} Following the dual-threaded structure of PTAM, the three-threaded structure of ORB-SLAM has achieved very good tracking and mapping effects, which can ensure the global consistency of the trajectory and the map. This three-thread structure will also be recognized and adopted by subsequent researchers.
\item ORB-SLAM has been optimized a lot around feature points. For example, based on the feature extraction of OpenCV, a uniform distribution of feature points is ensured. When optimizing the pose, a method of cyclic optimization 4 times to get more correct matches is adopted. A more relaxed key frame selection strategy than PTAM and many more. These small improvements make ORB-SLAM far more robust than other solutions: even for poor scenes and poor calibration internal parameters, ORB-SLAM can work smoothly.
\end{enumerate}

These advantages make the ORB-SLAM reach its peak in the feature point SLAM. Many researches use ORB-SLAM as the standard, or carry out subsequent development based on it. Its code is known for its legibility and well-annotated for further understanding by later researchers.

Of course, ORB-SLAM also has some shortcomings. First, because the entire SLAM system uses feature points for calculations, we must calculate the ORB features for each image again, which is very time-consuming. ORB-SLAM's three-thread structure also brings a heavy burden on the CPU, making it only possible to perform real-time operations on CPUs of the current PC architecture, and it is difficult to port to embedded devices. Secondly, ORB-SLAM maps are sparse feature points. At present, there is no open storage and relocation function after reading the map (although it is not difficult in terms of implementation). According to our analysis in the mapping section, the sparse feature point map can only meet our needs for positioning, and cannot provide many functions such as navigation, obstacle avoidance, and interaction. However, if we only use ORB-SLAM to deal with the positioning problem, it seems to be a bit too heavyweight. In contrast, other solutions provide a more lightweight positioning, allowing us to run SLAM on low-end processors, or to allow the CPU to spare other tasks.

\subsection{LSD-SLAM}

LSD-SLAM (Large Scale Direct monocular SLAM) is a SLAM work proposed by J. Engel et al. In 2014 \textsuperscript{\cite{Engel2013, Engel2014}}. Analogous to ORB-SLAM is the feature point, and LSD-SLAM marks the successful application of monocular direct method in SLAM. The core contribution of LSD-SLAM is to apply the direct method to semi-dense monocular SLAM. Not only does it not need to calculate feature points, it can also construct a semi-dense map-here semi-dense means mainly estimated pixel locations with obvious gradients. Its main advantages are as follows:

\begin{enumerate}
\item The direct method of LSD-SLAM is pixel-specific. The author creatively proposed the relationship between pixel gradient and direct method, and the angular relationship between pixel gradient and epipolar direction in dense reconstruction. These are discussed in lectures 8 and 13 of this book. However, LSD-SLAM performs semi-dense tracking on monocular images, and the implementation principle is more complicated than the book's routines.
\item LSD-SLAM implements reconstruction of semi-dense scenes on the CPU, which was rarely seen in previous solutions. Feature point-based methods can only be sparse, and most of the dense reconstruction schemes use RGB-D sensors, or use GPUs to build dense maps \textsuperscript{\cite{Kerl2013}}. Based on years of research on direct method, TUM computer vision group has realized this kind of real-time semi-dense SLAM on CPU.
\item also said before that LSD-SLAM's semi-dense tracking uses some delicate methods to ensure the real-time and stability of the tracking. For example, LSD-SLAM does not use a single pixel or an image block, but takes 5 points at equal distances on the epipolar line to measure its SSD. In depth estimation, LSD-SLAM first initializes the depth with a random number. After the estimation, the depth mean is normalized to adjust the scale. When measuring the depth uncertainty, not only the geometric relationship of the triangulation, but also the angle between the epipolar line and the depth is taken into account. ; Constraints between key frames use similar transformation groups and corresponding Lie algebras $ \bm{\zeta} \in \mathfrak{sim} (3) $ to explicitly express the scale, which can be used in back-end optimization. Taking scenes of different scales into account reduces the scale drift phenomenon.
\end{enumerate}

\autoref{fig: lsd-slam} ~ shows the operation of LSD. We can observe how this delicate semi-dense map is a form between sparse and dense maps. Semi-dense maps model the parts with obvious gradients in the grayscale map, which are displayed in the map, and a large part of them are the edges or textured parts of the surface. LSD-SLAM tracks them and builds key frames, and finally optimizes to obtain such a map. It seems to have more information than a sparse map, but it does not have a complete surface like a dense map (dense maps generally consider it impossible to achieve real-time performance with only the CPU).

\begin{figure}[! ht]
\centering
\includegraphics[width = 1.0 \textwidth]{past-and-future/lsd-slam}
\caption{LSD-SLAM run picture. The upper part is the estimated trajectory and map, and the lower part is the modeled part in the image, that is, the part with better pixel gradient. }
\label{fig: lsd-slam}
\end{figure}

Because LSD-SLAM uses the direct method for tracking, it has both the advantages of the direct method (not sensitive to missing regions of features) and the disadvantages of the direct method. For example, LSD-SLAM is very sensitive to camera internal parameters and exposure, and is easily lost when the camera moves quickly. In addition, in the loop detection part, since there is currently no loop detection method based on the direct method, LSD-SLAM must rely on the feature point method for loop detection, and it has not completely got rid of the calculation of feature points.

\subsection{SVO}

SVO is short for Semi-direct Visual Odoemtry \textsuperscript{\cite{Forster2014}}. It is a visual odometer based on sparse direct method proposed by Forster et al. In 2014. According to the author, it should be called "semi-direct" method, but according to the conceptual framework of this book, it is better to call "sparse direct method". \textbf{Semi-direct} means in the original text a mixture of feature points and direct methods: SVO tracks some key points (corners, no descriptors), and then uses the information around these key points like direct methods Estimate camera movement and its position (as shown by \autoref{fig: lsd-slam}). In the implementation, SVO uses the small blocks of $ 4 \times4 $ around the key points for block matching to estimate the camera's own motion.

Compared with other solutions, the biggest advantage of SVO is that it is extremely fast. Because it uses a sparse direct method, it does not have to laboriously calculate descriptors, and it does not have to process as much information as dense and semi-dense. Therefore, it can achieve real-time performance even on low-end computing platforms, while on PC Can reach the speed of more than 100 frames per second. In the subsequent SVO 2.0, the speed reached an astonishing 400 frames per second. This makes SVO ideal for applications where computing platforms are limited, such as the positioning of drones and handheld AR/VR devices. UAV is also the target application platform for the author to develop SVO.

\begin{figure}[H]
\centering
\includegraphics[width = 1.0 \textwidth]{past-and-future/svo}
\caption{SVO A picture of the key points. }
\label{fig: svo}
\end{figure}

Another innovation of SVO is to propose the concept of a depth filter and derive a depth filter based on a uniform-Gaussian mixture distribution. This is mentioned in lecture 13 of this book, but because the principle is more complicated, we have not explained it in detail. SVO uses this filter for the position estimation of key points, and uses inverse depth as a parameterized form to enable it to better calculate the position of feature points.

The open source version of SVO code is clear and easy to read, which is very suitable for readers to analyze as the first SLAM instance. However, there are some problems with the open source version of SVO:

\begin{enumerate}
\item Because the target application platform is a drone's overhead camera, the objects in its field of view are mainly the ground, and the camera's movement is mainly horizontal and vertical movement. Many details of SVO are designed around this application, which makes it in Poor performance in head-up cameras. For example, SVO uses the method of factoring the $ \bm{H} $ matrix instead of the traditional $ \bm{F} $ or $ \bm{E} $ matrix during monocular initialization, which requires the feature points to be on the plane on. This assumption is true for a head-up camera, but it is usually not true for a head-up camera, which may cause initialization to fail. For another example, SVO uses the translation amount as a strategy for determining new key frames when selecting key frames, without considering the amount of rotation. This is also effective in a drone top-down configuration, but it is easily lost in a head-up camera. Therefore, if you want to use SVO in a head-up camera, you must modify it yourself.
\item SVO has abandoned the back-end optimization and loop detection part for speed and light weight, and basically has no mapping function. This means that there is necessarily a cumulative error in the pose estimation of SVO, and it is not easy to relocate after loss (because there are no descriptors for loop detection). So, instead of calling it a complete SLAM, we call it a VO.
\end{enumerate}

\subsection{RTAB-MAP}

After introducing several monocular SLAM solutions, let's look at some SLAM solutions on RGB-D sensors. Compared to monocular and binocular, the principle of RGB-D SLAM is much simpler (although not necessarily implemented), and it can build dense maps on the CPU in real time.

RTAB-MAP (Real Time Appearance-Based Mapping) \textsuperscript{\cite{Labbe2014}} is a classic solution in RGB-D SLAM. It implements everything that should be in RGB-D SLAM: feature-based visual odometry, bag-of-words-based loop detection, back-end pose map optimization, and point cloud and triangular mesh maps. Therefore, RTAB-MAP provides a complete (but somewhat large) RGB-D SLAM scheme. At present, we can directly obtain its binary program from ROS. In addition, we can also obtain its application on Google Project Tango (such as \autoref{fig: rtabmap} ~).

\begin{figure}[! ht]
\centering
\includegraphics[width = 1.0 \textwidth]{past-and-future/rtabmap}
\caption{RTAB-MAP running example on Google Project Tango. }
\label{fig: rtabmap}
\end{figure}

RTAB-MAP supports some common RGB-D and binocular sensors, such as Kinect, Xtion, etc., and provides real-time positioning and mapping functions. However, due to the high degree of integration, it is difficult for other developers to carry out secondary development based on it, so RTAB-MAP is more suitable for SLAM applications rather than research.

\subsection{other}
In addition to these open source solutions, readers can find many other research on websites such as \url{openslam.org}, such as DVO-SLAM \textsuperscript{\cite{Kerl2013a}}, RGBD-SLAM-V2 \textsuperscript{\cite{Endres2014}}, DSO \textsuperscript{\cite{Engel2016}}, and some Kinect Fusion related work, etc. With the development of the times, newer and better open-source SLAM works will also appear in people's vision. I will not introduce them one by one due to space limitations.
Hell
% \subsection{OKVis}
% In addition to visual SLAM, there is another called VIO
% \subsection{some work not open sourced}

\section{Future SLAM Topics}
After reading the existing schemes, let's discuss some future development directions \footnote{There is a part of my understanding here, which may not be completely correct. }. Generally speaking, there are two major types of SLAM's future development trends: one is to move towards lightweight and miniaturization, so that SLAM can run well on small devices such as embedded or mobile phones, and then consider applications using it as the underlying function . After all, in most occasions, our real purpose is to realize the functions of robots and AR/VR devices, such as sports, navigation, teaching, and entertainment, and SLAM provides its own pose estimation for upper-level applications. In these applications, we do not want SLAM to occupy all computing resources, so there is a very strong demand for SLAM miniaturization and lightweight. On the other hand, it uses high-performance computing equipment to achieve precise 3D reconstruction and scene understanding. In these applications, our goal is to reconstruct the scene perfectly, without much limitation on the portability of computing resources and devices. Since GPUs can be used, this direction and deep learning also have a combination.

\subsection{Vision + Inertial Navigation SLAM}

First, we want to talk about a direction with a strong application background: vision-inertial navigation fusion SLAM solution. Whether it is an actual robot or a hardware device, it usually does not carry only one kind of sensor, but often a fusion of multiple sensors. Researchers in academia love the "Big Clean Problem", such as visual SLAM with a single camera. But friends in the industry are more focused on making the algorithm more practical, and have to face some complicated and trivial scenarios. In this application context, SLAM using fusion of vision and inertial navigation has become a focus of attention.

The inertial sensor (IMU) can measure the angular velocity and acceleration of the sensor body, which is considered to have obvious complementarity with the camera sensor, and has great potential to obtain a more complete SLAM system after fusion \textsuperscript{\cite{Gui2015}}. Why do you say that?

\begin{enumerate}
\item Although the IMU can measure angular velocity and acceleration, these quantities have significant drift (Drift), which makes the pose data obtained by two integrations very unreliable. For example, if we put the IMU on a table without moving it, the position obtained by integrating its readings will drift out of the air. However, for fast movements in a short time, the IMU can provide some good estimates. This is exactly the weakness of the camera.
Hell
\clearpage
\hspace{2em} When the motion is too fast, the camera (of the rolling shutter) will have motion blur, or there is too little overlap between the two frames to allow feature matching, so pure visual SLAM is very afraid of fast motion. With IMU, we can maintain a good pose estimation even during the period when the camera data is invalid, which is not possible with pure visual SLAM.
Hell
\item Compared to IMU, camera data is basically free of drift. If the camera is fixed in place, the pose estimation of the visual SLAM (in a static scene) is also fixed. Therefore, camera data can effectively estimate and correct drift in IMU readings, so that pose estimation after slow motion is still valid.
Hell
\item When the image changes, we basically cannot know whether the camera itself has moved or the external conditions have changed, so pure visual SLAM is difficult to handle dynamic obstacles. The IMU can sense its own motion information and mitigate the impact of dynamic objects to some extent.
\end{enumerate}

All in all, we see that the IMU provides a better solution for fast motion, and the camera can solve the drift problem of the IMU under slow motion-in this sense, they are complementary.

\begin{figure}[! thp]
\centering
\includegraphics[width = 1.0 \textwidth]{past-and-future/visensor}
\caption{More and more cameras are integrating IMU devices. }
\label{fig: visensor}
\end{figure}

Of course, although it sounds good, whether it is theory or practice, VIO (Visual Inertial Odometry) is quite complicated. Its complexity is mainly derived from the fact that the IMU measures acceleration and angular velocity, so it has to introduce kinematic calculations. At present, the framework of VIO has been shaped into two categories: Loosely Coupled and Tightly Coupled \textsuperscript{\cite{Martinelli2014}}. Loose coupling means that the IMU and the camera separately perform their own motion estimation, and then fuse their pose estimation results. Tight coupling refers to combining the state of the IMU with the state of the camera, constructing the equations of motion and observations, and then performing the state estimation-this is very similar to the theory we introduced earlier. We can predict that the tight coupling theory will also be divided into two directions based on filtering and optimization. In terms of filtering, traditional EKF \textsuperscript{\cite{Bloesch2015}} and improved MSCKF (Multi-State Constraint KF) \textsuperscript{\cite{Li2013}} have achieved certain results, and researchers have also performed EKF In-depth discussion (such as observability \textsuperscript{\cite{Huang2014}}); there is also a corresponding solution for optimization \textsuperscript{\cite{Leutenegger2015, Forster2015}}. It is worth mentioning that although the optimization method has occupied the mainstream in pure visual SLAM, in VIO, because the data frequency of IMU is very high, the amount of calculation required to optimize the state is larger, so it is still in the filtering and Optimization coexisting phase \textsuperscript{\cite{Tkocz2015, Usenko2016}}. Because it is too complicated and limited in space, I can only introduce this direction here.

VIO provides a very effective direction for the miniaturization and low cost of SLAM in the future. And combined with the sparse direct method, we are expected to achieve good SLAM or VO effects on low-end hardware, which is very promising.

\subsection{Semantic SLAM}
Another major direction of SLAM is to combine it with deep learning technology. So far, SLAM solutions are at the level of feature points or pixels. We don't know what these feature points or pixels come from. This makes SLAM in computer vision not very similar to our human approach. At least we never see the feature points ourselves, and we don't judge the direction of our own movement based on the feature points. What we see are objects, judging their distance through the left and right eyes, and then inferring the camera's movement based on their movement in the image.

Long ago, researchers tried to incorporate object information into SLAM. For example, the literature \cite{Nuechter2008, Civera2011, Koppula2011, Anand2012} has combined object recognition with visual SLAM to build a map with object labels. On the other hand, by introducing the label information into the objective function and constraints of the BA or optimization side, we can combine the position of the feature points with the label information to optimize \textsuperscript{\cite{Fioraio2013}}. These works can be called semantic SLAM. In summary, the combination of SLAM and semantics has two main aspects: \textsuperscript{\cite{Cadena2016}}:

\begin{enumerate}
\item semantics help SLAM. Traditional object recognition and segmentation algorithms often only consider one picture, and in SLAM we have a moving camera. If we put object labels on the pictures during the exercise, we can get a labeled map. In addition, object information can also bring more conditions for loop detection and BA optimization.
\item SLAM helps semantics. Both object recognition and segmentation require a lot of training data. In order for the classifier to recognize objects from various angles, it is necessary to collect data on the objects from different perspectives and then perform manual calibration, which is very hard. In SLAM, because we can estimate the camera's motion, we can automatically calculate the position of the object in the image, saving the cost of manual calibration. If there is automatically generated sample data with high-quality annotations, it can greatly speed up the training process of the classifier.
\end{enumerate}

\begin{figure}[! thp]
\centering
\includegraphics[width = 1.0 \textwidth]{past-and-future/semantic-slam}
\caption{Some results of semantic SLAM, the left and right images are from the literature \cite{Anand2012, Salas-Moreno2014}. }
\label{fig: semantic-slam}
\end{figure}

Before the widespread application of deep learning, we could only use traditional tools such as support vector machines and conditional random fields to segment and identify objects or scenes, or directly compare observation data with samples in the database \textsuperscript{\cite{Salas- Moreno2013, Salas-Moreno2014}}, trying to build a semantic map \textsuperscript{\cite{Anand2012, Stueckler2012, Kostavelis2013, Couprie2013}}. Because these tools have limitations on the accuracy of their classifications, their results are often unsatisfactory. With the development of deep learning, we started to use the network to recognize, detect and segment images more and more accurately \textsuperscript{\cite{Deng2009, Krizhevsky2012, He2015, Ren2015, Long2014, Zheng2015}}. This laid a better foundation for building accurate semantic maps \textsuperscript{\cite{Gupta2014}}. We are seeing that scholars are gradually introducing neural network methods to object recognition and segmentation in SLAM, and even pose estimation and loop detection in SLAM itself \textsuperscript{\cite{Konda2015, Kendall2015, Hou2015}}. Although these methods have not yet become mainstream, combining SLAM with deep learning to process images is also a promising research direction.

\subsection{Future of SLAM}
Besides, SLAM \textsuperscript{\cite{An2012, Zhou2015, Benedettelli2012}} based on line/area features, SLAM \textsuperscript{\cite{Saarinen2013, Maddern2012, Wang2008}} in dynamic scenes, SLAM \textsuperscript{\cite{Zou2013, Gil2010a, Vidal-Calleja2011}}, etc., are all places where researchers are interested and working hard. According to the literature \cite{Cadena2016}, visual SLAM has gone through three major eras: asking questions, finding algorithms, and improving algorithms. We are currently in the third era, facing how to further improve in the existing framework, so that the visual SLAM system can run stably under various interference conditions. This step requires the unremitting efforts of many researchers.

Of course, no one can predict the future, and we are not sure if one day, the entire framework will be overthrown and overwritten by new technologies. But even then, our contribution today will still be meaningful. Without today's research, there will be no future development. Finally, I hope that after reading this book, the reader will have a full understanding of the existing SLAM system. We also look forward to your contribution to SLAM research!

\section *{EXERCISE}
\begin{enumerate}
\item Choose any of the open source SLAM systems mentioned in this lecture, compile and run it on your machine, and experience the process intuitively.
\item You should already be able to understand most SLAM related papers. Pick up paper and pen and start your research!
\end{enumerate}