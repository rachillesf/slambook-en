% !Mode:: "TeX:UTF-8"
\chapter{nonlinear optimization}
\label{cpt:6}
\begin{mdframed}
\textbf{main target}
\begin{enumerate}[labelindent=0em,leftmargin=1.5em]
\item understands the meaning and handling of least squares.
\item Understand the descending strategies of Gauss-Newton and Levenburg-\\Marquadt.
\item Learn the basic usage of the Ceres library and the g2o library.
\end{enumerate}
\end{mdframed}

In the previous few lectures, we introduced the equations of motion and observation equations of the classical SLAM model. Now we know that the poses in the equation can be described by the transformation matrix and then optimized with Lie algebra. The observation equation is given by the camera imaging model, where the internal reference is fixed with the camera and the external reference is the pose of the camera. Thus, we have clarified the specific expression of the classic SLAM model in the visual situation.

However, due to the existence of noise, the equations of the equation of motion and the equation of observation must not be exactly true. Although the camera fits the pinhole model very well, unfortunately the data we get is usually affected by various unknown noises. Even if we have a high-precision camera, the equations of motion and observation equations can only be approximated. Therefore, instead of assuming that the data must conform to the equation, it is better to discuss how to make an accurate state estimate in the noisy data.

Solving the state estimation problem requires a certain degree of optimization background knowledge. This section introduces the basic unconstrained nonlinear optimization method and introduces how the optimized libraries g2o and Ceres are used.

\newpage
\includepdf{resources/other/ch6.pdf}

\newpage
\section{Status estimation problem}
\subsection{Batch state estimation and maximum a posteriori estimate}
Following the previous lectures, we review the classic SLAM model discussed in the second lecture. It consists of an equation of motion and an observing equation, as shown by the formula \eqref{eq:slamproblem}:
\begin{equation}
\left\{ \begin{array}{l}
{\bm{x}_k} = f\left( {{\bm{x}_{k - 1}},{\bm{u}_k}} \right) + \bm{w}_k\\
{\bm{z}_{k,j}} = h\left( {{ \bm{y}_j},{ \bm{x}_k}} \right)+ \bm{v}_{k, j}
\end{array} \right.
\end{equation}

Through the knowledge of Lecture 4, we learned that $\bm{x}_k$ is the pose of the camera and can be described by $\mathrm{SE}(3)$. As for the observation equation, the fifth lecture has already explained, that is, the pinhole camera model. In order to give readers a deeper impression of them, we may wish to discuss their specific parametric forms. First, the pose variable $\bm{x}_k$ can be expressed by $\bm{T}_k \in \mathrm{SE}(3) $. Second, the equation of motion is related to the specific form of the input, but there is no particularity in the visual SLAM (as in the case of ordinary robots and vehicles), we will not talk about it for the time being. The observation equation is given by the pinhole model. Suppose that an observation is made on the road sign $\bm{y}_j$ at $\bm{x}_k$, corresponding to the pixel position $\bm{z}_{k,j}$ on the image, then the observation The equation can be expressed as
\begin{equation}
s \bm{z}_{k,j}= \bm{K} (\bm{R}_k {\bm{y}_j}+\bm{t}_k).
\end{equation}
Where $\bm{K}$ is the camera's internal parameter, $s$ is the distance of the pixel, and is the third of $(\bm{R}_k {\bm{y}_j}+\bm{t}_k)$ Component. If the pose is described using the transformation matrix $\bm{T}_k$, the landmark point $\bm{y}_j$ must be described in homogeneous coordinates and converted to non-homogeneous coordinates after the calculation is completed. If you are not familiar with this process, please go back to the previous lecture.

Now, consider what happens when the data is affected by noise. In the motion and observation equations, we \textbf{usually} assume that two noise terms $\bm{w}_k, \bm{v}_{k,j}$ satisfy the zero mean Gaussian distribution, like this:
\begin{equation}
{\bm{w}_k} \sim \mathcal{N}\left( {\bm{0},{\bm{R}_k}} \right),{\bm{v}_k} \sim \mathcal {N}\left( {\bm{0},{{{\bm{Q}}}_{k,j}}} \right).
\end{equation}
Where $\mathcal{N}$ represents a Gaussian distribution, $\bm{0}$ represents a zero mean, $\bm{R}_k, and \bm{Q}_{k,j}$ is a covariance matrix. Under the influence of these noises, we hope to infer the pose $\bm{x}$ and the map $\bm{y}$ through the noisy data $\bm{z}$ and $\bm{u}$ (and Their probability distribution), which constitutes a state estimation problem.

The methods for dealing with this state estimation problem are roughly divided into two types. Since these data are coming over time during the SLAM process, intuitively, we should hold an estimate of the current time and then update it with new data. This method is called \textbf{incremental}, or \textbf{filter}. For a long time in history, researchers have used filters, especially the Extended Kalman Filter (EKF) and its derivatives to solve it. The other way is to "snap" the data together, which is called \textbf{batch}. For example, we can put all the input and observation data from 0 to $k$ at the moment, and ask, under such input and observation, how to estimate the trajectory and map from 0 to $k$?

These two different approaches lead to many different estimation methods. In general, the incremental method only cares about the state estimate of \textbf{current time}, $\bm{x}_k$, and does not consider the previous state; relatively, the batch method can be larger in \textbf{ Range} is optimized and is considered superior to the traditional filter \textsuperscript{\cite{Strasdat2012}}, becoming the mainstream method of current visual SLAM. In extreme cases, we can let the robot or drone collect data at all times and bring it back to the computing center for unified processing, which is the mainstream practice of SfM (Structure from Motion). Of course, this extreme situation is obviously not \textbf{real time}, and does not conform to the SLAM application scenario. So in SLAM, practical methods are usually a compromise. For example, we fixed some historical trajectories and optimized only some trajectories near the current time. This is the \textbf{sliding window estimation} method to be mentioned later.

In theory, batch methods are easier to introduce. At the same time, it is understood that the batch method is also easier to understand the incremental method. Therefore, in this section, we focus on the batch optimization method based on nonlinear optimization. The Kalman filter and more in-depth knowledge are left to the back-end chapters. Since the discussion is on a batch approach, consider all the moments from 1 to $N$ and assume that there are $M$ landmark points. Define the robot pose and landmark point coordinates at all times:
\[
\bm{x}=\{ \bm{x}_1, \ldots, \bm{x}_N \}, \quad \bm{y} = \{\bm{y}_1, \ldots, \bm{ y}_M \}.
\]
Similarly, $\bm{u}$ without subscripts indicates input at all times, and $\bm{z}$ indicates observations at all times. Then we say that the estimation of the state of the robot, from the point of view of probability, is the condition that the input data $\bm{u}$ and the observed data $\bm{z}$ are known, and the state $\bm{ Conditional probability distribution for x}, \bm{y}$:
\begin{equation}
P( \bm{x},\bm{y} | \bm{z}, \bm{u}).
\end{equation}
In particular, when we don't know the control input, only one image is considered, that is, only the data brought by the observation equation is considered, which is equivalent to estimating $P( \bm{x},\bm{y} | \bm{ The conditional probability distribution of z})$, also known as Structure from Motion (SfM), is how to reconstruct the three-dimensional structure \textsuperscript{\cite{Agarwal2009}} from many images.

In order to estimate the conditional distribution of state variables, Bayes' rule is used to:
\begin{equation}
P\left( { \bm{x},\bm{y}| \bm{z}, \bm{u}} \right) = \frac{{P\left( {\bm{z},\bm {u}|\bm{x},\bm{y}} \right)P\left( \bm{x}, \bm{y} \right)}}{{P\left( \bm{z} ,\bm{u}\right)}} \propto \underbrace{P\left( { \bm{z},\bm{u}| \bm{x},\bm{y} } \right)}_ {\text{liker}} \underbrace{P\left( \bm{x},\bm{y} \right)}_{\text{priority}}.
\end{equation}
Bayes' rule is called \textbf{posterior probability} on the left, $P(\bm{z}|\bm{x})$ on the right is called \textbf{Likehood}, and another part is $ P(\bm{x})$ is called \textbf{priority} (Prior). \textbf{It is difficult to directly find the posterior distribution, but to find a state optimal estimate}, so that Maximize a Posterior (MAP) is feasible:
\begin{equation}
{(\bm{x},\bm{y})^*}_{\mathrm{MAP}} = \arg {\mathop{\rm max}\nolimits}\ P \left( {\bm{x} ,\bm{y}|\bm{z},\bm{u}} \right) = \arg \max P(\bm{z},\bm{u}|\bm{x},\bm{ y})P(\bm{x},\bm{y}).
\end{equation}
Note that the denominator part of Bayes' rule is independent of the state to be estimated $\bm{x}, \bm{y}$ and can therefore be ignored. The Bayesian rule tells us that solving the maximum posterior probability \textbf{ is equivalent to maximizing the likelihood and the prior product}. Further, of course we can also say, sorry, I don't know where the robot pose or road sign is, and there is no \textbf{priority}. Then, you can solve the \textbf{Maximize Likelihood Estimation} (MLE):
\begin{equation}
{ (\bm{x},\bm{y})^*}_{\mathrm{MLE}} = \arg \max P( \bm{z},\bm{u}| \bm{x}, \bm{y}).
\end{equation}

Intuitively, the likelihood is “what kind of observation data may be produced in the current position”. Since we know the observed data, the maximum likelihood estimate can be understood as: "\textbf{what state is most likely to produce the currently observed data"}. This is the intuitive meaning of maximum likelihood estimation.

\subsection{Least squares lead}
So how do you find the maximum likelihood estimate? We say that under the assumption of Gaussian distribution, the maximum likelihood can have a simpler form. Review the observation model for an observation:
\[
{\bm{z}_{k,j}} = h\left( {{ \bm{y}_j},{ \bm{x}_k}} \right)+ \bm{v}_{k, j},
\]
Since we assumed the noise term ${\bm{v}_k} \sim \mathcal{N}\left( {\bm{0},{{{\bm{Q}}}_{k,j}}} \right)$, so the conditional probability of the observed data is:
\[
P( \bm{z}_{j,k} | \bm{x}_k, \bm{y}_j ) = N\left( h(\bm{y}_j, \bm{x}_k), \bm{Q}_{k,j} \right).
\]
It is still a Gaussian distribution. Considering the maximum likelihood estimate for a single observation, you can use \textbf{minimize the negative logarithm} to find the maximum likelihood of a Gaussian distribution.

We know that the Gaussian distribution has a good mathematical form under the negative logarithm. Consider any high-dimensional Gaussian distribution $\bm{x} \sim \mathcal{N}(\bm{\mu}, \bm{\Sigma})$, whose probability density function is expanded as:
\begin{equation}
P\left( \bm{x} \right) = \frac{1}{{\sqrt {{{(2\pi )}^N}\det ( \bm{\Sigma} )} }}\exp \left( { - \frac{1}{2}{{\left( { \bm{x} - \bm{\mu} } \right)}^\mathrm{T}}{ \bm{\Sigma} ^{ - 1}}\left( { \bm{x} - \bm{\mu} } \right)} \right).
\end{equation}
If you take a negative logarithm, it becomes:
\begin{equation}
- \ln \left( {P\left( \bm{x} \right)} \right) = \frac{1}{2}\ln \left( {{{\left( {2\pi } \right )}^N}\det \left( \bm{\Sigma} \right)} \right) + \frac{1}{2}{\left( { \bm{x} - \bm{\mu} } \right)^\mathrm{T}}{\bm{\Sigma} ^{ - 1}}\left( {\bm{x} - \bm{\mu} } \right).
\end{equation}
Since the logarithmic function is monotonically increasing, maximizing the original function is equivalent to minimizing the negative logarithm. When the $\bm{x}$ of the above formula is minimized, the first item has nothing to do with $\bm{x}$ and can be omitted. Thus, as long as the quadratic term on the right side is minimized, the maximum likelihood estimate for the state is obtained. Substituting the observation model of SLAM is equivalent to seeking:
\begin{equation}
\begin{aligned}
(\bm{x}_k,\bm{y}_j)^* &= \arg \max \mathcal{N}(h(\bm{y}_j, \bm{x}_k), \bm{Q }_{k,j}) \\ &= \arg \min \left( {{{\left( {{ \bm{z}_{k,j}} - h\left( {{\bm{x }_k},{\bm{y}_j}} \right)} \right)}^\mathrm{T}} \bm{Q}_{k,j}^{ - 1}\left( {{\ Bm{z}_{k,j}} - h\left( {{\bm{x}_k},{\bm{y}_j}} \right)} \right)} \right).
\end{aligned}
\end{equation}

We have found that this equation is equivalent to a quadratic form that minimizes the noise term (ie, the error). This quadratic type is called \textbf{Mahalanobis distance}, also called \textbf{Markov distance}. It can also be thought of as the Euclidean distance (two norms) weighted by $\bm{Q}_{k,j}^{-1}$, where $\bm{Q}_{k,j} ^{-1}$ is also called \textbf{information matrix}, which is the inverse of the Gaussian distribution covariance matrix.

Now let's consider the data for the batch time. It is usually assumed that the inputs and observations at each moment are independent of each other, which means that the inputs are independent, the observations are independent, and the inputs and observations are independent. So we can factor factor the joint distribution:
\begin{equation}
P\left( {\bm{z},\bm{u}|\bm{x},\bm{y}} \right) = \prod\limits_k {P\left( {{\bm{u}_k }|{\bm{x}_{k - 1}},{\bm{x}_k}} \right)} \prod\limits_{k,j} {P\left( {{\bm{z} _{k,j}}|{\bm{x}_k},{\bm{y}_j}} \right)},
\end{equation}
This shows that we can handle motion and observation at all times independently. Define the error between each input and observation data and the model:
\begin{equation}
\begin{array}{l}
{\bm{e}_{u,k}} = {\bm{x}_k} - f\left( {{\bm{x}_{k - 1}},{\bm{u}_k} } \right)\\
{\bm{e}_{z,j,k}} = {\bm{z}_{k,j}} - h\left( {{\bm{x}_k},{\bm{y} _j}} \right),
\end{array}
\end{equation}
Then, minimizing the Mahalanobis distance between all time estimates and real readings is equivalent to finding the maximum likelihood estimate. Negative logarithms allow us to turn the product into a sum:
\begin{equation}
\label{eq:least-square}
\min J (\bm{x},\bm{y}) = \sum\limits_k {\bm{e}_{u,k}^\mathrm{T} \bm{R}_k^{ - 1} { \bm{e}_{u,k}}} + \sum\limits_k {\sum\limits_j {\bm{e}_{z,k,j}^\mathrm{T} \bm{Q}_ {k,j}^{ - 1}{\bm{e}_{z,k,j}}} } .
\end{equation}
This gives a \textbf{Least Square Problem}, whose solution is equivalent to the maximum likelihood estimate of the state. Intuitively, due to the existence of noise, when we substitute the estimated trajectories and maps into the motion and observation equations of SLAM, they are not perfect. What should I do then? We perform \textbf{fine tuning} on the estimated state, which causes the overall error to drop. Of course, this decline is also limited, it will generally reach a \textbf {minimum value}. This is a typical nonlinear optimization process.

Looking closely at \eqref{eq:least-square}, we find that the least squares problem in SLAM has some specific structure:

\begin{itemize}
\item First, the objective function of the whole problem consists of a number of error (weighted) quadratic forms. Although the overall state variable has a high dimensionality, each error term is simple and is only related to one or two state variables. For example, the motion error is only related to $\bm{x}_{k-1}, \bm{x}_k$, and the observation error is only related to $\bm{x}_k, \bm{y}_j$. This relationship will make the whole problem have a form of \textbf{sparse}, which we will see in the backend chapter.

\item Second, if you use Lie algebra to represent increments, the problem is the least squares problem of \textbf{unconstrained}. However, if the pose is described by the rotation matrix/transformation matrix, the constraint of the rotation matrix itself is introduced, that is, $\mathrm{st}\ \bm{R}^\mathrm{T} \bm{R} is added to the problem. =\bm{I} \text{and} \det (\bm{R})=1$ This is a big condition. Additional constraints make optimization more difficult. This reflects the advantages of Lie algebra.

\item Finally, we used a quadratic metric error. The distribution of errors will affect the weight of this item throughout the problem. For example, if a certain observation is very accurate, then the covariance matrix will be "small" and the information matrix will be "large", so this error term will have a higher weight in the whole problem. We will see later that there are some problems, but we will not discuss them at present.
\end{itemize}

Now, we show how to solve this least squares problem, which requires some \textbf{the basic knowledge of nonlinear optimization}. In particular, we will explore how it is solved for such a general unconstrained nonlinear least squares problem. In the following lectures, we will use the results of this lecture extensively to discuss its application in the SLAM front-end and back-end.

\subsection{Example: Batch status estimation}
I find it a better idea to give a simple example here. Consider a very simple discrete time system:
\begin{equation}
\begin{array}{lll}
{x_k} &= {x_{k - 1}} + {u_k} + {w_k},&\qquad w_k \sim \mathcal{N}\left( {0,Q_k} \right)\\
{z_k} &= {x_k} + {n_k},&\qquad {n_k}\sim \mathcal{N}\left( {0,R_k} \right)
\end{array}
\end{equation}
This can express a car that moves forward or backward along the $x$ axis. The first formula is the equation of motion, $u_k$ is the input, $w_k$ is the noise, the second is the observation equation, and $z_k$ is the measurement of the position of the car. Take the time $k=1, \ldots, 3$, and now I want to estimate the state based on the existing $v,y$. Let the initial state $x_0$ be known. Let's derive the maximum likelihood estimate for the batch state.

First, let the batch state variable be $\bm{x} = [x_0,x_1, x_2, x_3]^\mathrm{T}$, and make the batch observation $\bm{z} = [z_1,z_2,z_3]^ \mathrm{T}$, define $\bm{u}=[u_1,u_2,u_3]^\mathrm{T}$ in the same way. According to previous derivation, we know that the maximum likelihood estimate is:
\begin{equation}
\begin{aligned}
{\bm{x}_{\mathrm{map}}^*} &= \arg \max P(\bm{x}|\bm{u},\bm{z}) = \arg \max P( \bm{u},\bm{z}|\bm{x})\\
 &= \prod\limits_{k = 1}^3 {P({u_k}|{x_{k - 1}},{x_k})\prod\limits_{k = 1}^3 {P\left( { {z_k}|{x_k}} \right)} },
\end{aligned}
\end{equation}
For each specific item, such as the equation of motion, we know that:
\begin{equation}
P({u_k}|{x_{k - 1}},{x_k}) = \mathcal{N}({x_k} - {x_{k - 1}},{Q_k}),
\end{equation}
The observation equations are similar:
\begin{equation}
P\left( {{z_k}|{x_k}} \right) = \mathcal{N}\left( {{x_k},{R_k}} \right).
\end{equation}
Based on these methods, we can actually solve the above batch state estimation problem. According to the previous description, the error variable can be constructed:
\begin{equation}
{e_{u,k}} = {x_k} - {x_{k - 1}} - {u_k}, \quad {e_{z,k}} = {z_k} - {x_k},
\end{equation}
Then the objective function of least squares is:
\begin{equation}
\min \sum\limits_{k = 1}^3 {e_{u,k}^\mathrm{T} Q_k^{ - 1}{e_{u,k}}} + \sum\limits_{k = 1 }^3 {e_{z,k}^\mathrm{T}{R^{ - 1}_k}{e_{z,k}}}.
\end{equation}

In addition, this system is a linear system, and we can easily write it as a vector form. Define the vector $\bm{y}=[\bm{u}, \bm{z}]^\mathrm{T}$, then write the matrix $\bm{H}$ so that:
\begin{equation}
\bm{y}-\bm{H}\bm{x} = \bm{e} \sim \mathcal{N}(\bm{0}, \boldsymbol{\Sigma}).
\end{equation}
Then:
\begin{equation}
\bm{H} = \left[ {\begin{array}{*{20}{c}}
1&{ - 1}&0&0\\
0&1&{ - 1}&0\\
0&0&1&{ - 1}\\
\hline
0&1&0&0\\
0&0&1&0\\
0&0&0&1
\end{array}} \right],
\end{equation}
And $\boldsymbol{\Sigma}=\mathrm{diag}(Q_1, Q_2, Q_3, R_1, R_2, R_3)$. The whole question can be written as:
\begin{equation}
\bm{x}^*_{\mathrm{map}} = \arg \min \bm{e}^\mathrm{T} \boldsymbol{\Sigma}^{-1} \bm{e},
\end{equation}
Later we will see that this problem has a unique solution:
\begin{equation}
\bm{x}^*_{\mathrm{map}} = (\bm{H}^\mathrm{T} \boldsymbol{\Sigma}^{-1} \bm{H})^{-1} \bm{H}^\mathrm{T} \boldsymbol{\Sigma}^{-1} \bm{y}.
\end{equation}

\section{nonlinear least squares}
\label{sec:6.2}
Let's consider a simple least squares problem first:
\begin{equation}
\mathop {\min }\limits_{\bm{x}} F(\bm{x}) = \frac{1}{2}{\left\| {f\left( \bm{x} \right) } \right\|^2_2}.
\end{equation}
Where the argument $\bm{x} \in \mathbb{R}^n$, $f$ is an arbitrary scalar nonlinear function $f(\bm{x}): \mathbb{R}^n \mapsto \ Mathbb{R}$. Note that the coefficient $\frac{1}{2}$ is irrelevant. Some documents have this coefficient, and some documents do not. It does not affect the subsequent conclusions. Let's discuss how to solve such an optimization problem. Obviously, if $f$ is a mathematically simple function, then the problem can be solved in analytical form. Let the derivative of the objective function be zero, and then solve the optimal value of $\bm{x}$, just like the extreme value of the binary function:
\begin{equation}
\frac{ \mathrm{d} F}{ \mathrm{d} \bm{x} } = \bm{0}.
\end{equation}
Solving this equation yields an extremum with a derivative of zero. They may be maximal, very small, or values ​​at the saddle point, as long as they compare the size of their function values ​​one by one. But is this equation easy to solve? This depends on the form of the $f$ derivative. If $f$ is a simple linear function, then the problem is a simple linear least squares problem, but some derivatives may be complex in form, making the equation not easy to solve. Solving this equation requires us to know the \textbf{global nature} of the objective function, which is usually not possible. For the least squares problem that is inconvenient to solve directly, we can use the \textbf{iteration} method to continuously update the current optimization variable from an initial value to make the objective function drop. The specific steps can be listed as follows:

\begin{mdframed}
\begin{enumerate}
\item gives an initial value of $\bm{x}_0$.
\item For the $k$ iteration, look for an increment of $\Delta \bm{x}_k$, making $\left\| {f\left( \bm{x}_k + \Delta \bm{x} _k \right)} \right \|^2_2$ reaches a minimum value.
\item Stop if $\Delta \bm{x}_k$ is small enough.
\item Otherwise, let $\bm{x}_{k+1} = \bm{x}_k+\Delta \bm{x}_k$ return to step 2.
\end{enumerate}
\end{mdframed}
This turns the problem of solving the \textbf{derivative zero} is turned into a constant \textbf{find drop increment}$\Delta \bm{x}_k$ problem, as we will see, since it can be $f$ Linearization, the calculation of the increment will be much simpler. When the function drops until the increment is very small, the algorithm is considered to converge and the objective function reaches a minimum value. In this process, the problem is how to find the increment of each iteration point, and this is a local problem, we only need to care about the local nature of $f$ at the iteration value rather than the global nature. Such methods are widely used in areas such as optimization and machine learning.

Next we look at how to find this increment $\Delta \bm{x}_k$. This part of the knowledge is actually in the field of numerical optimization, let's look at some widely used results.

\subsection{First-order and two-step methods}
Now consider the $k$ iteration, assuming we are looking for the increment $\Delta \bm{x}_k$ at $\bm{x}_k$, then the most intuitive way is to put the target function at $ Taylor expansion near \bm{x}_k$:
\begin{equation}
F(\bm{x}_k+\Delta \bm{x}_k) \approx F{\left( \bm{x}_k \right)} + \bm{J} \left( \bm{x}_k \right) ^\mathrm{T} \Delta \bm{x}_k + \frac{1}{2}\Delta {\bm{x}_k^\mathrm{T}}\bm{H}(\bm{ x}_k) \Delta \bm{x}_k.
\end{equation}
Where $\bm{J}(\bm{x}_k)$ is $F(\bm{x})$ for the first derivative of $\bm{x}$ (also called gradient, \textbf{Jacobi} Matrix [Jacobian]) \footnote{We write $\bm{J}(\bm{x})$ as a column vector, then it can be inner product with $\Delta \bm{x}$ to get a scalar. }, $\bm{H}$ is the second derivative (\textbf{Hessian} matrix, they all take values ​​at $\bm{x}_k$, the reader should be in the university undergraduate multivariate calculus Learned in the course. We can choose to retain the first or second order of Taylor expansion, then the corresponding solution method is called a step or two step method. If you keep a step, then take the gradient in the inverse direction to ensure that the function drops:
\begin{equation}
\Delta \bm{x}^* = - \bm{J}(\bm{x}_k).
\end{equation}
Of course this is just a direction, usually we have to specify another step $\lambda$. The step size can be calculated according to certain conditions \textsuperscript{\cite{Wolfe1969}}, and there are some empirical methods in machine learning, but we don't talk about it. This method is called \textbf{the steepest descent method}. Its intuitive meaning is very simple, as long as we move in the direction of the reverse gradient, the first-order (linear) approximation, the objective function must fall.

Note that the above discussion was made at the $k$ iteration and does not involve other iteration information. So in order to simplify the symbol, we will omit the subscript $k$ later, and think that these discussions are true for any iteration.

On the other hand, we can choose to retain the two-step information, where the incremental equation is:
\begin{equation}
\Delta \bm{x}^* = \arg \min \left(F\left( \bm{x} \right) + \bm{J} \left( \bm{x} \right)^\mathrm{ T} \Delta \bm{x} + \frac{1}{2}\Delta {\bm{x}^\mathrm{T}}\bm{H} \Delta \bm{x} \right).
\end{equation}
Only zero, one, and quadratic items of $\Delta \bm{x}$ are included on the right side. Find the derivative of the right-hand equation about $\Delta \bm{x}$ and make it zero. \footnote{For those who are unfamiliar with matrix derivation, please refer to Appendix B. },get:
\begin{equation}
\bm{J} + \bm{H} \Delta \bm{x} = \bm{0} \Rightarrow
\bm{H} \Delta \bm{x} = -\bm{J}.
\end{equation}
Solving this linear equation yields an increment. This method is also known as \textbf{Newton method}.

We see that both the first-order and two-step methods are straightforward, as long as the function is Taylor-expanded around the iteration point and minimized for updates. In fact, we approximate the original function with a one- or two-time function, and then use the minimum value of the approximation function to guess the minimum value of the original function. As long as the original objective function locally looks like a first or quadratic function, such an algorithm is true (this is also the case in reality). However, these two methods also have their own problems. The steepest descent method is too greedy, easy to get out of the jagged route, but increases the number of iterations. Newton's law needs to calculate the $\bm{H}$ matrix of the objective function, which is very difficult when the problem size is large. We usually tend to avoid the calculation of $\bm{H}$. For the general problem, some quasi-Newton methods can get better results, and for the least squares problem, there are several more practical methods: \textbf{Gauss-Newton's method} and \textbf{column Levernburg-Marquardt's method}.

\subsection{Gaussian Newton method}
The Gauss-Newton method is one of the easiest methods in the optimization algorithm. Its idea is to carry out the first-order Taylor expansion of $f(\bm{x})$. Note that this is not the target function $F(\bm{x})$ but $f(\bm{x})$, otherwise it becomes Newton.
\begin{equation}
\label{eq:approximation}
f\left( {\bm{x} + \Delta \bm{x}} \right) \approx f\left( \bm{x} \right) + \bm{J} \left( \bm{x} \right)^\mathrm{T} \Delta \bm{x}.
\end{equation}
Here $\bm{J}(\bm{x})^\mathrm{T}$ is $f(\bm{x})$ for the derivative of $\bm{x}$, $n \times 1$ Column vector. According to the previous framework, the current goal is to find the increment $\Delta \bm{x}$, so that $\left\| {f\left( \bm{x} + \Delta \bm{x} \right)} \right \|^2$ reaches the minimum. In order to find $\Delta \bm{x}$, we need to solve a linear least squares problem:
\begin{equation}
\Delta \bm{x}^* = \arg \mathop {\min }\limits_{\Delta \bm{x}} \frac{1}{2}{\left\| {f\left( \bm{ x} \right) + \bm{J} \left( \bm{x} \right)^\mathrm{T} \Delta \bm{x} } \right\|^2}.
\end{equation}

What is the difference between this equation and the previous one? Based on the extreme conditions, the above objective function is derived for $\Delta \bm{x}$ and the derivative is zero. To do this, first expand the squared term of the objective function:
\begin{align*}
\frac{1}{2}{\left\| {f\left( \bm{x} \right) + \bm{J} \left( \bm{x} \right)^\mathrm{T} \ Delta \bm{x}} \right\|^2} &= \frac{1}{2}{\left( {f\left( \bm{x} \right) + \bm{J}\left( \bm{x} \right)^\mathrm{T} \Delta \bm{x}} \right)^\mathrm{T}}\left( {f\left( \bm{x} \right) + \ Bm{J} \left( \bm{x} \right)^\mathrm{T} \Delta \bm{x}} \right)\\
&= \frac{1}{2}\left( \| f{{\left( \bm{x} \right)}\|^2_2 + 2 f\left( \bm{x} \right) \bm {J} {{\left( \bm{x} \right)}}^\mathrm{T} \Delta \bm{x} + \Delta { \bm{x}^\mathrm{T}}{\bm {J} (\bm{x})} \bm{J}(\bm{x})^\mathrm{T} \Delta \bm{x}} \right).
\end{align*}

Find the derivative of $\Delta \bm{x}$ and make it zero:
\begin{displaymath}
\bm{J} {\left( \bm{x} \right)}f\left( \bm{x} \right) + \bm{J} {\left( \bm{x} \right)} \ Bm{J}^\mathrm{T} \left( \bm{x} \right)\Delta \bm{x} = \bm{0}.
\end{displaymath}

The following equations can be obtained:
\begin{equation}
\underbrace{\bm{J} {\left( \bm{x} \right)} \bm{J}^\mathrm{T}}_{\bm{H}(\bm{x})} \left ( \bm{x} \right)\Delta \bm{x} = \underbrace{- \bm{J} {\left( \bm{x} \right)} f\left( \bm{x} \right )}_{\bm{g}(\bm{x})}.
\end{equation}

This equation is about the \textbf{linear equations} of the variable $\Delta \bm{x}$, which we call \textbf{incremental equation}, also known as \textbf{Gauss Newton's equation} (Gauss-Newton) Equation) or \textbf{Normal equation}. We define the coefficient on the left as $\bm{H}$ and the right as $\bm{g}$, then the above formula becomes:
\begin{equation}
\label{eq:minimize-deltax}
\bm{H} \Delta \bm{x} = \bm{g}.
\end{equation}
It makes sense to write the left side as $\bm{H}$. Compared with the Newton method, the Gauss-Newton method uses $\bm{J}\bm{J}^\mathrm{T}$ \textbf{ as the approximation of the second-order Hessian matrix in Newton's method}, thus omitting the calculation of $\bm{ The process of H}$. \textbf{Solving the incremental equation is at the heart of the overall optimization problem}. If we can solve the equation smoothly, the algorithmic steps of the Gauss-Newton method can be written as:

\begin{mdframed}
\begin{enumerate}
\item gives the initial value $\bm{x}_0$.
\item For the $k$ iteration, find the current Jacobian matrix $\bm{J}(\bm{x}_k)$ and the error $f(\bm{x}_k)$.
\item solves the incremental equation: $\bm{H} \Delta \bm{x}_k = \bm{g}$.
\item Stop if $\Delta \bm{x}_k$ is small enough. Otherwise, let $\bm{x}_{k+1} = \bm{x}_k+\Delta \bm{x}_k$ return to step 2.
\end{enumerate}
\end{mdframed}

It can be seen from the algorithm steps that the solution of the incremental equation occupies a dominant position. As long as we can solve the increment smoothly, we can ensure that the objective function can drop correctly.

To solve the incremental equation, we need to solve for $\bm{H}^{-1}$, which requires the $\bm{H}$ matrix to be reversible, but the $\bm{J} \bm{ calculated in the actual data. J}^\mathrm{T}$ is only semi-positive. That is to say, when using the Gauss-Newton method, it may appear that $\bm{J}\bm{J}^\mathrm{T}$ is a singular matrix or an ill-condition, and the increment is stable. Poor sex, resulting in the algorithm does not converge. Intuitively, the local approximation of the original function at this point is not like a quadratic function. More seriously, even if we assume that $\bm{H}$ is not singular, it is not morbid. If we find the step size $\Delta \bm{x}$ is too large, it will lead to the local approximation we use. Eqref{eq:approximation} is not accurate enough, so we can't even guarantee its iterative convergence, even if it makes the objective function bigger.

Although the Gauss-Newton method has these shortcomings, it is still a simple and effective method for nonlinear optimization, which is worth learning. In the field of nonlinear optimization, quite a few algorithms can be reduced to variants of the Gauss-Newton method. These algorithms all rely on the idea of ​​the Gauss-Newton method and correct its shortcomings through its own improvements. For example, some \textbf{line search method} added a step size of $\alpha$, and after finding $\Delta \bm{x}$, further find $\alpha$ to make $\left\| f (\bm{x} + \alpha \Delta \bm{ x}) \right\|^2$ reaches the minimum, instead of simply making $\alpha = 1$.

The Levenberg-Marquart method corrects these problems to some extent. It is generally considered to be more robust than the Gauss-Newton method, but its convergence rate may be slower than the Gauss-Newton method, known as the \textbf{damped Newton Method}.

\subsection{Levinburg-Marquart method}
The approximate second-order Taylor expansion used in the Gauss-Newton method can only have a good approximation near the expansion point, so we naturally think that we should add a range to $\Delta \bm{x}$, called \textbf{ Trust Region}. This range defines the circumstances under which a second-order approximation is valid. This type of method is also known as \textbf{Trust Region Method}. In the area of ​​trust, we think that the approximation is valid; if this area is out, the approximation may be problematic.

So how do you determine the scope of this trust zone? A better method is based on the difference between our approximation model and the actual function: if the difference is small, the approximation effect is good, we expand the approximation range; conversely, if the difference is large, the approximation range is narrowed. We define an indicator $\rho$ to describe how good or bad the approximation is:
\begin{equation}\label{eq:6.24}
\rho = \frac{{f\left( {\bm{x} + \Delta \bm{x}} \right)} - ​​{{ {f\left( \bm{x} \right)} }}} { \bm{J}\left( \bm{x} \right)^\mathrm{T} \Delta \bm{x} } .
\end{equation}
The numerator of $\rho$ is the value of the actual function drop, and the denominator is the value of the approximate model drop. If $\rho$ is close to 1, the approximation is good. If $\rho$ is too small, indicating that the actual reduced value is much less than the approximate reduced value, then the approximation is considered to be poor and the approximate range needs to be reduced. Conversely, if $\rho$ is large, the actual decline is larger than expected, and we can zoom in on the approximate range.

So, we build a modified version of the nonlinear optimization framework, which will have a better effect than the Gauss Newton method:

\begin{mdframed}
\begin{enumerate}
\item gives the initial value $\bm{x}_0$ and the initial optimization radius $\mu$.
\item For the $k$ iteration, add a confidence region based on the Gauss-Newton method to solve:
\begin{equation}\label{eq:LM}
\mathop {\min }\limits_{\Delta \bm{x}_k} \frac{1}{2}{\left\| {f\left( \bm{x}_k \right) + \bm{J } \left( \bm{x}_k \right)^\mathrm{T} \Delta \bm{x}_k} \right\|^2}, \quad \mathrm{st}\quad {\left\| {\bm{D} \Delta \bm{x}_k} \right\|^2} \leqslant \mu ,
\end{equation}
Where $\mu$ is the radius of the confidence zone and $\bm{D}$ is the coefficient matrix, which will be explained later.
\item calculates $\rho$ by the formula \eqref{eq:6.24}.
\item If $\rho > \frac{3}{4}$, set $\mu = 2 \mu$.
\item If $\rho < \frac{1}{4}$, set $\mu = 0.5 \mu$.
\item If $\rho$ is greater than a certain threshold, it is considered to be approximate. Let $\bm{x}_{k+1} = \bm{x}_k+\Delta \bm{x}_k$.
\item determines if the algorithm converges. If it does not converge, return to step 2, otherwise it ends.
\end{enumerate}
 \end{mdframed}

Here, the multiples and thresholds of the approximate range expansion are empirical values ​​and can be replaced with other values. In the formula \eqref{eq:LM}, we limit the increment to a ball with a radius of $\mu$, which is considered valid only in this ball. After taking $\bm{D}$, the ball can be seen as an ellipsoid. In Levinberg's optimization method, taking $\bm{D}$ as a unit matrix $\bm{I}$ is equivalent to directly constraining $\Delta \bm{x}_k$ in a ball. . Subsequently, Marquartt proposed to take $\bm{D}$ as a non-negative diagonal matrix—in practice, the square root of the diagonal element of $\bm{J}^T \bm{J}$ is usually used, so that The constraint range is larger in the small gradient.

In any case, in the Levenberg-Marquart optimization, we need to solve the sub-question \eqref{eq:LM} to get the gradient. This subproblem is an optimization problem with inequality constraints. We use the Lagrangian multiplier to put the constraint into the objective function to form a Lagrangian function:
\begin{equation}
\mathcal{L}(\Delta \bm{x}_k, \lambda)= \frac{1}{2} {\left\| {f\left( \bm{x}_k \right) + \bm{J} \left( \bm{x}_k \right)^\mathrm{T} \Delta \bm{x}_k} \right\|^2} + \frac{\lambda}{2} \left( \left\| \bm{D} \Delta \bm{x}_k \right\|^2 - \mu \right).
\end{equation}
Here $\lambda$ is the Lagrange multiplier. Similar to the Gaussian Newton method, the Lagrangian function has zero derivative for $\Delta \bm{x}$, and its core is still a linear equation for calculating increments:
\begin{equation}
\left( \bm{H} +\lambda \bm{D}^\mathrm{T} \bm{D} \right) \Delta \bm{x}_k = \bm{g}.
\end{equation}

As you can see, the incremental equation has one more $\lambda \bm{D}^T \bm{D}$ compared to the Gauss-Newton method. If you consider its simplified form, ie $\bm{D}=\bm{I}$, then the equivalent of solving \footnote{strict readers may not be satisfied with the narrative here. Constraints on the original problem of the trust region In addition to the Lagrangian function deriving zero, the KKT condition has some other constraints: $\lambda>0$, and $\lambda(\|\bm{D} \Delta \bm{x}\|^2-\mu)=0$. But in the L-M iteration, we might as well consider it as a penalty with $\lambda$ as the weighting function on the objective function of the original problem. After each iteration, if the trust region condition is found to be unsatisfied, or the objective function increases, the weight of $\lambda$ is increased until the trust region condition is finally met. Therefore, there are different interpretations of the L-M algorithm in theory, but in practice we only care about whether it works smoothly. }:
\begin{displaymath}
\left( \bm{H} +\lambda \bm{I} \right) \Delta \bm{x}_k = \bm{g}.
\end{displaymath}

We see that when the parameter $\lambda$ is small, $\bm{H}$ is dominant, which means that the quadratic approximation model is better in this range, Levinburg-Marquartt method is more Close to the Gauss Newton method. On the other hand, when $\lambda$ is large, $\lambda \bm{I}$ dominates, and the Levenberg-Marquart method is closer to a step-down method (ie, the steepest drop). This shows that the secondary approximation in the vicinity is not good enough. The Levinberg-Marquart method can solve the non-singular and ill-conditioned problems of the coefficient matrix of linear equations to a certain extent, providing a more stable and accurate increment $\Delta \bm{x} $.

In practice, there are many other ways to solve for increments, such as Dog-Leg\cite{Nocedal2006}. What we've introduced here is just the most common and basic method, and the most used method in visual SLAM. In practical problems, we usually choose one of the Gauss Newton method or the Levinburg-Marquart method as the gradient descent strategy. When the problem is of a good nature, use Gauss Newton. If the problem is close to morbidity, use the Levenberg-Marquart method.

\subsection{小结}
Since I don't want this book to become a mathematics textbook that is a headache, here are just two of the most common nonlinear optimization schemes, the Gauss-Newton method and the Levinberg-Marquart method. We have avoided many discussions on the nature of mathematics. If you are interested in optimization, you can read a book that specializes in numerical optimization (this is a big topic)\cite{Nocedal2006}. The optimization methods represented by the Gauss Newton method and the Levenberg-Marquart method have been implemented and provided to users in many open source optimization libraries. We will conduct experiments below. Optimization is a basic mathematical tool for dealing with many practical problems. It plays a central role not only in visual SLAM, but also in other fields like deep learning. It is also one of the core methods for solving problems. The order method is mainly). We hope that readers will be able to understand more optimization algorithms based on their capabilities.

Perhaps you have discovered that both the Gauss Newton method and the Levinburg-Marquardt method need to provide the initial value of the variable when doing the optimization calculation. You may ask, can this initial value be set at will? of course not. In fact, all iterative solution solutions for nonlinear optimization require the user to provide a good initial value. Since the objective function is too complex, the change in the solution space is difficult to predict, and providing different initial values ​​for the problem often leads to different calculation results. This situation is a common problem of nonlinear optimization: most algorithms are prone to fall into local minima. Therefore, no matter what kind of scientific problem, we should provide scientific basis for the initial value. For example, in the visual SLAM problem, we will use the algorithms such as ICP and PnP to provide optimized initial values. In short, a good initial value is very important for the optimization problem!

Perhaps the reader will also have questions about the optimization mentioned above: How to solve linear incremental equations? We only talked about the incremental equation is a linear equation, but directly inverting the coefficient matrix is ​​not a lot of calculations? of course not. In the visual SLAM algorithm, the dimension of $\Delta \bm{x}$ is often as large as several hundred or thousands. If you are doing large-scale visual 3D reconstruction, you will often find that this dimension can be easily reached. Hundreds of thousands or even higher. Inverting such a large matrix is ​​unaffordable for most processors, so there are many numerical solutions for linear equations. There are different solutions in different fields, but there is almost no way to directly find the inverse of the coefficient matrix. We will use matrix decomposition to solve linear equations, such as QR, Cholesky and other decomposition methods. These methods are usually found in textbooks such as matrix theory, and we will not introduce them.

Fortunately, this matrix in the visual SLAM tends to have a specific sparse form, which provides the possibility to solve the optimization problem in real time. We will explain its principles in detail in Lecture 9. Using the sparse form of the elimination, decomposition, and finally the solution increment, will greatly improve the efficiency of the solution. In many open source optimization libraries, variables with more than 10,000 dimensions can be solved in a few seconds or less on a typical PC, and the reason is to use more advanced mathematical tools. The visual SLAM algorithm can now be implemented in real time, and thanks to the fact that the coefficient matrix is ​​sparse. If the matrix is ​​dense, I am afraid that optimizing such a visual SLAM algorithm will not be widely adopted by the academic community \textsuperscript{\cite{Lourakis2009, Sibley2009a, Triggs2000 }}.

\section{Practice: curve fitting problem}
\subsection{Handwritten Gauss Newton Method}
Next we use a simple example to illustrate how to solve the least squares problem. We will demonstrate how to handwrite the Gauss-Newton method and then how to use the optimization library to solve this problem. For the same problem, these implementations will achieve the same result because their core algorithms are the same.

Consider a curve that satisfies the following equation:
\[
y = \exp( ax^2 + bx + c ) + w,
\]
Where $a,b,c$ are the parameters of the curve, and $w$ is Gaussian noise, satisfying $w \sim (0, \sigma^2)$. We deliberately chose such a nonlinear model so that the problem is not too simple. Now, suppose we have $N$ observation data points for $x,y$, and we want to find the parameters of the curve based on these data points. Then, the following least squares problem can be solved to estimate the curve parameters:
\begin{equation}
\min \limits_{a,b,c} \frac{1}{2}\sum\limits_{i = 1}^N {{{\left\| {{y_i} - \exp \left( {ax_i^ 2 + bx_i + c} \right)} \right\|}^2}} .
\end{equation}

Note that in this question, the variables to be estimated are $a,b,c$ instead of $x$. In our program, we first generate the true value of $x, y$ according to the model, and then add the Gaussian distribution noise to the true value. Subsequently, the Gauss-Newton method is used to fit the parametric model from the noisy data. The definition error is:
\begin{equation}
E_i = y_i - \exp \left( {ax_i^2 + bx_i + c} \right),
\end{equation}
Then we can find the derivative of each error term for the state variable:
\begin{equation}
\begin{aligned}
\frac{{\partial {e_i}}}{{\partial a}} &= - x_i^2\exp \left( {ax_i^2 + b{x_i} + c} \right)\\
\frac{{\partial e_i}}{{\partial b}} &= - {x_i}\exp \left( {ax_i^2 + b{x_i} + c} \right)\\
\frac{{\partial {e_i}}}{{\partial c}} &= - \exp \left( {ax_i^2 + b{x_i} + c} \right)
\end{aligned}
\end{equation}
Then $\bm{J}_i = \left[\frac{{\partial {e_i}}}{{\partial a}},\frac{{\partial {e_i}}}{{\partial b}}, \frac{{\partial {e_i}}}{{\partial c}} \right]^\mathrm{T}$, the incremental equation for the Gauss-Newton method is:
\begin{equation}
\left(\sum\limits_{i = 1}^{100} {\bm{J}_i{(\sigma^2)^{ - 1}}{\bm{J}_i}}^\mathrm{T } \right) \Delta \bm{x}_k = \sum\limits_{i = 1}^{100} { - {\bm{J}_i}{(\sigma^2)^{ - 1}}{ E_i}},
\end{equation}
Of course, we can also choose to put all the $\bm{J}_i$ in a column and write the equation as a matrix, but its meaning is consistent with the summation form. The code below demonstrates how this process works.
\begin{lstlisting}[language=sh,caption=slambook2/ch6/gaussNewton.cpp]
#include <iostream>
#include <opencv2/opencv.hpp>
#include <Eigen/Core>
#include <Eigen/Dense>

using namespace std;
using namespace Eigen;

int main(int argc, char **argv) {
    double ar = 1.0, br = 2.0, cr = 1.0;         // 真实参数值
    double ae = 2.0, be = -1.0, ce = 5.0;        // 估计参数值
    int N = 100;                                 // 数据点
    double w_sigma = 1.0;                        // 噪声Sigma值
    double inv_sigma = 1.0 / w_sigma;
    cv::RNG rng;                                 // OpenCV随机数产生器
    
    vector<double> x_data, y_data;      // 数据
    for (int i = 0; i < N; i++) {
        double x = i / 100.0;
        x_data.push_back(x);
        y_data.push_back(exp(ar * x * x + br * x + cr) + rng.gaussian(w_sigma * w_sigma));
    }
    
    // 开始Gauss-Newton迭代
    int iterations = 100;    // 迭代次数
    double cost = 0, lastCost = 0;  // 本次迭代的cost和上一次迭代的cost
    
    chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
    for (int iter = 0; iter < iterations; iter++) {
        
        Matrix3d H = Matrix3d::Zero();             // Hessian = J^T W^{-1} J in Gauss-Newton
        Vector3d b = Vector3d::Zero();             // bias
        cost = 0;
        
        for (int i = 0; i < N; i++) {
            double xi = x_data[i], yi = y_data[i];  // 第i个数据点
            double error = yi - exp(ae * xi * xi + be * xi + ce);
            Vector3d J; // 雅可比矩阵
            J[0] = -xi * xi * exp(ae * xi * xi + be * xi + ce);  // de/da
            J[1] = -xi * exp(ae * xi * xi + be * xi + ce);  // de/db
            J[2] = -exp(ae * xi * xi + be * xi + ce);  // de/dc
            
            H += inv_sigma * inv_sigma * J * J.transpose();
            b += -inv_sigma * inv_sigma * error * J;
            
            cost += error * error;
        }
        
        // 求解线性方程 Hx=b
        Vector3d dx = H.ldlt().solve(b);
        if (isnan(dx[0])) {
            cout << "result is nan!" << endl;
            break;
        }
        
        if (iter > 0 && cost >= lastCost) {
            cout << "cost: " << cost << ">= last cost: " << lastCost << ", break." << endl;
            break;
        }
        
        ae += dx[0];
        be += dx[1];
        ce += dx[2];
        
        lastCost = cost;
        
        cout << "total cost: " << cost << ", \t\tupdate: " << dx.transpose() <<
          "\t\testimated params: " << ae << "," << be << "," << ce << endl;
    }
    
    chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
    chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
    cout << "solve time cost = " << time_used.count() << " seconds. " << endl;
    cout << "estimated abc = " << ae << ", " << be << ", " << ce << endl;
    return 0;
}
\end{lstlisting}

In this example, we demonstrate how to iteratively optimize a simple fitting problem. It's easy to see the entire optimization process with your own handwritten code. The program outputs the target function value and update amount for each iteration of the iteration, as follows:
\begin{lstlisting}[language=sh,caption=Terminal output:]
/home/xiang/Code/slambook2/ch6/cmake-build-debug/gaussNewton
Total cost: 3.19575e+06, update: 0.0455771 0.078164 -0.985329 estimated params: 2.04558,-0.921836,4.01467
Total cost: 376785, update: 0.065762 0.224972 -0.962521 estimated params: 2.11134,-0.696864,3.05215
Total cost: 35673.6, update: -0.0670241 0.617616 -0.907497 estimated params: 2.04432,-0.0792484, 2.14465
Total cost: 2195.01, update: -0.522767 1.19192 -0.756452 estimated params: 1.52155, 1.11267, 1.3882
Total cost: 174.853, update: -0.537502 0.909933 -0.386395 estimated params: 0.984045,2.0226,1.00181
Total cost: 102.78, update: -0.0919666 0.147331 -0.0573675 estimated params: 0.892079,2.16994,0.944438
Total cost: 101.937, update: -0.00117081 0.00196749 -0.00081055 estimated params: 0.890908,2.1719,0.943628
Total cost: 101.937, update: 3.4312e-06 -4.28555e-06 1.08348e-06 estimated params: 0.890912,2.1719,0.943629
Total cost: 101.937, update: -2.01204e-08 2.68928e-08 -7.86602e-09 estimated params: 0.890912,2.1719,0.943629
Cost: 101.937>= last cost: 101.937, break.
Solve time cost = 0.000212903 seconds.
Estimated abc = 0.890912, 2.1719, 0.943629
\end{lstlisting}
The objective function that is easy to see the whole problem approaches convergence after 9 iterations, and the update amount approaches zero. The final estimated value is close to the true value, as shown in \autoref{fig:ceres-fitting}. On my machine (my CPU is i7-8700), the optimization takes about 0.2 milliseconds. Below we try to use the optimization library to accomplish the same task.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{optimization/ceresFitting.pdf}
    \caption{The result of curve fitting when noise $\sigma=1$. The real model is very close to the estimated model. }
    \label{fig:ceres-fitting}
\end{figure}

\subsection{Curve fitting with Ceres}
This section introduces two C++ optimization libraries: the Keres library from Google, \textsuperscript{\cite{Ceres}}, and the graph-optimized g2o library \textsuperscript{\cite{Kummerle2011}}. Since the use of g2o also needs to introduce some knowledge about graph optimization, let us introduce Ceres first, then introduce some graph optimization theory, and finally talk about g2o. Since the optimization algorithm will appear in the following "visual odometer" and "backend", the reader must grasp the meaning of the optimization algorithm and understand the content of the program.

\subsubsection{Ceres introduction}
Google Ceres is a widely used solution library for least squares problems. In Ceres, as a user, we only need to define the optimization problem to be solved according to certain steps, and then give it to the solver for calculation. The most general form of the least squares problem solved by Ceres is as follows (least squares of kernel functions with boundaries):
\begin{equation}
\begin{array}{ll}
\min \limits_x \quad & \frac{1}{2}\sum\limits_i {{\rho _i}\left( {{{\left\| {{f_i}\left( {{x_{{i_1}} }, \cdots {x_{{i_n}}}} \right)} \right\|}^2}} \right)} \\
\mathrm{s.t.} \quad & {l_j} \leqslant {x_j} \leqslant {u_j}.
\end{array}
\end{equation}

In this problem, $x_1, \cdots, x_n$ are optimization variables, also known as \textbf{Parameter blocks}, and $f_i$ is called \textbf{Cost function}, also known as disability. Residual blocks can also be understood as error terms in SLAM. $l_j$ and $u_j$ are the upper and lower limits of the $j$ optimization variables. In the simplest case, take $l_j = -\infty, u_j=\infty$ (without limiting the boundaries of the optimization variables). At this point, the objective function consists of a number of squared terms after a \textbf{nuclear function}$\rho(\cdot)$ and then summed up to form a \footnote{nuclear function. See ninth lecture for a detailed discussion. }. Similarly, $\rho$ can be taken as an identity function, then the objective function is the sum of the squares of many terms, and we get the unconstrained least squares problem, which is consistent with the previously introduced theory.

In order for Ceres to solve this problem for us, we need to do the following:
\begin{enumerate}
\item defines each parameter block. Parameter blocks are usually trivial vectors, but in SLAM they can also be defined as special structures such as quaternions and Lie algebras. If it is a vector, then we need to allocate a double array for each parameter block to store the value of the variable.
\item Then, define how the residual block is calculated. The residual block is usually associated with several parameter blocks, some custom calculations are performed on them, and the residual values ​​are returned. After Ceres squares them, it is the value of the objective function.
The \item residual block also often needs to define the way Jacobi is calculated. In Ceres, you can use the "auto-derivation" feature provided by it, or you can manually specify the calculation process of Jacobi. If automatic derivation is to be used, the residual block needs to be written in a specific way: the calculation of the residual should be a parenthetical operator with a template. This is illustrated by an example.
\item Finally, add all the parameter blocks and residual blocks to the Problem object defined by Ceres, and call Solve function to solve. Before solving, we can pass in some configuration information, such as the number of iterations, termination conditions, etc., or use the default configuration.
\end{enumerate}
Below, let's take a look at Ceres to solve the curve fitting problem and understand the optimization process.

\subsubsection{Install Ceres}
In order to use Ceres, we need to compile and install it. Ceres' github address is: \url{https://github.com/ceres-solver/ceres-solver}, but you can also use the Ceres in the 3rdparty directory of this book code directly, so that you will use exactly the same as me. version.

Like the library I encountered before, Ceres is a cmake project. First install its dependencies, you can use apt-get to install in Ubuntu, mainly some of the logs and test tools used by Google itself:
\begin{lstlisting}[language=sh,caption=terminal input:]
Sudo apt-get install liblapack-dev libsuitesparse-dev libcxsparse3 libgflags-dev libgoogle-glog-dev libgtest-dev
\end{lstlisting}

Then, go to the Ceres library directory, compile and install it using cmake. We have done this process many times, and we won't go into details here. After the installation is complete, find the Ceres header file under /usr/local/include/ceres and find the library file named libceres.a under /usr/local/lib/. With these files, you can use Ceres for optimization calculations.
\subsubsection{Use Ceres to fit curves}
The following code demonstrates how to solve the same problem using Ceres.

\begin{lstlisting}[language=c++,caption=slambook/ch6/ceresCurveFitting.cpp]
#include <iostream>
#include <opencv2/core/core.hpp>
#include <ceres/ceres.h>
#include <chrono>

Using namespace std;

// The calculation model of the cost function
Struct CURVE_FITTING_COST {
    CURVE_FITTING_COST(double x, double y) : _x(x), _y(y) {}
    
    // Calculation of residuals
    Template<typename T>
    Bool operator()(
        Const T *const abc, // model parameter, 3D
        T *residual) const {
        // y-exp(ax^2+bx+c)
        Residual[0] = T(_y) - ceres::exp(abc[0] * T(_x) * T(_x) + abc[1] * T(_x) + abc[2]);
        Return true;
    }
    
    Const double _x, _y; // x,y data
};

Int main(int argc, char **argv) {
    Double ar = 1.0, br = 2.0, cr = 1.0; // true parameter value
    Double ae = 2.0, be = -1.0, ce = 5.0; // estimate parameter value
    Int N = 100; // data points
    Double w_sigma = 1.0; // noise sigma value
    Double inv_sigma = 1.0 / w_sigma;
    Cv::RNG rng; // OpenCV random number generator
    
    Vector<double> x_data, y_data; // data
    For (int i = 0; i < N; i++) {
        Double x = i / 100.0;
        X_data.push_back(x);
        Y_data.push_back(exp(ar * x * x + br * x + cr) + rng.gaussian(w_sigma * w_sigma));
    }
    
    double abc[3] = {ae, be, ce};
    
    // Build the least squares problem
    Ceres::Problem problem;
    for (int i = 0; i < N; i++) {
        problem.AddResidualBlock( // Add an error term to the question
            // Use automatic derivation, template parameters: error type, output dimension, input dimension, dimension must be consistent with the previous struct
            New ceres::AutoDiffCostFunction<CURVE_FITTING_COST, 1, 3>(
                New CURVE_FITTING_COST(x_data[i], y_data[i])
            ),
            Nullpt, // kernel function, not used here, empty
            Abc // parameter to be estimated
        );
    }
    
    // Configure the solver
    Ceres::Solver::Options options; // There are a lot of configuration items to fill in
    Options.linear_solver_type = ceres::DENSE_NORMAL_CHOLESKY; // How to solve the incremental equation
    Options.minimizer_progress_to_stdout = true; // output to cout
    
    Ceres::Solver::Summary summary; // Optimization information
    Chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
    Ceres::Solve(options, &problem, &summary); // Start optimizing
    Chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
    Chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
    Cout << "solve time cost = " << time_used.count() << " seconds. " << endl;
    
    // output result
    Cout << summary.BriefReport() << endl;
    Cout << "estimated a,b,c = ";
    For (auto a:abc) cout << a << " ";
    Cout << endl;
    
    Return 0;
}
\end{lstlisting}

The places in the program that need to be explained are commented. As you can see, we used OpenCV's noise generator to generate 100 Gaussian noise data, which was then fitted using Ceres. The Ceres usage demonstrated here is as follows:

\begin{enumerate}
\item Defines the class of the residual block. The method is to write a class (or structure) and define the () operator with the template parameter in the class, so that the class becomes a \textbf{fiction} (Functor)\footnote{C++ terminology The class of the bracket operator is like a function when using the bracket operator. }. This way of defining makes Ceres call the a<double>() method on an object of that class (such as a) just like calling a function. In fact, Ceres will pass the Jacobian matrix as a type parameter to this function, thus implementing the automatic derivation function.
The double abc[3] in the \item program is the parameter block, and for the residual block, we construct the CURVE\_FITTING\_COST object for each data, and then call AddResidualBlock to add the error term to the target function. Since optimization requires gradients, we have several choices: (1) using Ceres' Auto Derivation (Auto Diff); (2) using Numeric Diff\footnote{automatic derivation is also done with numerical derivatives, but Because it is a template operation, it runs faster. }; (3) Deriving the analytical derivative form by itself and providing it to Ceres. Because automatic derivation is the most convenient in coding, we use automatic derivation.
\item Autoderivation requires specifying the dimensions of the error term and the optimization variable. The error here is scalar, the dimension is 1; the optimization is three quantities of $a, b, c$, and the dimension is 3. Therefore, the variable dimension is set to 1, 3 in the template parameter of the automatic derivation class AutoDiffCostFunction.
\item After setting the problem, call the Solve function to solve. You can configure (very detailed) optimization options in options. For example, you can choose to use Line Search or Trust Region, number of iterations, step size, and more. Readers can look at the definition of Options to see which optimization methods are available, and of course the default configuration is already available for a wide range of issues.
\end{enumerate}

Finally, let's take a look at the experimental results. Call build/ceresCurveFitting to see the optimization results:
\begin{lstlisting}
Iter cost cost_change |gradient| |step| tr_ratio tr_radius ls_iter iter_time total_time
0 1.597873e+06 0.00e+00 3.52e+06 0.00e+00 0.00e+00 1.00e+04 0 2.10e-05 7.92e-05
1 1.884440e+05 1.41e+06 4.86e+05 9.88e-01 8.82e-01 1.81e+04 1 5.60e-05 1.05e-03
2 1.784821e+04 1.71e+05 6.78e+04 9.89e-01 9.06e-01 3.87e+04 1 2.00e-05 1.09e-03
3 1.099631e+03 1.67e+04 8.58e+03 1.10e+00 9.41e-01 1.16e+05 1 6.70e-05 1.16e-03
4 8.784938e+01 1.01e+03 6.53e+02 1.51e+00 9.67e-01 3.48e+05 1 1.88e-05 1.19e-03
5 5.141230e+01 3.64e+01 2.72e+01 1.13e+00 9.90e-01 1.05e+06 1 1.81e-05 1.22e-03
6 5.096862e+01 4.44e-01 4.27e-01 1.89e-01 9.98e-01 3.14e+06 1 1.79e-05 1.25e-03
7 5.096851e+01 1.10e-04 9.53e-04 2.84e-03 9.99e-01 9.41e+06 1 1.81e-05 1.28e-03
Solve time cost = 0.00130755 seconds.
Ceres Solver Report: Iterations: 8, Initial cost: 1.597873e+06, Final cost: 5.096851e+01, Termination: CONVERGENCE
Estimated a,b,c = 0.890908 2.1719 0.943628
\end{lstlisting}

The final optimization value is basically the same as the experimental results in our previous section, but Ceres is relatively slower in speed. Ceres used about 1.3 milliseconds on my machine, which is about six times slower than the handwritten Gauss Newton method.

I hope that readers can get a general idea of ​​how to use Ceres through this simple example. It has the advantage of providing an automatic derivation tool that eliminates the need to calculate a troublesome Jacobian matrix. Ceres' automatic derivation is implemented by template elements, which can be done automatically at compile time, but still be a numerical derivative. Most of the time, the book will still introduce the calculation of the Jacobian matrix, because it is more helpful to understand the problem, and there are fewer problems in the optimization. In addition, Ceres' optimization process configuration is also very rich, making it suitable for a wide range of least squares optimization problems, including various issues outside of SLAM.

\subsection{Curve fitting with g2o}
The second practical part of this lecture will introduce another optimized library (mainly in the SLAM field): g2o (General Graphic Optimization, G$^2$O). It is a library based on \textbf{map optimization}. Graph optimization is a theory that combines nonlinear optimization with graph theory, so before we use it, let's take a moment to introduce graph optimization theory.

\subsubsection{Introduction to graph optimization theory}
We have introduced the solution of nonlinear least squares. They are made up of the sum of many error terms. However, there is only one set of optimization variables and many error terms, and we don't know the \textbf{association} between them. For example, how many error terms does an optimization variable $x_j$ exist in? Can we guarantee that the optimization of it makes sense? Further, we hope to be able to visually see the optimization problem \textbf{what it looks like}. Therefore, it involves the optimization of the graph.

Graph optimization is a way to represent optimization problems as \textbf{Graph}. The \textbf{图} here is a graph in the sense of graph theory. A graph consists of several \textbf{Vertex}, and the \textbf{Edge} that connects these vertices. Furthermore, \textbf{vertex} is used to represent \textbf{optimized variable}, and \textbf{edge} is used to denote \textbf{error term}. Thus, for any of the above-mentioned forms of nonlinear least squares problem, we can construct a corresponding \textbf{graph}. We can simply call it \textbf{图}, or use the definition in the probability map, called \textbf{Bayesian} or \textbf{factor map}.

\autoref{fig:graph-optimization}~ is a simple example of graph optimization. We use triangles to represent the camera pose nodes, and circles to represent the landmark points, which form the vertices of the graph optimization; at the same time, the solid line represents the motion model of the camera, and the dashed line represents the observation model, which constitute the edge of the graph optimization. At this point, although the mathematical form of the whole problem is still like \eqref{eq:least-square}, now we can visually see the \textbf{structure} of the problem. If you want, you can also do \textbf{remove the isolated vertex} or \textbf{the priority of optimizing the number of edges} (or the vertices according to the terminology of the graph theory). But the most basic graph optimization is to use graph models to express a nonlinear least squares optimization problem. And we can use the certain properties of the graph model to make better optimizations.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1.0\textwidth]{optimization/graphOptimization.pdf}
    \caption{Figure optimization example. }
    \label{fig:graph-optimization}
    \end{figure}
    
    G2o is a generic graph optimization library. "Universal" means that you can solve any least squares problem that can be represented as a graph optimization in g2o, obviously including the curve fitting problem discussed above. Let us demonstrate this process.
    
    \subsection{g2o compilation and installation}
    Before using a library, we need to compile and install it. Readers should have experienced this process many times, and they are basically the same. For g2o, readers can download it from GitHub: \url{https://github.com/RainerKuemmerle/g2o} or from the third-party codebase provided with this book. Since g2o is still going to be updated, I suggest you use g2o under 3rdparty to ensure that the version is the same as mine.
    
    G2o is also a cmake project. Let's first install its dependencies (some dependencies coincide with Ceres):
    \begin{lstlisting}[language=sh,caption=terminal input:]
    Sudo apt-get install qt5-qmake qt5-default libqglviewer-dev-qt5 libsuitesparse-dev libcxsparse3 libcholmod3
    \end{lstlisting}
    
    Then, compile and install g2o according to cmake. The description of the process is omitted here. After the installation is complete, the g2o header file will be located under /usr/local/g2o and the library file will be located under /usr/local/lib/. Now, let's revisit the curve fitting experiment in the Ceres routine and experiment again in g2o.
    
    \subsection{Use g2o to fit the curve}
    In order to use g2o, we first abstract the curve fitting problem into a graph optimization. In this process, just remember that the \textbf{node is the optimization variable and the edge is the error term}. The graph optimization problem of curve fitting can be drawn as \autoref{fig:graph-fitting}~.
    
    \begin{figure}[!ht]
    \centering
    \includegraphics[width=.9\textwidth]{optimization/graphFitting.pdf}
    \caption{The curve is fitted to the corresponding graph optimization model. (There are some signs like Huawei.)}
    \label{fig:graph-fitting}
    \end{figure}
    
    In the curve fitting problem, the whole problem has only one vertex: the parameters of the curve model are $a, b, c$; and each noisy data point constitutes an error term, that is, the edge of the graph optimization. But the edges here are not the same as the ones we usually think. They are \textbf{Unary Edge}, ie \textbf{connects only one vertex} - because the whole graph has only one vertex. So in \autoref{fig:graph-fitting}~, we can only paint it as if we are connected to ourselves. In fact, one edge of graph optimization can be connected to one, two or more vertices, which mainly reflects how many optimization variables each error is related to. In a slightly mysterious way, we call it \textbf{Hyper Edge}, the whole picture is called \textbf{Hyper Graph}\footnote{ obviously I personally don't like some tricks. I am a naturalist. }.
    
    After figuring out the graph model, the next step is to build the model in g2o for optimization. As a g2o user, what we have to do mainly consists of the following steps:
    \begin{enumerate}
        \item defines the type of vertex and edge.
        \item build diagram.
        \item selects the optimization algorithm.
        \item calls g2o for optimization and returns the result.
        \end{enumerate}
        
        This part is very similar to Ceres, of course, the program will be written differently. Let's demonstrate the program below.

\begin{lstlisting}[language=c++,caption=slambook/ch6/g2oCurveFitting.cpp]
    #include <iostream>
    #include <g2o/core/g2o_core_api.h>
    #include <g2o/core/base_vertex.h>
    #include <g2o/core/base_unary_edge.h>
    #include <g2o/core/block_solver.h>
    #include <g2o/core/optimization_algorithm_levenberg.h>
    #include <g2o/core/optimization_algorithm_gauss_newton.h>
    #include <g2o/core/optimization_algorithm_dogleg.h>
    #include <g2o/solvers/dense/linear_solver_dense.h>
    #include <Eigen/Core>
    #include <opencv2/core/core.hpp>
    #include <cmath>
    #include <chrono>
    
    Using namespace std;
    
    // Vertex of the curve model, template parameters: optimized variable dimensions and data types
    Class CurveFittingVertex : public g2o::BaseVertex<3, Eigen::Vector3d> {
    Public:
        EIGEN_MAKE_ALIGNED_OPERATOR_NEW
        
        // reset
        Virtual void setToOriginImpl() override {
            _estimate << 0, 0, 0;
        }
        
        // update
        Virtual void oplusImpl(const double *update) override {
            _estimate += Eigen::Vector3d(update);
        }
        
        // save and read: leave blank
        Virtual bool read(istream &in) {}
        Virtual bool write(ostream &out) const {}
    };
    
    // Error model Template parameters: observation dimension, type, join vertex type
    Class CurveFittingEdge : public g2o::BaseUnaryEdge<1, double, CurveFittingVertex> {
    Public:
        EIGEN_MAKE_ALIGNED_OPERATOR_NEW
        
        CurveFittingEdge(double x) : BaseUnaryEdge(), _x(x) {}
        
        // Calculate the curve model error
        Virtual void computeError() override {
            Const CurveFittingVertex *v = static_cast<const CurveFittingVertex *> (_vertices[0]);
            Const Eigen::Vector3d abc = v->estimate();
            _error(0, 0) = _measurement - std::exp(abc(0, 0) * _x * _x + abc(1, 0) * _x + abc(2, 0));
        }
        
        // Calculate the Jacobian matrix
        Virtual void linearizeOplus() override {
            Const CurveFittingVertex *v = static_cast<const CurveFittingVertex *> (_vertices[0]);
            Const Eigen::Vector3d abc = v->estimate();
            Double y = exp(abc[0] * _x * _x + abc[1] * _x + abc[2]);
            _jacobianOplusXi[0] = -_x * _x * y;
            _jacobianOplusXi[1] = -_x * y;
            _jacobianOplusXi[2] = -y;
        }
        
        Virtual bool read(istream &in) {}
        Virtual bool write(ostream &out) const {}
    Public:
        Double _x; // x value, y value is _measurement
    };
    
    Int main(int argc, char **argv) {
        / / Omit the data generation part of the code
        // Build graph optimization, first set g2o
        Typedef g2o::BlockSolver<g2o::BlockSolverTraits<3, 1>> BlockSolverType; // Each error term has an optimized variable dimension of 3 and an error value dimension of 1
        Typedef g2o::LinearSolverDense<BlockSolverType::PoseMatrixType> LinearSolverType; // Linear solver type
        
        // Gradient descent method, which can be selected from GN, LM, DogLeg
        Auto solver = new g2o::OptimizationAlgorithmGaussNewton(
            G2o::make_unique<BlockSolverType>(g2o::make_unique<LinearSolverType>()));
        G2o::SparseOptimizer optimizer; // graph model
        optimizer.setAlgorithm(solver); // Set the solver
        optimizer.setVerbose(true); // Turn on debug output
        
        // Add vertices to the graph
        CurveFittingVertex *v = new CurveFittingVertex();
        V->setEstimate(Eigen::Vector3d(ae, be, ce));
        V->setId(0);
        optimizer.addVertex(v);
        
        // Add a side to the picture
        For (int i = 0; i < N; i++) {
            CurveFittingEdge *edge = new CurveFittingEdge(x_data[i]);
            Edge->setId(i);
            Edge->setVertex(0, v); // Set the vertices of the connection
            Edge->setMeasurement(y_data[i]); // Observe the value
            Edge->setInformation(Eigen::Matrix<double, 1, 1>::Identity() * 1 / (w_sigma * w_sigma)); // Information matrix: the inverse of the covariance matrix
            optimizer.addEdge(edge);
        }
        
        // Perform optimization
        Cout << "start optimization" << endl;
        Chrono::steady_clock::time_point t1 = chrono::steady_clock::now();
        optimizer.initializeOptimization();
        Optimizer.optimize(10);
        Chrono::steady_clock::time_point t2 = chrono::steady_clock::now();
        Chrono::duration<double> time_used = chrono::duration_cast<chrono::duration<double>>(t2 - t1);
        Cout << "solve time cost = " << time_used.count() << " seconds. " << endl;
        
        // output optimized value
        Eigen::Vector3d abc_estimate = v->estimate();
        Cout << "estimated model: " << abc_estimate.transpose() << endl;
        
        Return 0;
    }
\end{lstlisting}

In this program, we derive the graph-optimized vertices and edges for curve fitting from g2o: CurveFittingVertex and CurveFittingEdge, which essentially extends the way g2o is used. These two classes are derived from the BaseVertex and BaseUnaryEdge classes, respectively. In the derived class, we rewrote important virtual functions:

\begin{enumerate}
\item Update function for vertex: oplusImpl. We know that the most important part of the optimization process is the calculation of the incremental $\Delta \bm{x}$, which handles $\bm{x}_{k+1} = \bm{x}_k + \Delta The process of \bm{x}$.

Readers may think that this is not something worth mentioning, because it is just a simple addition. Why doesn't g2o help us? In the curve fitting process, since the optimization variable (curve parameter) itself is located in \textbf{vector space}, this update calculation is indeed a simple addition. However, when the optimization variable is not in the vector space, for example, $\bm{x}$ is the camera pose, it does not necessarily have an addition operation. At this point, you need to redefine the behavior of \textbf{incremental increments on existing estimates}. According to the explanation in Lecture 4, we may use left-multiply update or right-multiply update instead of direct addition.

\item The reset function of the vertex: setToOriginImpl. This is trivial, we can zero the estimate.

\item The error calculation function of the edge: computeError. This function takes the current estimate of the vertex to which the edge is connected and compares it to its observations based on the curve model. This is consistent with the error model in the least squares problem.
    
\item The Jacobian function of the side: linearizeOplus. In this function we calculate the Jacobian of each edge relative to the vertex.

\item Save and read disk functions: read, write. Since we don't want to do read/write operations, leave it blank.
\end{enumerate}

After defining the vertices and edges, we declare a graph model in the main function, then add the vertices and edges to the graph model according to the generated noise data, and finally call the optimization function to optimize. G2o will give an optimized result:

\clearpage
\begin{lstlisting}[language=sh,caption=Terminal output:]
Start optimization
Iteration= 0 chi2= 376785.128234 time= 3.3299e-05 cumTime= 3.3299e-05 edges= 100 schur= 0
Iteration= 1 chi2= 35673.566018 time= 1.3789e-05 cumTime= 4.7088e-05 edges= 100 schur= 0
Iteration= 2 chi2= 2195.012304 time= 1.2323e-05 cumTime= 5.9411e-05 edges= 100 schur= 0
Iteration= 3 chi2= 174.853126 time= 1.3302e-05 cumTime= 7.2713e-05 edges= 100 schur= 0
Iteration= 4 chi2= 102.779695 time= 1.2424e-05 cumTime= 8.5137e-05 edges= 100 schur= 0
Iteration= 5 chi2= 101.937194 time= 1.2523e-05 cumTime= 9.766e-05 edges= 100 schur= 0
Iteration= 6 chi2= 101.937020 time= 1.2268e-05 cumTime= 0.000109928 edges= 100 schur= 0
Iteration= 7 chi2= 101.937020 time= 1.2612e-05 cumTime= 0.00012254 edges= 100 schur= 0
Iteration= 8 chi2= 101.937020 time= 1.2159e-05 cumTime= 0.000134699 edges= 100 schur= 0
Iteration= 9 chi2= 101.937020 time= 1.2688e-05 cumTime= 0.000147387 edges= 100 schur= 0
Solve time cost = 0.000919301 seconds.
\end{lstlisting}

We use the Gauss-Newton method for gradient descent, and after 9 iterations, we get the optimization results, which is similar to Ceres and handwritten Gauss-Newton method. From the perspective of running speed, our experimental conclusion is that handwriting is faster than g2o, and g2o is faster than Ceres. This is a generally intuitive experience, and versatility and efficiency are often contradictory. However, Ceres used automatic derivation in this experiment, and the solver configuration is not exactly the same as Gauss Newton, so it looks slower.

\section{小结}
This section introduces a nonlinear optimization problem often encountered in SLAM: the least squares problem consisting of the sum of squares of many error terms. We introduced its definition and solution, and discussed two main ways of gradient descent: the Gauss-Newton method and the Levenberg-Machalt method. In the practical part, the handwritten Gauss-Newton method, Ceres and g2o optimization libraries are used to solve the same curve fitting problem, and they are found to give similar results.

Since we haven't talked about the Bundle Adjustment in detail, we chose a simple but representative example of curve fitting in the practice section to demonstrate the general nonlinear least squares solution. In particular, if you use g2o to fit a curve, you must first convert the problem to a graph optimization, defining new vertices and edges. There are some roundabouts in this approach—the main purpose of g2o is not here. In contrast, it is natural for Ceres to define the error term for the curve fitting problem, because it is an optimization library itself. However, more problems in SLAM are how to solve an optimization problem with many camera poses and many spatial points. In particular, when the camera pose is expressed in Lie algebra, how the error term is calculated with respect to the derivative of the camera pose will be a matter worthy of detailed discussion. We will find in subsequent content that g2o provides a large number of ready-made vertices and edges, which is very convenient for camera pose estimation. In Ceres, we have to implement each Cost Function ourselves, which is inconvenient.

In the two programs in the practice section, we did not calculate the derivative of the curve model for the three parameters, but used the numerical derivation of the optimization library, which makes the theory and code simple. The Ceres library provides automatic derivation and runtime numerical derivation based on template elements, while g2o only provides a way to derive runtime values. However, for most problems, if you can derive the analytical form of the Jacobian matrix and tell the optimization library, you can avoid many problems in numerical derivation.

Finally, I hope that readers will be able to adapt to the extensive use of template programming by Ceres and g2o. Maybe it will look scary at first (especially Ceres sets the parentheses of the residual block and the code for the g2o initialization part), but after familiarity, it feels like this is natural and easy to extend. We will continue to discuss sparsity, kernel functions, Pose Graph and other issues in the SLAM backend.
\section*{ Exercises}
\begin{enumerate}
\item proves that the linear equation $\bm{A} \bm{x} = \bm{b}$ when the coefficient matrix $\bm{A}$ is overtimed, and the least squares solution is $\bm{x} = (\ Bm{A}^\mathrm{T}\bm{A})^{-1}\bm{A}^\mathrm{T} \bm{b}$.
\item investigates the advantages and disadvantages of the steepest descent method, the Newton method, the Gauss-Newton method, and the Levenberg-Marquart method. In addition to our Ceres library and g2o library, what other optimization libraries are there? (You may find some libraries on MATLAB.)
\item Why is the Gauss-Newton method's incremental equation coefficient matrix not correct? What is the geometric meaning of uncertainty? Why is the solution unstable in this case?
What is \item DogLeg? What are the similarities and differences between it and the Gauss Newton method and Levinburg-Marquart method? Please search for the relevant material \footnote{\mbox{for example,}\url{http://www.numerical.rl.ac.uk/people/nimg/course/lectures/raphael/lectures/lec7slides.pdf}. }.
\item Read Ceres' teaching materials (\url{http://ceres-solver.org/tutorial.html}) to better understand its usage.
\item Read the documentation that comes with g2o. Can you read it? If you still can't fully understand it, please come back after the 10th and 11th lectures.
\item[\optional] Please change the curve model in the curve fitting experiment and optimize the experiment with Ceres and g2o. For example, more parameters and more complex models can be used.
\end{enumerate}