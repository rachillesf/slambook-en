% !Mode:: "TeX:UTF-8"
\chapter{visual odometer 2}
\label{cpt:vo2}
\begin{mdframed}
\textbf{main target}
\begin{enumerate}[labelindent=0em,leftmargin=1.5em]
\item Understand the principle of optical flow tracking feature points.
\item Understand how the direct method estimates the pose of the camera.
\item uses g2o for direct method calculations.
\end{enumerate}
\end{mdframed}

The direct method is another major branch of the visual odometer, which is quite different from the feature point method. Although it has not become the mainstream of the current VO, but after several years of development, the direct method has been able to equal the feature point method to a certain extent. In this lecture we will introduce the principles of direct law and implement the core part of the direct method.

\newpage
\includepdf{resources/other/ch8.pdf}

\newpage
\section{Direct method extraction}
In the last lecture, we introduced the method of estimating camera motion using feature points. Although the feature point method dominates the visual odometer, researchers have realized that it has at least the following shortcomings:

\begin{enumerate}
\item The extraction of key points and the calculation of descriptors are time consuming. In practice, SIFT is currently not calculated in real time on the CPU, and ORB requires nearly 20ms of calculation. If the entire SLAM is running at 30ms/frame, then most of the time will be spent calculating feature points.

\item When uses feature points, all information except feature points is ignored. An image has hundreds of thousands of pixels, and the feature points are only a few hundred. Most of the \textbf{maybe useful} image information is discarded using only feature points.

\item The camera sometimes moves to the \textbf{feature missing} place, which often has no obvious texture information. For example, sometimes we face a white wall or an empty corridor. The number of feature points in these scenes will be significantly reduced, and we may not find enough matching points to calculate camera motion.
\end{enumerate}

We see that there are some problems with the use of feature points. Is there any way to overcome these shortcomings? We have the following ideas:

\begin{itemize}
\item retains feature points, but only computes key points and does not calculate descriptors. At the same time, use \textbf{Optical Flow} to track the motion of feature points. This avoids the time required to calculate and match the descriptor, and the calculation time of the optical flow itself is smaller than the calculation and matching of the descriptor.
\item only computes key points and does not calculate descriptors. At the same time, use \textbf{Direct Method} to calculate the position of the feature point in the image at the next moment. This also skips the calculation process of the descriptor and also saves the calculation time of the optical flow.
\end{itemize}

The first method still uses feature points, but replaces the matching descriptor with optical flow tracking. It is estimated that the camera geometry is still using the inverse geometry, PnP or ICP algorithm. This will still require the key points extracted to be distinguishable, that is, we need to mention the corner points. In the direct method, we estimate the motion of the camera and the projection of the point based on the \textbf{pixel grayscale information} of the image. The point that is not required to be extracted must be a corner point. As will be seen later, they can even be random selection points.

When estimating the camera motion using the feature point method, we regard the feature point as a fixed point fixed in the three-dimensional space. Camera motion is optimized by \textbf{Reprojection error} based on their projected position in the camera. In this process, we need to know exactly where the pixel points are projected in the two cameras - this is why we want to match or track the features. At the same time, we also know that computing and matching features require a lot of computation. In contrast, in the direct method, we do not need to know the correspondence between points, but to find them by minimizing \textbf{Photometric error}.

The direct method is the focus of this lecture. It exists to overcome the above disadvantages of the feature point method. The direct method estimates the motion of the camera based on the luminance information of the pixel, and can completely eliminate the calculation of key points and descriptors, thus avoiding the calculation time of the feature and avoiding the missing feature. As long as there are light and dark changes in the scene (which can be gradients without local image gradients), the direct method works. According to the number of pixels used, the direct method is divided into three types: sparse, dense, and semi-dense. Compared with the feature point method, only the sparse feature points (sparse maps) can be reconstructed, and the direct method also has the ability to restore dense or semi-dense structures.

Historically, there was also the use of direct law in the early days \textsuperscript{\cite{Silveira2008}}. With the emergence of open source projects using direct methods (such as SVO\textsuperscript{\cite{Forster2014}}, LSD-SLAM\textsuperscript{\cite{Engel2014}}, DSO\textsuperscript{\cite{Engel2016}}, etc. They are gradually taking the mainstream stage and becoming an important part of the visual mileage calculation.

\section{2D Optical Flow}
The direct method evolved from the optical flow. They are very similar and have the same assumptions. The optical flow describes the motion of the pixel in the image, while the direct law is accompanied by a camera motion model. In order to illustrate the direct method, we may wish to introduce the optical flow first.

Optical flow is a method of describing the movement of pixels between images over time, as shown by \autoref{fig:LK}~. As time goes by, the same pixel will move in the image, and we want to track its motion. Among them, the calculation of partial pixel motion is called \textbf{sparse optical flow}, and the calculation of all pixels is called \textbf{dense optical flow}. The sparse optical flow is represented by Lucas-Kanade optical flow \textsuperscript{\cite{Lucas1981}} and can be used to track feature point locations in SLAM. The dense optical flow is represented by Horn-Schunck optical flow \textsuperscript{\cite{Horn1981}}. Therefore, this section mainly introduces Lucas-Kanade optical flow, also known as LK optical flow.

\begin{figure}[!htp]
\centering
\includegraphics[width=1.0\linewidth]{vo2/opticalFlow}
\caption{LK optical flow diagram. }
\label{fig:LK}
\end{figure}

\subsection*{Lucas-Kanade Optical Stream}
In the LK optical flow, we think that the image from the camera changes over time. The image can be thought of as a function of time: $\bm{I}(t)$. Then, at the time of $t$, the pixel at $(x,y)$, its gray scale can be written as
\[
\bm{I}(x,y,t).
\]
This way the image is seen as a function of position and time, and its range is the gray level of the pixels in the image. Now consider a fixed spatial point whose pixel coordinates at $t$ is $x,y$. Due to the motion of the camera, its image coordinates will change. We want to estimate the position of this spatial point in the image at other times. How to estimate it? Here we introduce the basic assumptions of the optical flow method.

\textbf{Gray invariant hypothesis}: The pixel gray value of the same spatial point is fixed in each image.

For the pixel where $t$ is at $(x,y)$, we set $t+\mathrm{d}t$ to move it to $(x+\mathrm{d}x, y+\mathrm{d}y) $. Since the gray level is unchanged, we have:
\begin{equation}
\bm{I}(x+\mathrm{d}x, y+\mathrm{d}y, t+\mathrm{d}t) = \bm{I} (x,y,t).
\end{equation}

Note that the gray-scale invariant assumption is a strong assumption, and it may not be true in practice. In fact, due to the different materials of the object, the pixels will appear highlights and shadows; sometimes, the camera will automatically adjust the exposure parameters to make the image overall brighter or darker. At these times, the gray-scale invariant assumptions are not true, so the result of the optical flow is not necessarily reliable. However, on the other hand, all algorithms work under certain assumptions. If we don't make any assumptions, we can't design a practical algorithm. So let's assume that the assumption is true and see how to calculate the motion of the pixel.

Taylor expansion on the left, retaining the first-order item, you get:
\begin{equation}
\bm{I} \left( {x + \mathrm{d}x,y + \mathrm{d}y,t + \mathrm{d}t} \right) \approx \bm{I} \left( { x,y,t} \right) + \frac{{\partial \bm{I} }}{{\partial x}}\mathrm{d}x + \frac{{\partial \bm{I}}} {{\partial y}}\mathrm{d}y + \frac{{\partial \bm{I}}}{{\partial t}}\mathrm{d}t.
\end{equation}

Because we assume that the gray level is constant, then the gray level at the next moment is equal to the previous gray level, thus:
\begin{equation}
 \frac{{\partial \bm{I} }}{{\partial x}}\mathrm{d}x + \frac{{\partial \bm{I}}}{{\partial y}}\mathrm{ d}y + \frac{{\partial \bm{I}}}{{\partial t}}\mathrm{d}t = 0.
\end{equation}

Dividing both sides by $\mathrm{d}t$ gives:
\begin{equation}\label{key}
 \frac{{\partial \bm{I} }}{{\partial x}} \frac{\mathrm{d}x}{\mathrm{d}t} + \frac{{\partial \bm{I} }}{{\partial y}} \frac{\mathrm{d}y}{\mathrm{d}t} =- \frac{{\partial \bm{I}}}{{\partial t}}.
\end{equation}

Where $\mathrm{d}x / \mathrm{d}t$ is the speed of the pixel on the $x$ axis, and $\mathrm{d}y/\mathrm{d}t$ is on the $y$ axis The speed, remember them as $u, v$. At the same time $\partial \bm{I}/{\partial x}$ is the gradient of the image in the $x$ direction at this point, and the other is the gradient in the $y$ direction, denoted as $\bm{I} _x, \bm{I}_y$. Record the amount of change in grayscale of the image as $\bm{I}_t$, written in matrix form, with:
\begin{equation}
\left[ {\begin{array}{*{20}{c}}
{{ \bm{I}_x}}&{{ \bm{I}_y}}
\end{array}} \right]\left[ \begin{array}{l}
u\\
v
\end{array} \right] = - {\bm{I}_t}.
\end{equation}

We want to calculate the motion of the pixel $u, v$, but this is a one-time equation with two variables, which can't calculate $u,v$. Therefore, additional constraints must be introduced to calculate $u,v$. In the LK optical flow, we assume that \textbf{pixels within a window have the same motion}.

Consider a window of size $w \times w$ that contains $w^2$ of pixels. Since the pixels in the window have the same motion, we have a total of $w^2$ equations:
\begin{equation}
\left[ {\begin{array}{*{20}{c}}
{{ \bm{I}_x}}&{{ \bm{I}_y}}
\end{array}} \right]_k
\left[ \begin{array}{l}
u\\
v
\end{array} \right] = - {\bm{I}_t}_k, \quad k=1, \ldots, w^2.
\end{equation}

Remember:
\begin{equation}
\bm{A} = \left[ {\begin{array}{*{20}{c}}
{{{\left[ {{\bm{I}_x},{\bm{I}_y}} \right]}_1}}\\
\vdots \\
{{{\left[ {{\bm{I}_x},{\bm{I}_y}} \right]}_k}}
\end{array}} \right],\bm{b} = \left[ {\begin{array}{*{20}{c}}
{{ \bm{I}_{t1}}}\\
\vdots \\
{{ \bm{I}_{tk}}}
\end{array}} \right].
\end{equation}

So the whole equation is
\begin{equation}
\bm{A}\left[ \begin{array}{l}
u\\
v
\end{array} \right] = - \bm{b}.
\end{equation}

This is an overdetermined linear equation for $u,v$, and the traditional solution is to find the least squares solution. Least squares have been used many times:
\begin{equation}
{\left[ \begin{array}{l}
u\\
v
\end{array} \right]^*} = -{\left( {{ \bm{A}^\mathrm{T}}\bm{A}} \right)^{ - 1}}{ \bm{ A}^\mathrm{T}}\bm{b}.
\end{equation}

This gives the speed of movement of the pixels between the images $u, v$. When $t$ takes discrete moments instead of continuous time, we can estimate where a block of pixels appears in several images. Since the pixel gradient is only valid locally, if one iteration is not good enough, we will iterate several times more. In SLAM, LK optical flow is often used to track the movement of corner points, we may wish to experience it through the program.

\section{Practice: LK optical flow}
\label{sec:LKFlow}
\subsection{Using LK optical flow}
In the practice section, we will use several sample images to track the feature points above using OpenCV's optical flow. At the same time, we will also manually implement an LK optical flow to achieve a deeper understanding. We use two sample images from the Euroc dataset to extract the corners in the first image and then track their position in the second sheet with the optical flow. First let's use the LK optical flow in OpenCV:

\begin{lstlisting}[language=c++,caption=slambook2/ch8/optical_flow.cpp(fragment)]
// use opencv's flow for validation
Vector<Point2f> pt1, pt2;
For (auto &kp: kp1) pt1.push_back(kp.pt);
Vector<uchar> status;
Vector<float> error;
Cv::calcOpticalFlowPyrLK(img1, img2, pt1, pt2, status, error);
\end{lstlisting}

OpenCV's optical flow is very simple to use, just call the cv::calcOpticalFlowPyrLK function, provide two images before and after and the corresponding feature points, you can get the tracked points, as well as the status and error of each point. We can determine if the corresponding point is correctly tracked based on whether the status variable is 1. This function has some optional parameters, but in the demo we only use the default parameters. We omit other code that mentions features and draws results here, which have been shown in previous programs.

\subsection{Using Gauss-Newton method to achieve optical flow}
\subsubsection{single layer optical flow}
Optical flow can also be viewed as an optimization problem: the optimal pixel offset is estimated by minimizing grayscale errors. So, similar to the various Gaussian Newton normalizations that were implemented before, we now also implement an optical flow based on the Gauss-Newton method.

\begin{lstlisting}[language=c++,caption=slambook2/ch8/optical_flow.cpp(fragment)]
	class OpticalFlowTracker {
		public:
			OpticalFlowTracker(
				const Mat &img1_,
				const Mat &img2_,
				const vector<KeyPoint> &kp1_,
				vector<KeyPoint> &kp2_,
				vector<bool> &success_,
				bool inverse_ = true, bool has_initial_ = false) :
				img1(img1_), img2(img2_), kp1(kp1_), kp2(kp2_), success(success_), inverse(inverse_),
				has_initial(has_initial_) {}
			
			void calculateOpticalFlow(const Range &range);
		
		private:
			const Mat &img1;
			const Mat &img2;
			const vector<KeyPoint> &kp1;
			vector<KeyPoint> &kp2;
			vector<bool> &success;
			bool inverse = true;
			bool has_initial = false;
		};
		
		void OpticalFlowSingleLevel(
			const Mat &img1,
			const Mat &img2,
			const vector<KeyPoint> &kp1,
			vector<KeyPoint> &kp2,
			vector<bool> &success,
			bool inverse, bool has_initial) {
			kp2.resize(kp1.size());
			success.resize(kp1.size());
			OpticalFlowTracker tracker(img1, img2, kp1, kp2, success, inverse, has_initial);
			parallel_for_(Range(0, kp1.size()),
				std::bind(&OpticalFlowTracker::calculateOpticalFlow, &tracker, placeholders::_1));
		}
		
		void OpticalFlowTracker::calculateOpticalFlow(const Range &range) {
			// parameters
			int half_patch_size = 4;
			int iterations = 10;
			for (size_t i = range.start; i < range.end; i++) {
				auto kp = kp1[i];
				double dx = 0, dy = 0; // dx,dy need to be estimated
				if (has_initial) {
					dx = kp2[i].pt.x - kp.pt.x;
					dy = kp2[i].pt.y - kp.pt.y;
				}
				
				double cost = 0, lastCost = 0;
				bool succ = true; // indicate if this point succeeded
				
				// Gauss-Newton iterations
				Eigen::Matrix2d H = Eigen::Matrix2d::Zero();    // hessian
				Eigen::Vector2d b = Eigen::Vector2d::Zero();    // bias
				Eigen::Vector2d J;  // jacobian
				for (int iter = 0; iter < iterations; iter++) {
					if (inverse == false) {
						H = Eigen::Matrix2d::Zero();
						b = Eigen::Vector2d::Zero();
					} else {
						// only reset b
						b = Eigen::Vector2d::Zero();
					}
					
					cost = 0;
					
					// compute cost and jacobian
					for (int x = -half_patch_size; x < half_patch_size; x++)
					for (int y = -half_patch_size; y < half_patch_size; y++) {
						double error = GetPixelValue(img1, kp.pt.x + x, kp.pt.y + y) -
							GetPixelValue(img2, kp.pt.x + x + dx, kp.pt.y + y + dy);;  // Jacobian
						if (inverse == false) {
							J = -1.0 * Eigen::Vector2d(
								0.5 * (GetPixelValue(img2, kp.pt.x + dx + x + 1, kp.pt.y + dy + y) -
									GetPixelValue(img2, kp.pt.x + dx + x - 1, kp.pt.y + dy + y)),
								0.5 * (GetPixelValue(img2, kp.pt.x + dx + x, kp.pt.y + dy + y + 1) -
									GetPixelValue(img2, kp.pt.x + dx + x, kp.pt.y + dy + y - 1))
							);
						} else if (iter == 0) {
							// in inverse mode, J keeps same for all iterations
							// NOTE this J does not change when dx, dy is updated, so we can store it and only compute error
							J = -1.0 * Eigen::Vector2d(
								0.5 * (GetPixelValue(img1, kp.pt.x + x + 1, kp.pt.y + y) -
									GetPixelValue(img1, kp.pt.x + x - 1, kp.pt.y + y)),
								0.5 * (GetPixelValue(img1, kp.pt.x + x, kp.pt.y + y + 1) -
									GetPixelValue(img1, kp.pt.x + x, kp.pt.y + y - 1))
							);
						}
						// compute H, b and set cost;
						b += -error * J;
						cost += error * error;
						if (inverse == false || iter == 0) {
							// also update H
							H += J * J.transpose();
						}
					}
					
					// compute update
					Eigen::Vector2d update = H.ldlt().solve(b);
					
					if (std::isnan(update[0])) {
						// sometimes occurred when we have a black or white patch and H is irreversible
						cout << "update is nan" << endl;
						succ = false;
						break;
					}
					
					if (iter > 0 && cost > lastCost) {
						break;
					}
					
					// update dx, dy
					dx += update[0];
					dy += update[1];
					lastCost = cost;
					succ = true;
					
					if (update.norm() < 1e-2) {
						// converge
						break;
					}
				}
				
				success[i] = succ;
				
				// set kp2
				kp2[i].pt = kp.pt + Point2f(dx, dy);
			}
		}
\end{lstlisting}

We implemented a single-layer optical flow function in the OpticalFlowSingleLevel function, which calls cv::parallel\_for\_ to call OpticalFlowTracker::calculateOpticalFlow in parallel, which calculates the optical flow of the feature points in the specified range. This parallel for loop is internally implemented by the Intel tbb library. We only need to define the function ontology according to its interface, and then pass the function as a std::function object to it.

In the concrete function implementation (ie calculateOpticalFlow), we solve such a problem:
\begin{equation}
\mathop {\min }\limits_{\Delta x,\Delta y} \left\| {{\bm{I}_1}\left( {x,y} \right) - {\bm{I}_2} \left( {x + \Delta x,y + \Delta y} \right)} \right\|_2^2.
\end{equation}
Therefore, the residual is the part inside the parentheses, and the corresponding Jacobi is the gradient of the second image at $x + \Delta x, y + \Delta y$. In addition, according to the literature \cite{Baker2004}, the gradient here can also be replaced by the gradient of the first image $\bm{I}_1 (x,y)$. This alternative method is called the \textbf{inverse} optical flow method. In the reverse optical flow, the gradient of $\bm{I}_1 (x,y)$ remains the same, so we can retain the result of the calculation on the first iteration and use it in subsequent iterations. When the Jacobian is unchanged, the $\bm{H}$ matrix is ​​unchanged, and only one residual is calculated for each iteration, which saves a portion of the calculation.

\subsubsection{Multilayer optical flow}
Since we write the optical flow as an optimization problem, we must assume that the initial value of the optimization is close to the optimal value, in order to guarantee the convergence of the algorithm to a certain extent. Therefore, if the camera moves faster and the difference between the two images is more obvious, the single-layer image optical flow method easily reaches a local minimum value. This situation can be improved by introducing an image pyramid.

\begin{figure}[!htp]
\centering
\includegraphics[width=.7\linewidth]{vo2/image-pyramid}
\caption{Image pyramid and Coarse-to-fine process. }
\label{fig:image-pyramid}
\end{figure}

Image pyramid refers to scaling the same image to get images at different resolutions, as shown by \autoref{fig:image-pyramid}. Taking the original image as the bottom layer of the pyramid, each time the layer is up, the lower layer image is scaled at a certain magnification to obtain a pyramid. Then, when calculating the optical flow, the calculation is started from the top image, and then the tracking result of the upper layer is taken as the initial value of the next optical flow. Since the image of the upper layer is relatively rough, this process is also called \textbf{Coarse-to-fine} optical flow, which is also the usual flow of practical optical flow method.

The advantage from coarse to fine is that when the pixel motion of the original image is large, the motion is still in a small range in the view of the image at the top of the pyramid. For example, the feature points of the original image are moved by 20 pixels, and it is easy to be trapped in the minimum value due to the non-convexity of the image. But now suppose that there is a pyramid with a magnification of 0.5 times, then in the upper two images, the pixel motion is only 5 pixels, and the result is obviously better than optimizing directly on the original image.

We implemented a multi-layer optical flow in the program, the code is as follows:
\begin{lstlisting}[language=c++,caption=slambook2/ch8/optical_flow.cpp(fragment)]
Void OpticalFlowMultiLevel(
Const Mat &img1,
Const Mat &img2,
Const vector<KeyPoint> &kp1,
Vector<KeyPoint> &kp2,
Vector<bool> &success,
Bool inverse) {

// parameters
Int pyramids = 4;
Double pyramid_scale = 0.5;
Double scales[] = {1.0, 0.5, 0.25, 0.125};

// create pyramids
Vector<Mat> pyr1, pyr2; // image pyramids
For (int i = 0; i < pyramids; i++) {
If (i == 0) {
Pyr1.push_back(img1);
Pyr2.push_back(img2);
} else {
Mat img1_pyr, img2_pyr;
Cv::resize(pyr1[i - 1], img1_pyr,
Cv::Size(pyr1[i - 1].cols * pyramid_scale, pyr1[i - 1].rows * pyramid_scale));
Cv::resize(pyr2[i - 1], img2_pyr,
Cv::Size(pyr2[i - 1].cols * pyramid_scale, pyr2[i - 1].rows * pyramid_scale));
Pyr1.push_back(img1_pyr);
Pyr2.push_back(img2_pyr);
}
}

// coarse-to-fine LK tracking in pyramids
Vector<KeyPoint> kp1_pyr, kp2_pyr;
For (auto &kp:kp1) {
Auto kp_top = kp;
Kp_top.pt *= scales[pyramids - 1];
Kp1_pyr.push_back(kp_top);
Kp2_pyr.push_back(kp_top);
}

For (int level = pyramids - 1; level >= 0; level--) {
// from coarse to fine
Success.clear();
OpticalFlowSingleLevel(pyr1[level], pyr2[level], kp1_pyr, kp2_pyr, success, inverse, true);

If (level > 0) {
For (auto &kp: kp1_pyr)
Kp.pt /= pyramid_scale;
For (auto &kp: kp2_pyr)
Kp.pt /= pyramid_scale;
}
}

For (auto &kp: kp2_pyr)
Kp2.push_back(kp);
}
\end{lstlisting}

This code constructs a four-layer pyramid with a magnification of 0.5 and calls a single-layer optical flow function to implement a multi-layer optical flow. In the main function, we tested the performance of OpenCV optical flow, single-layer optical flow and multi-layer optical flow for two images, and calculated their running time:
\begin{lstlisting}[language=sh,caption=terminal input:]
./build/optical_flow
Build pyramid time: 0.000150349
Track pyr 3 cost time: 0.000304633
Track pyr 2 cost time: 0.000392889
Track pyr 1 cost time: 0.000382347
Track pyr 0 cost time: 0.000375099
Optical flow by gauss-newton: 0.00189268
Optical flow by opencv: 0.00220134
\end{lstlisting}
In terms of runtime, the time-consuming of the multi-layer optical flow method is roughly equivalent to that of OpenCV. Since the parallelizers behave differently on each run, these numbers are not exactly the same on the reader's machine. See the \autoref{fig:optical-flow-result} for a comparison of the optical flows. From the results graph, the multi-layer optical flow is equivalent to the effect of OpenCV, and the single-layer optical flow is significantly weaker than the multi-layer optical flow.

\begin{figure}[!htp]
\centering
\includegraphics[width=.85\linewidth]{vo2/optical-flow}
\caption{Comparison of results of various optical flows}
\label{fig:optical-flow-result}
\end{figure}

\subsection{Optical flow practice summary}
We can see that LK optical flow tracking can directly obtain the correspondence of feature points. This correspondence is like a match of descriptors, but the optical flow requires more continuity of images and illumination stability. We can estimate camera motion using PnP, ICP, or counter-geometry by feature points of optical flow tracking. These methods were introduced in the previous lecture and are not discussed here.

In terms of runtime, the demo experiment is about 230 feature points. OpenCV and multi-layer optical flow take about 2 milliseconds to complete tracking (the CPU I use is Intel I7-8550U), which is actually quite fast. If we use a key point like FAST before, then the entire optical flow calculation can be done for about 5 milliseconds, which is very fast compared to feature matching. However, if the position of the corner is not good, the optical flow is easy to lose or give a wrong result. This requires the subsequent algorithm to have a certain abnormal value removal mechanism. We will leave it to the engineering chapter.

In summary, the optical flow method can accelerate the visual mileage calculation based on feature points, avoiding the process of calculating and matching the descriptors, but requires the camera to be smoother (or higher acquisition frequency).

\section{Direct Method}
Next, let's discuss the direct method that has some similarity to the optical flow. Similar to the previous content, we first introduce the principle of the direct method, and then use the direct method to achieve one pass.

\subsection{Derivation of direct method}
In the optical flow, we first track the position of the feature points and then determine the motion of the camera based on these positions. Then, such a two-step approach, it is difficult to guarantee global optimality. We can ask, can you adjust the results of the previous step in the next step? For example, if I think that the camera turns right 15 degrees, can the optical flow be based on the assumption that the 15 degree motion is the initial value, and adjust the calculation result of the optical flow? The direct method is the result of following this idea.

As shown in \autoref{fig:directMethod}~, consider a space point $P$ and a camera at two moments. The world coordinate of $P$ is $[X,Y,Z]$, which is imaged on two cameras with pixel coordinates of $\bm{p}_1, \bm{p}_2$.

\begin{figure}[!htp]
\centering
\includegraphics[width=.85\linewidth]{vo2/directMethod}
\caption{Direct method diagram. }
\label{fig:directMethod}
\end{figure}

Our goal is to find the relative pose change of the first camera to the second camera. We use the first camera as the frame of reference, and set the rotation and translation of the second camera to $\bm{R}, \bm{t}$ (corresponding to Lie group as $\bm{T}$). At the same time, the internal parameters of the two cameras are the same, recorded as $\bm{K}$. For the sake of clarity, we write the complete projection equation:
\begin{align*}
{\bm{p}_1} &= {\left[ \begin{array}{l}
u\\
v\\
	1
\end{array} \right]_1} = \frac{1}{Z_1} \bm{KP}, \\
{\bm{p}_2} &= {\left[ \begin{array}{l}
u\\
v\\
	1
\end{array} \right]_2} = \frac{1}{Z_2} \bm{K}\left( {\bm{RP} +\bm{t}} \right) = \frac{1}{ Z_2} \bm{K} \left(\bm{T} \bm{P} \right)_{1:3}.
\end{align*}
Where $Z_1$ is the depth of $P$, $Z_2$ is the depth of $P$ in the second camera coordinate system, which is the third coordinate value of $\bm{RP}+\bm{t}$ . Since $\bm{T}$ can only be multiplied by the homogeneous coordinates, we need to take the first 3 elements after we have finished. This is consistent with the content of \ref{cpt:5}.

In the recall feature point method, since we know the pixel position of $\bm{p}_1, \bm{p}_2$ by matching the descriptor, we can calculate the position of the reprojection. But in the direct method, since there is no feature matching, we have no way of knowing which $\bm{p}_2$ corresponds to the same point as $\bm{p}_1$. The idea of ​​the direct method is to find the location of $\bm{p}_2$ based on the current camera pose estimate. But if the camera is not well positioned, the appearance of $\bm{p}_2$ will be significantly different from $\bm{p}_1$. So, to reduce this difference, we optimized the camera's pose to look for $\bm{p}_2$ more similar to $\bm{p}_1$. This can also be done by solving an optimization problem, but at this point the minimum is not the reprojection error, but the \textbf{Photometric Error}, which is the brightness error of two pixels of $P$:
\begin{equation}
e = {\bm{I}_1}\left( {{\bm{p}_1}} \right) - {\bm{I}_2}\left( {{\bm{p}_2}} \right ).
\end{equation}

Note that $e$ is a scalar here. Similarly, the optimization goal is the two norm of the error, temporarily taking the form of unweighted, as:
\begin{equation}
\mathop {\min }\limits_{\bm{T}} J\left( \bm{T} \right) = \|e\|^2.
\end{equation}

The reason for this optimization is still based on the \textbf{gray invariant assumption}. We assume that the gradation of a spatial point imaged at various angles is constant. We have a lot of (such as $N$) space points $P_i$, then the whole camera pose estimation problem becomes
\begin{equation}
\mathop {\min }\limits_{\bm{T}} J\left( \bm{T} \right) = \sum\limits_{i = 1}^N {e_i^\mathrm{T}{e_i} }, \quad {e_i} = {\bm{I}_1}\left( {{\bm{p}_{1,i}}} \right) - {\bm{I}_2}\left( { { \bm{p}_{2,i}}} \right).
\end{equation}

Note that the optimization variable here is the camera pose $\bm{T}$, instead of optimizing the motion of each feature point like the optical flow. To solve this optimization problem, we are concerned with how the error $e$ changes with the camera pose $\bm{T}$, and we need to analyze their derivative relationships. Therefore, define two intermediate variables:
\begin{align*}
\bm{q} &= \bm{T} \bm{P}, \\
\bm{u} &= \frac{1}{{{Z_2}}} \bm{K} \bm{q}.
\end{align*}
Here $\bm{q}$ is the coordinates of $P$ in the second camera coordinate system, and $\bm{u}$ is its pixel coordinates. Obviously $\bm{q}$ is a function of $\bm{T}$, $\bm{u}$ is a function of $\bm{q}$, which is also a function of $\bm{T}$. Consider the left perturbation model of Lie algebra, using a first-order Taylor expansion, because:
\begin{equation}
e(\bm{T})=\bm{I}_1(\bm{p}_{1})-\bm{I}_2(\bm{u}),
\end{equation}
and so:
\begin{equation}
\frac{\partial e}{\partial \bm{T}} = \frac{{\partial {\bm{I}_2}}}{{\partial \bm{u}}}\frac{{\partial \bm{u}}}{{\partial \bm{q}}}\frac{{\partial \bm{q}}}{{\partial \delta \bm{\xi} }}\delta \bm{ \xi},
\end{equation}
Where $\delta \bm{\xi}$ is the left perturbation of $\bm{T}$. We see that the first derivative is divided into three terms by the chain rule, and these three items are easy to calculate:

\begin{enumerate}
	\item $ \partial \bm{I}_2 / \partial \bm{u} $ is the pixel gradient at $\bm{u}$.
	\item $ \partial \bm{u} / \partial \bm{q} $ is the derivative of the projection equation for the 3D point in the camera coordinate system. Remember $\bm{q}=[X,Y,Z]^\mathrm{T}$, according to the deduction of $\ref{cpt:7}$, the derivative is
	\begin{equation}
	\frac{{\partial \bm{u}}}{{\partial \bm{q}}} = \left[ {\begin{array}{*{20}{c}}
		{\frac{{\partial u}}{{\partial X}}}&{\frac{{\partial u}}{{\partial Y}}}&{\frac{{\partial u}}{{\partial Z}}}\\
		{\frac{{\partial v}}{{\partial X}}}&{\frac{{\partial v}}{{\partial Y}}}&{\frac{{\partial v}}{{\partial Z}}}
		\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
		{\frac{{{f_x}}}{{\rm{Z}}}}&0&{ - \frac{{{f_x}X}}{{{Z^2}}}}\\
		0&{\frac{{{f_y}}}{Z}}&{ - \frac{{{f_y}Y}}{{{Z^2}}}}
		\end{array}} \right].
	\end{equation}

\item ${\partial \bm{q}}/{\partial \delta \bm{\xi} }$ is the derivative of the transformed 3D point-to-point transformation, which was introduced in Lie algebra:
\begin{equation}
\frac{{\partial \bm{q}}}{{\partial \delta \bm{\xi} }} = \left[ { \bm{I}, - {\bm{q}^ \wedge }} \right].
\end{equation}
\end{enumerate}

In practice, since the last two items are only related to the 3D point $\bm{q}$ and are not related to the image, we often combine them together:
\begin{equation}
\frac{{\partial \bm{u}}}{{\partial \delta \bm{\xi} }} = \left[ {\begin{array}{*{20}{c}}
{\frac{{{f_x}}}{Z}}&0&{ - \frac{{{f_x}X}}{{{Z^2}}}}&{ - \frac{{{f_x}XY}} {{{Z^2}}}}&{{f_x} + \frac{{{f_x}{X^2}}}{{{Z^2}}}}&{ - \frac{{{f_x} Y}}{Z}}\\
	0&{\frac{{{f_y}}}{Z}}&{ - \frac{{{f_y}Y}}{{{Z^2}}}}&{ - {f_y} - \frac{{{f_y}{Y^2}}}{{{Z^2}}}}&{\frac{{{f_y}XY}}{{{Z^2}}}}&{\frac{{{f_y}X}}{Z}}
	\end{array}} \right].
\end{equation}

这个$2 \times 6$的矩阵在上一讲中也出现过。于是，我们推导出误差相对于李代数的雅可比矩阵：
\begin{equation}
\label{eq:jacobianofDirect}
\bm{J} =  - \frac{{\partial { \bm{I}_2}}}{{\partial \bm{u}}}\frac{{\partial \bm{u}}}{{\partial \delta \bm{\xi} }}.
\end{equation}

For the problem of $N$ points, we can use this method to calculate the Jacobian matrix of the optimization problem, and then use the Gauss-Newton method or the Levinberg-Marquart method to calculate the increment and iterative solution. So far, we have derived the direct process of estimating the pose of the camera. Here is a program to demonstrate how the direct method is used.

\subsection{direct law discussion}
In the above derivation, $P$ is a spatial point of a known location. How did it come about? Under the RGB-D camera, we can backproject any pixel back into 3D space and then project it into the next image. If it is binocular, the depth of the pixel can also be calculated from the parallax. If you are in a monocular camera, this is even more difficult because we have to consider the uncertainty caused by the depth of $P$. Detailed depth estimates are discussed in Lecture 13. Now let's consider the simple case where the $P$ depth is known.

Based on the source of $P$, we can classify the direct method:
\begin{enumerate}
\item $P$ comes from sparse key points, which we call sparse direct methods. Usually we use hundreds to thousands of key points, and like the L-K optical flow, assume that the pixels around it are also invariant. This sparse direct method does not have to calculate descriptors and uses only a few hundred pixels, so it is the fastest, but only sparse reconstruction can be computed.
\item $P$ comes from a partial pixel. We see that in the \eqref{eq:jacobianofDirect}, if the pixel gradient is zero, the entire Jacobian matrix is ​​zero and does not contribute to the calculated motion increment. Therefore, consider using only pixels with gradients and discarding where the pixel gradient is not obvious. This is called the semi-dense direct method and can reconstruct a semi-dense structure.
\item $P$ is all pixels, called the dense direct method. Dense reconciliation requires the calculation of all pixels (typically hundreds of thousands to millions), so most of them cannot be calculated in real time on existing CPUs and require GPU acceleration. However, as discussed earlier, points where the pixel gradient is not significant will not contribute much to motion estimation, and it will be difficult to estimate the position during reconstruction.
\end{enumerate}

It can be seen that from sparse to dense reconstruction, it can be calculated by the direct method. Their calculations are gradually increasing. The sparse method can quickly solve the camera pose, while the dense method can create a complete map. Which method is used depends on the application environment of the robot. In particular, on the low-end computing platform, the sparse direct method can achieve very fast results, suitable for occasions with high real-time and limited computing resources\textsuperscript{\cite{Engel2016}}.

\section{Practice: Direct Law}
\subsection{single layer direct method}
Now let's demonstrate how to use the sparse direct method. Since the book does not involve GPU programming, the dense direct method is omitted. At the same time, in order to keep the program simple, we use data with depth instead of unary data, which can omit the single-purpose deep recovery part. Depth recovery based on feature points (ie, triangulation) has been introduced in the previous lecture, and depth recovery based on block matching will be described later. So in this section we consider the sparse direct method of dual purpose.

Since solving the direct method is equivalent to solving an optimization problem, you can use the optimization library g2o or Ceres to help solve the problem, or you can implement the Gauss-Newton method yourself. Similar to the optical flow, the direct method can also be divided into a single-layer direct method and a pyramid-based multi-layer direct method. We also first implement the single-layer direct method, and then expand to multiple layers.

In the single-layer direct method, similar to the parallel optical flow, we can also calculate the error and Jacobian of each pixel in parallel. For this we define a class that is Jacobian:

\begin{lstlisting}[language=c++,caption=slambook2/ch8/direct_method.cpp（片段）]
/// class for accumulator jacobians in parallel
class JacobianAccumulator {
public:
	JacobianAccumulator(
		const cv::Mat &img1_,
		const cv::Mat &img2_,
		const VecVector2d &px_ref_,
		const vector<double> depth_ref_,
		Sophus::SE3d &T21_) :
	img1(img1_), img2(img2_), px_ref(px_ref_), depth_ref(depth_ref_), T21(T21_) {
		projection = VecVector2d(px_ref.size(), Eigen::Vector2d(0, 0));
	}
	
	/// accumulate jacobians in a range
	void accumulate_jacobian(const cv::Range &range);
	
	/// get hessian matrix
	Matrix6d hessian() const { return H; }
	
	/// get bias
	Vector6d bias() const { return b; }
	
	/// get total cost
	double cost_func() const { return cost; }
	
	/// get projected points
	VecVector2d projected_points() const { return projection; }
	
	/// reset h, b, cost to zero
	void reset() {
		H = Matrix6d::Zero();
		b = Vector6d::Zero();
		cost = 0;
	}
	
private:
	const cv::Mat &img1;
	const cv::Mat &img2;
	const VecVector2d &px_ref;
	const vector<double> depth_ref;
	Sophus::SE3d &T21;
	VecVector2d projection; // projected points
	
	std::mutex hessian_mutex;
	Matrix6d H = Matrix6d::Zero();
	Vector6d b = Vector6d::Zero();
	double cost = 0;
};

void JacobianAccumulator::accumulate_jacobian(const cv::Range &range) {
	
	// parameters
	const int half_patch_size = 1;
	int cnt_good = 0;
	Matrix6d hessian = Matrix6d::Zero();
	Vector6d bias = Vector6d::Zero();
	double cost_tmp = 0;
	
	for (size_t i = range.start; i < range.end; i++) {
		// compute the projection in the second image
		Eigen::Vector3d point_ref =
		depth_ref[i] * Eigen::Vector3d((px_ref[i][0] - cx) / fx, (px_ref[i][1] - cy) / fy, 1);
		Eigen::Vector3d point_cur = T21 * point_ref;
		if (point_cur[2] < 0)   // depth invalid
			continue;
		
		float u = fx * point_cur[0] / point_cur[2] + cx, v = fy * point_cur[1] / point_cur[2] + cy;
		if (u < half_patch_size || u > img2.cols - half_patch_size || v < half_patch_size ||
		v > img2.rows - half_patch_size)
			continue;
		
		projection[i] = Eigen::Vector2d(u, v);
		double X = point_cur[0], Y = point_cur[1], Z = point_cur[2],
		Z2 = Z * Z, Z_inv = 1.0 / Z, Z2_inv = Z_inv * Z_inv;
		cnt_good++;
		
		// and compute error and jacobian
		for (int x = -half_patch_size; x <= half_patch_size; x++)
		for (int y = -half_patch_size; y <= half_patch_size; y++) {
			double error = GetPixelValue(img1, px_ref[i][0] + x, px_ref[i][1] + y) -
				GetPixelValue(img2, u + x, v + y);
			Matrix26d J_pixel_xi;
			Eigen::Vector2d J_img_pixel;
			
			J_pixel_xi(0, 0) = fx * Z_inv;
			J_pixel_xi(0, 1) = 0;
			J_pixel_xi(0, 2) = -fx * X * Z2_inv;
			J_pixel_xi(0, 3) = -fx * X * Y * Z2_inv;
			J_pixel_xi(0, 4) = fx + fx * X * X * Z2_inv;
			J_pixel_xi(0, 5) = -fx * Y * Z_inv;
			
			J_pixel_xi(1, 0) = 0;
			J_pixel_xi(1, 1) = fy * Z_inv;
			J_pixel_xi(1, 2) = -fy * Y * Z2_inv;
			J_pixel_xi(1, 3) = -fy - fy * Y * Y * Z2_inv;
			J_pixel_xi(1, 4) = fy * X * Y * Z2_inv;
			J_pixel_xi(1, 5) = fy * X * Z_inv;
			
			J_img_pixel = Eigen::Vector2d(
				0.5 * (GetPixelValue(img2, u + 1 + x, v + y) - GetPixelValue(img2, u - 1 + x, v + y)),
				0.5 * (GetPixelValue(img2, u + x, v + 1 + y) - GetPixelValue(img2, u + x, v - 1 + y))
			);
			
			// total jacobian
			Vector6d J = -1.0 * (J_img_pixel.transpose() * J_pixel_xi).transpose();
			hessian += J * J.transpose();
			bias += -error * J;
			cost_tmp += error * error;
		}
	}
	
	if (cnt_good) {
		// set hessian, bias and cost
		unique_lock<mutex> lck(hessian_mutex);
		H += hessian;
		b += bias;
		cost += cost_tmp / cnt_good;
	}
}
\end{lstlisting}

In the accumulate\_jacobian function of this class, we calculate the pixel error and the Jacobian matrix for the pixels in the specified range according to the previous derivation, and finally add it to the overall $\bm{H}$ matrix. Then, define a function to iterate through the process:
\begin{lstlisting}[language=c++,caption=slambook2/ch8/direct_method.cpp（片段）]
void DirectPoseEstimationSingleLayer(
	const cv::Mat &img1,
	const cv::Mat &img2,
	const VecVector2d &px_ref,
	const vector<double> depth_ref,
	Sophus::SE3d &T21) {
	const int iterations = 10;
	double cost = 0, lastCost = 0;
	JacobianAccumulator jaco_accu(img1, img2, px_ref, depth_ref, T21);
	
	for (int iter = 0; iter < iterations; iter++) {
		jaco_accu.reset();
		cv::parallel_for_(cv::Range(0, px_ref.size()),
			std::bind(&JacobianAccumulator::accumulate_jacobian, &jaco_accu, std::placeholders::_1));
		Matrix6d H = jaco_accu.hessian();
		Vector6d b = jaco_accu.bias();
		
		// solve update and put it into estimation
		Vector6d update = H.ldlt().solve(b);;
		T21 = Sophus::SE3d::exp(update) * T21;
		cost = jaco_accu.cost_func();
		
		if (std::isnan(update[0])) {
			// sometimes occurred when we have a black or white patch and H is irreversible
			cout << "update is nan" << endl;
			break;
		}
		if (iter > 0 && cost > lastCost) {
			cout << "cost increased: " << cost << ", " << lastCost << endl;
			break;
		}
		if (update.norm() < 1e-3) {
			// converge
			break;
		}
		
		lastCost = cost;
		cout << "iteration: " << iter << ", cost: " << cost << endl;
	}
}
\end{lstlisting}
The function finds the corresponding pose update based on the calculated $\bm{H}$ and $\bm{b}$, and then updates to the current estimate. Because we have already introduced the details in the theoretical part, this part of the code does not seem to be particularly difficult.

\subsection{Multilayer Direct Method}
Then, similar to the optical flow, we extend the direct method to the pyramid and use the Coarse-to-fine process to calculate the relative motion. This part of the code and the optical flow are also very similar:
\begin{lstlisting}[language=c++,caption=slambook2/ch8/direct_method.cpp（片段）]
void DirectPoseEstimationMultiLayer(
	const cv::Mat &img1,
	const cv::Mat &img2,
	const VecVector2d &px_ref,
	const vector<double> depth_ref,
	Sophus::SE3d &T21) {
	// parameters
	int pyramids = 4;
	double pyramid_scale = 0.5;
	double scales[] = {1.0, 0.5, 0.25, 0.125};
	
	// create pyramids
	vector<cv::Mat> pyr1, pyr2; // image pyramids
	for (int i = 0; i < pyramids; i++) {
		if (i == 0) {
			pyr1.push_back(img1);
			pyr2.push_back(img2);
		} else {
			cv::Mat img1_pyr, img2_pyr;
			cv::resize(pyr1[i - 1], img1_pyr,
				cv::Size(pyr1[i - 1].cols * pyramid_scale, pyr1[i - 1].rows * pyramid_scale));
			cv::resize(pyr2[i - 1], img2_pyr,
				cv::Size(pyr2[i - 1].cols * pyramid_scale, pyr2[i - 1].rows * pyramid_scale));
			pyr1.push_back(img1_pyr);
			pyr2.push_back(img2_pyr);
		}
	}
	
	double fxG = fx, fyG = fy, cxG = cx, cyG = cy;  // backup the old values
	for (int level = pyramids - 1; level >= 0; level--) {
		VecVector2d px_ref_pyr; // set the keypoints in this pyramid level
		for (auto &px: px_ref) {
			px_ref_pyr.push_back(scales[level] * px);
		}
		
		// scale fx, fy, cx, cy in different pyramid levels
		fx = fxG * scales[level];
		fy = fyG * scales[level];
		cx = cxG * scales[level];
		cy = cyG * scales[level];
		DirectPoseEstimationSingleLayer(pyr1[level], pyr2[level], px_ref_pyr, depth_ref, T21);
	}	
}
\end{lstlisting}
It should be noted that because the direct method is used to get the internal parameters of the camera, when the pyramid scales the image, the corresponding internal parameters also need to be multiplied by the corresponding magnification.

\subsection{Result Discussion}
Finally, we use some sample images to test the results of the direct method. We use several images of the Kitti\textsubscript{\cite{Geiger2013}} autopilot dataset. First, we read the first image left.png, calculate the depth corresponding to each pixel in the corresponding disparity map disparity.png, and then calculate the camera using the direct method for the five images of 000001.png-000005.png. Pose. In order to demonstrate the insensitivity of the direct method to feature points, we randomly select some points in the first image, and do not use any corner points or feature point extraction algorithms to see its results.
\begin{lstlisting}[language=c++,caption=slambook2/ch8/direct_method.cpp（片段）]
int main(int argc, char **argv) {
	
	cv::Mat left_img = cv::imread(left_file, 0);
	cv::Mat disparity_img = cv::imread(disparity_file, 0);
	
	// let's randomly pick pixels in the first image and generate some 3d points in the first image's frame
	cv::RNG rng;
	int nPoints = 2000;
	int boarder = 20;
	VecVector2d pixels_ref;
	vector<double> depth_ref;
	
	// generate pixels in ref and load depth data
	for (int i = 0; i < nPoints; i++) {
		int x = rng.uniform(boarder, left_img.cols - boarder);  // don't pick pixels close to boarder
		int y = rng.uniform(boarder, left_img.rows - boarder);  // don't pick pixels close to boarder
		int disparity = disparity_img.at<uchar>(y, x);
		double depth = fx * baseline / disparity; // you know this is disparity to depth
		depth_ref.push_back(depth);
		pixels_ref.push_back(Eigen::Vector2d(x, y));
	}
	
	// estimates 01~05.png's pose using this information
	Sophus::SE3d T_cur_ref;
	
	for (int i = 1; i < 6; i++) {  // 1~10
		cv::Mat img = cv::imread((fmt_others % i).str(), 0);
		DirectPoseEstimationMultiLayer(left_img, img, pixels_ref, depth_ref, T_cur_ref);
	}
	return 0;
}
\end{lstlisting}

The reader can try to run this program on your machine, it will output the tracking points on each layer of the pyramid of each image, and output the running time. The result of the multi-layer direct method is shown in \autoref{fig:direct-experiment}. According to the program output, you can see that the fifth image is about 3.8 meters when the camera moves forward. It can be seen that even if we randomly select points, the direct method can correctly track most of the pixels and estimate the motion of the camera. There is no process of feature extraction, matching or optical flow in between. From the perspective of running time, at 2000 points, the direct method takes 1-2 milliseconds per iteration, so the four-layer gold layer takes about 8 milliseconds. In contrast, the optical flow of 2000 points is about ten milliseconds, and does not include subsequent pose estimation. Therefore, the direct method is usually faster than the traditional feature points and optical flow.

\begin{figure}[!htp]
\centering
\includegraphics[width=1.0\linewidth]{vo2/direct-experiment}
\caption{The experimental result of the direct method. Top left: original image; top right: disparity map corresponding to the original image; bottom left: fifth tracking image; bottom right: tracking result}
\label{fig:direct-experiment}
\end{figure}

Below we briefly explain the iterative process of the direct method. Compared to the feature point method, the direct method relies entirely on optimization to solve the camera pose. As can be seen from the formula \eqref{eq:jacobianofDirect}, the pixel gradient guides the direction of optimization. If you want to get the right optimization results, you must ensure that \textbf{most pixel gradients can direct optimization to the right direction}.

What does it mean? Let's take a look at the optimization algorithm. Assume that for the reference image, we measured a pixel with a gray value of 229. And, since we know its depth, we can infer the position of the space point $P$ (\autoref{fig:directExperiment}~ the gray level measured in $I_1$).

\begin{figure}[!htp]
\centering
\includegraphics[width=.9\linewidth]{vo2/directExperiment}
\caption{A graphical display of one iteration. }
\label{fig:directExperiment}
\end{figure}

At this point we get a new image and we need to estimate its camera pose. This pose is obtained by continuously optimizing the iteration of an initial value. Suppose our initial value is relatively poor. Under this initial value, the pixel gray value after the spatial point $P$ projection is 126. Thus, the error of this pixel is $229-126=103$. In order to reduce this error, we want \textbf{fine-tune the camera's pose to make the pixels brighter}.

How do you know where to fine tune the pixels to be brighter? This requires a partial pixel gradient. We found in the image that we took a step forward along the $u$ axis, where the gray value became 123, which is minus 3. Similarly, a step forward along the $v$ axis reduces the gray value by 18 to 108. Around this pixel, we see that the gradient is $[-3,-18]$. To improve the brightness, we recommend an optimization algorithm to fine tune the camera and move the $P$ image toward \textbf{top left}. In this process, we approximate the grayscale distribution near it with the local gradient of the pixel, but note that the real image is not smooth, so this gradient is not true in the distance.

However, the optimization algorithm can't just listen to the side of this pixel, but also need to listen to other pixels' suggestions\footnote{ This may be an imprecise anthropomorphic statement, but it helps to understand. }. After listening to the opinions of many pixels, the optimization algorithm chooses a place that is not far from the direction we suggest, and calculates an update amount $\exp ({\bm{\xi}^\wedge } )$. With the update, the image moved from $I_2$ to $I_2'$, and the pixel's projected position also changed to a brighter place. We see that with this update, \textbf{error has become smaller}. In the ideal case, we expect the error to continue to drop and finally converge.

But is this actually the case? Are we really going to go along the gradient direction and get to an optimal value? Note that the gradient of the direct method is determined directly by the image gradient, so we must ensure that the \textbf{following the image gradient, the grayscale error will continue to drop}. However, the image is usually a very strong \textbf{non-convex function}, as shown by \autoref{fig:non-convex}~. In practice, if we proceed along the image gradient, it is easy to continue optimization because the non-convexity (or noise) of the image itself falls into a local minimum. The direct method can only be established when the camera motion is small and the gradient in the image does not have a strong non-convexity.

\begin{figure}[!htp]
\centering
\includegraphics[width=1.0\linewidth]{vo2/nonconvex}
\caption{3D display of an image. The path from one point in the image to another is not necessarily a "straight downhill", but it is often necessary to "over the mountains." This embodies the non-convexity of the image itself. }
\label{fig:non-convex}
\end{figure}

In the routine, we only calculated the difference of a single pixel, and this difference is directly subtracted from the grayscale. However, a single pixel is not discriminating, and there are likely to be many pixels around it and its brightness is similar. Therefore, we sometimes use small image patches and use more complex measure of difference, such as Normalized Cross Correlation (NCC). For the sake of simplicity, the routine uses the sum of the squares of the errors to maintain consistency with the derivation.

\subsection{Direct method advantages and disadvantages summary}
Finally, we summarize the advantages and disadvantages of the direct method. In general, its advantages are as follows:

\begin{itemize}
\item can save the time to calculate feature points and descriptors.
\item only requires a pixel gradient, no feature points are required. Therefore, the direct method can be used in the case where the feature is missing. An extreme example is an image with only a gradient. It may not be able to extract the corner feature, but it can be estimated directly using the direct method. In the demonstration experiment, we saw that the direct method works well for randomly selected points. This is critical in practice, as it is likely that there are not many corner points available for use in a practical scenario.
\item can build semi-dense or even dense maps, which is not possible with feature points.
\end{itemize}

On the other hand, its shortcomings are also obvious:
\begin{itemize}
\item \textbf{non-convex}. The direct method relies entirely on gradient search to reduce the objective function to calculate the camera pose. The objective function needs to take the gray value of the pixel, and the image is a strongly non-convex function. This makes the optimization algorithm easy to enter very small, and the direct method can only succeed when the movement is very small. In response to this, the introduction of the pyramid can reduce the influence of non-convexity to some extent.
\item \textbf{single pixel has no discrimination}. It's too much like it! So we either calculate the image block or calculate the complex correlation. Since each pixel is inconsistent with the "opinion" of changing camera motion, only a few can obey the majority, replacing the quality by quantity. Therefore, the direct method has a significant decline in performance when the number of points is small. We usually recommend more than 500 points.
\item \textbf{The gray value is constant is a strong assumption}. If the camera is auto-exposure, it will make the image as bright or dark as it adjusts the exposure parameters. This can also happen when the light changes. The feature point method has a certain tolerance to illumination, while the direct method calculates the gray level difference, and the overall gray level change will destroy the gray level invariant hypothesis, which makes the algorithm fail. For this, a practical direct method will simultaneously estimate the camera's exposure parameter \cite{Engel2016} to work even when the exposure time changes.
\end{itemize}

\section*{ Exercises}
\begin{enumerate}
\item In addition to LK optical flow, what other optical flow methods? What are their characteristics?
\item In the image gradient process of this section of the program, we simply find the difference between the grayscales of $u+1$ and $u-1$ divided by 2 as the gradient value in the $u$ direction. What are the disadvantages of this approach? Hint: For features with close distances, the change should be faster; while the farther features change slowly in the image. Can you use this information when seeking gradients?
Can the \item direct method be the same as the optical flow, and propose the concept of "reverse method"? That is, use the gradient of the original image instead of the gradient of the target image?
\item[\optional] Uses Ceres or g2o to implement sparse direct and semi-dense direct methods.
\item Compared to the direct method of RGB-D, the monocular direct method is often more complicated. In addition to the unknown match, the distance of the pixel is also to be estimated, we need to use the pixel depth as an optimization variable in the optimization. Read the article \cite{Engel2013, Engel2014}, can you understand its principles?
\end{enumerate}

