%! Mode :: "TeX: UTF-8"
\chapter{loopback detection}
\begin{mdframed}
\textbf{main target}
\begin{enumerate}[labelindent = 0em, leftmargin = 1.5em]
\item Understand the need for loopback detection.
\item Master the appearance-based loopback detection based on the bag of words.
\item Through the experiments of DBoW3, learn the practical use of the bag of words model.
\end{enumerate}
\end{mdframed}

In this lecture, we introduce another major module in SLAM: loopback detection. We know that the main purpose of the SLAM main body (front-end, back-end) is to estimate camera movement, and the loop detection module, whether it is on the target or the method, is significantly different from the content described above, so it is generally considered as an independent module . We will introduce the way of detecting loops in mainstream visual SLAM: bag-of-words model, and through the program experiments on the DBoW library, make readers understand more intuitively.

\newpage
\includepdf{resources/other/ch12.pdf}

\newpage

\section{Loop Detection Overview}
\subsection{Meaning of loopback detection}
We have already introduced the front-end and back-end: the front-end provides feature point extraction and trajectory, the initial value of the map, and the back-end is responsible for optimizing all of this data. However, if only key frames in adjacent time are considered like VO, then the errors generated before will inevitably accumulate to the next moment, so that the whole SLAM will appear \textbf{cumulative error}, and the result of long-term estimation will not Reliably, or rather, we can't build \textbf{globally consistent} tracks and maps.

To give a simple example: In the mapping phase of automatic driving, we usually specify the collection vehicle to make a number of turns in a given area to cover all the collection range. Suppose we extract features on the front end, then ignore the feature points, and use Pose Graph on the back end to optimize the entire trajectory, as shown in \autoref{fig: drift} (a) ~. Because the front end only gives local constraints between poses, for example, it may be $\bm{x} _1-\bm{x} _2, \bm{x} _2- \bm{x} _3 $, and so on. However, due to errors in the estimation of $\bm{x} _1 $, and $\bm{x} _2 $is determined based on $\bm{x} _1 $, $\bm{x} _3 $is again determined by $\bm{x} _2 $. By analogy, the errors will be accumulated, so that the results of the back-end optimization are slowly inaccurate as shown in \autoref{fig: drift} ~ (b). In this application scenario, we should use loopback information to determine that the same data should be collected each time it passes the same place.

\begin{figure}[! htp]
\centering
\includegraphics[width = 0.8 \textwidth]{loopclosure/illustrate-loop.pdf}
\caption{Drift illustration. (A) True trajectory; (b) Because the front end only gives estimates between adjacent frames, the optimized Pose Graph drifts; (c) Adding loop detection Pose Graph can eliminate the cumulative error. }
\label{fig: drift}
\end{figure}

Although the back end can estimate the maximum a posteriori error, the so-called "good model cannot hold bad data". When there is only adjacent key frame data, we can do a lot of things and we cannot eliminate the accumulated error. However, the loopback detection module can give some constraints on \textbf{more time apart} besides adjacent frames: for example, the bits between $\bm{x} _1- \bm{x} _{100} $Posture change. Why are there constraints between them? This is because we noticed that the camera \textbf{passed the same place} and \textbf{collected similar data}. The key to loop detection is how to effectively detect \textbf{the camera passes through the same place}. If we can successfully detect this, we can provide more valid data for the back-end Pose Graph, so that it can get a better estimate, especially a \textbf{globally consistent} estimate. Since Pose Graph can be regarded as a mass-spring system, loop detection is equivalent to adding an additional spring to the image, which improves the stability of the system. The reader can also intuitively imagine that the loopback edge "pulls" the edge with accumulated errors to the correct position-if the loopback itself is correct.

Loopback detection is significant for SLAM systems. It is related to the accuracy of our estimated trajectory and map at \textbf{under long time}. On the other hand, since loopback detection provides the association of current data with all historical data, we can also use loopback detection for \textbf{relocation}. Relocation is more useful. For example, if we recorded a trajectory and built a map in advance for a scene, then we can always follow this trajectory to navigate in that scene, and relocation can help us determine our position on this trajectory. Therefore, the improvement of the accuracy and robustness of the entire SLAM system by loopback detection is very obvious. Even sometimes, we call a system with only front-end and local back-end as VO, and a system with loopback detection and global back-end as SLAM.

\subsection{method}
Let us consider how to implement loopback detection. In fact, there are several different ways to look at this problem, including theoretical and engineering.

The easiest way is to do a feature match on any two images and determine which two images are related according to the number of correct matches-this is indeed a simple and effective idea. The disadvantage is that we blindly assume that "any two images may have loops", so that the number to be detected is too large: for $N $possible loops, we want to detect $C_N ^ 2 $as many times, this The complexity is $O (N ^ 2) $, which grows too fast as the trajectory becomes longer, which is not practical in most real-time systems. Another simple way is to randomly extract historical data and perform loopback detection. For example, randomly select 5 frames among the $n $frames to compare with the current frame. This method can maintain a constant amount of computing time, but when this blind heuristic method increases the number of frames $N $, the probability of drawing loops decreases significantly, making the detection efficiency not high.

The simple ideas mentioned above are too rough. Although random detection is indeed useful in some implementations \textsuperscript{\cite{Endres2014}}, we at least hope that there is a "\textbf{where loopbacks can occur}" prediction, so that it can be detected less blindly. This approach is broadly divided into two ideas: Odometry based, or Appearance based. Based on the geometric relationship, it means that when we find that the current camera moves near a certain position before, check whether they have a loopback relationship \textsuperscript{\cite{Hahnel2003}}-this is naturally an intuitive idea, but due to accumulation The existence of the error, we often can not correctly find the fact that "moved to a certain location before", the loop detection is also impossible to talk about. Therefore, this approach is logically problematic because the goal of loop detection is to discover the fact that the camera is back to its previous position, thereby eliminating the cumulative error. The geometry-based approach assumes that the "camera returns to near its previous position" so that loops can be detected. This is suspected to have a negative effect, so it cannot work when the cumulative error is large \textsuperscript{\cite{Beeson2010}}.

Another method is based on appearance. It has nothing to do with the estimation of the front end and the back end, and only determines the loop detection relationship based on the similarity of the two images. This approach gets rid of the accumulated error and makes the loopback detection module a relatively independent module in the SLAM system (of course, the front end can provide it with characteristic points). Since it was proposed at the beginning of the 21st century, the appearance-based loop detection method can effectively work in different scenarios, has become the mainstream method in visual SLAM, and has been applied to actual systems. , \cite{Mur-Artal2015}.

In addition, from the engineering perspective, we can also propose some solutions to loop detection. For example, outdoor unmanned vehicles are often equipped with GPS, which can provide global location information. GPS information can be used to easily determine whether a car has returned to a certain passing point, but such methods are not very useful indoors.

In the appearance-based loop detection algorithm, the core problem is \textbf{how to calculate the similarity between images}. For example, for image $\bm{A} $and image $\bm{B} $, we will design a method to calculate the similarity score between them: $s (\bm{A}, \bm{B }) $. Of course, this score will take a value within a certain interval. When it is greater than a certain amount, we think that a loop occurs. Readers may have questions: Is it difficult to calculate the similarity between two images? For example, intuitively, images can be represented as matrices, so what if we just subtract two images and then take some norm?
\clearpage
\begin{equation}
s(\bm{A}, \bm{B}) = \| \bm{A}-\bm{B} \|.
\end{equation}

Why don't we do this?

\begin{enumerate}
\item First of all, as mentioned earlier, pixel grayscale is an unstable measurement that is heavily affected by ambient light and camera exposure. Assuming the camera is not moving and we turn on a light, the image will be brighter overall. In this way, even for the same data, we will get a large difference.
\item On the other hand, when the camera's viewing angle changes slightly, even if the brightness of each object does not change, their pixels will shift in the image, causing a large difference.
\end{enumerate}

Due to the existence of these two situations, in practice, even for very similar images, $\bm{A}-\bm{B} $will often get a (not realistic) large value. So we say that this function \textbf{cannot reflect the similarity relationship between images well}. This involves a definition of "good" and "bad". We have to ask, what kind of function can better reflect the similarity relationship, and what kind of function is not good enough? From here, two concepts, \textbf{Perceptual Aliasing} and \textbf{Perceptual Variability} can be derived. Now let's discuss it in more detail.

\subsection{Accuracy and recall}
From a human perspective, (at least we think of it) we are able to perceive the fact that "the two images are similar" or "the two photos were taken from the same place" with high accuracy, but due to The working principle of the human brain is not yet known, and we cannot clearly describe how we accomplish this. From a procedural point of view, we hope that procedural algorithms can make judgments that are consistent with humans or with facts. When we think, or in fact, that two images are taken from the same place, then the loop detection algorithm should also give a "this is a loop" result. Conversely, if we feel, or in fact, that the two images were taken from different places, the program should also give a "this is not a loopback" judgment. \footnote{Readers with a background in machine learning should appreciate how similar this passage is to machine learning. Are you already thinking about how to train the network? } Of course, the judgment of the program is not always consistent with our human thoughts, so there may be 4 cases in \autoref{table: loopclosure} ~:

\begin{table}[! htp]
\centering
\caption{Result classification of loopback detection}
\label{table: loopclosure}
\begin{tabu}{c | c | c}
\toprule
Algorithm $\backslash $fact & is loopback & is not loopback \\
\midrule
Yes Loopback & True Positive & False Positive \\
Not loopback & False Negative & True Negative \\
\bottomrule
\end{tabu}
\end{table}

The negative/positive argument here is borrowed from the medical term. False Positive is also called Perceptual Bias, and False Negative is called Perceptual Variation (see \autoref{fig: FPandFN}). For the convenience of writing, the abbreviation TP stands for True Positive, and the rest is analogized. Because we want the algorithm to be consistent with human judgments, we want TP and TN to be as high as possible, and FP and FN to be as low as possible. Therefore, for a certain algorithm, we can count the occurrences of TP, TN, FP, and FN on a certain data set, and calculate two statistics: \textbf{accuracy} and \textbf{recall rate} (Precision \& Recall).
\begin{equation}
\mathrm{Precision} = \mathrm{TP}/(\mathrm{TP}+\mathrm{FP}), \quad \mathrm{Recall} = \mathrm{TP}/(\mathrm{TP}+\mathrm{FN}).
\end{equation}

\vspace{-\medskipamount}
\begin{figure}[! htp]
\centering
\includegraphics[width = 0.9 \textwidth]{loopclosure/FPandFN}
\caption{Examples of false positives and false negatives. The left side is a false positive, the two images look similar, but not the same corridor; the right side is a false negative. Due to changes in lighting, photos at different times in the same place look very different. }
\label{fig: FPandFN}
\end{figure}

In the literal sense of the formula, the accuracy rate describes the probability that all loops extracted by the algorithm are indeed true loops. The recall rate is the probability of being correctly detected in all real loops. Why take these two statistics? Because they are somewhat representative and usually a pair of \textbf{contradictions}.

An algorithm often has many setting parameters. For example, when a certain threshold is raised, the algorithm may become more "strict"-it detects fewer loops and improves accuracy. But at the same time, because the number of detections has decreased, many places that were originally loopbacks may be missed, resulting in a decline in the recall rate. Conversely, if we choose a more relaxed configuration, the number of detected loops will increase and a higher recall rate will be obtained, but there may be some cases that are not loopbacks, and the accuracy rate will decrease.

In order to evaluate the quality of the algorithm, we will test its $P $and $R $values ​​in various configurations, and then make a Precision-Recall curve (see \autoref{fig: PRCurve}). When the recall is used on the horizontal axis and the accuracy is used on the vertical axis, we will care about the degree of the entire curve deviating to the upper right, the recall rate at 100 \% accuracy rate, or the accuracy rate at 50 \% recall rate, as the evaluation The indicator of the algorithm. Note, however, that apart from some "world-wide" algorithms, we cannot generally say that Algorithm A is better than Algorithm B. We may say that A has a good recall rate when the accuracy rate is high, and B can guarantee a better accuracy rate with a 70% recall rate, and so on.

\begin{figure}[! ht]
\centering
\includegraphics[width = 0.66 \textwidth]{loopclosure/prcurve}
\caption{Example of accuracy-recall curve \textsuperscript{\cite{Gao2015a}}. As the recall rate increases, the detection conditions become relaxed, and the accuracy rate decreases. Good algorithms can still guarantee better accuracy at higher recalls. }
\label{fig: PRCurve}
\end{figure}

It is worth mentioning that in SLAM, we have higher requirements for accuracy, and we are relatively tolerant of recall. Because false positive (the test result is but not actually) loops will add fundamentally wrong edges to the backend Pose Graph, sometimes it will cause the optimization algorithm to give completely wrong results. Imagine if the SLAM program mistakenly regarded all the desks as the same, what would happen to the created graph? You may see that the corridors are not straight, the walls are intertwined, and the entire map is broken. In contrast, the recall rate is lower, at most part of the loopbacks are not detected, and the map may be affected by some cumulative errors-but only one or two loopbacks can completely eliminate them. Therefore, when choosing a loop detection algorithm, we prefer to set the parameters more strictly, or add the \textbf{loop verification} step after detection.

So, back to the previous question, why not use $\bm{A}-\bm{B} $to calculate similarity? We will find that its accuracy and recall are very poor, and there may be a lot of False Positive or False Negative situations, so it is "bad" to do so. So, which method is better?

\section{Word bag model}
Since the method of subtracting two images directly is not good enough, we need a more reliable method. Combining the content of the previous lectures, an intuitive idea is: why not use feature points for loop detection like VO? As with VO, we match the feature points of the two images. As long as the number of matches is greater than a certain value, a loop is considered to have occurred. Further, based on the feature point matching, we can also calculate the motion relationship between the two images. Of course, there are some problems with this approach. For example, feature matching can be time-consuming, and feature descriptions may be unstable when the lighting changes. Let's talk about the bag of words first, and then discuss the implementation details such as data structures.

Bags of words, also known as Bag-of-Words (BoW), are designed to describe an image by "how many features are there in the image". For example, let's say there is one person, a car in one photo, and two people, a dog in another. According to this description, the similarity of the two images can be measured. To be more specific, we need to do the following:

\begin{enumerate}
\item Determines the concepts of “person”, “car”, and “dog” —corresponding to “\textbf{Word}” (Word) in BoW. Many words are put together to form “\textbf{Dictionary}”. .
\item Determines what dictionary-defined concepts appear in an image-we describe the entire image in terms of word occurrences (or histograms). This transforms an image into a vector description.
\item Compares the similarity described in the previous step.
\end{enumerate}

In the example above, we first got a "dictionary" somehow. There are many words recorded in the dictionary, and each word has a certain meaning. For example, "person", "car" and "dog" are words recorded in the dictionary. We might as well write them as $w_1, w_2, w_3 $. Then, for any image A, based on the words they contain, it can be written as
\begin{equation}
A = 1 \cdot w_1+1\cdot w_2 + 0 \cdot w_3.
\end{equation}

The dictionary is fixed, so as long as the vector $[1,1,0] ^ \mathrm{T} $can express the meaning of $A $. With a dictionary and words, only one vector is needed to describe the entire image. This vector describes the information of "whether the image contains certain features", which is more stable than the simple gray value. And because the description vector says "\textbf{whether it appears}", regardless of their "\textbf{where it appears}", it has nothing to do with the spatial position and order of the objects. Therefore, when the camera undergoes a small amount of motion, as long as the Still appearing in the field of view, we still guarantee that the description vector will not change \footnote{Although this property sometimes brings some problems, for example, is the face with eyes under the mouth still a human face? }. Based on this feature, we call it Bag-of-Words rather than List-of-Words, and emphasize the presence or absence of Words, regardless of their order. Therefore, it can be said that a dictionary is similar to a collection of words.

Going back to the example above, for the same reason, $[2,0,1] ^ \mathrm{T} $can describe the image $B $. If you only consider "whether it appears" without considering the quantity, it can also be $[1,0,1] ^ \mathrm{T} $. At this time, this vector is binary. Therefore, based on these two vectors, a certain calculation method is designed to determine the similarity between the images. Of course, if there are still some different ways to calculate the difference between two vectors, such as $\bm{a}, \bm{b} \in \mathbb{R} ^ W $, you can calculate:
\begin{equation}
s\left( {\bm{a},\bm{b}} \right) = 1 - \frac{1}{W}\left\| {\bm{a} - \bm{b}} \right\|_1.
\end{equation}

The norm is $L_1 $norm, which is the sum of the absolute values ​​of the elements. Note that when the two vectors are exactly the same, we will get 1; when they are exactly the opposite (where $\bm{a} $is 0, $\bm{b} $is 1), we get 0. This defines the similarity between the two description vectors, and thus the degree of similarity between the images.

What's the next question?

\begin{enumerate}
\item Although we know the definition of the dictionary, how does it come from?
\item If we can calculate the similarity score between two images, is it enough to judge the loopback?
\end{enumerate}

So next, we first introduce how to generate a dictionary, and then how to use the dictionary to actually calculate the similarity between two images.

\section{dictionary}
\subsection{Dictionary structure}
According to the previous introduction, the dictionary is composed of many words, and each word represents a concept. A word is different from a single feature point. It is not extracted from a single image, but a combination of certain types of features. So, the dictionary generation problem is similar to a \textbf{Clustering} problem.

The clustering problem is particularly common in Unsupervised ML, which is used to let the machine find the rules in the data by itself. BoW's dictionary generation problem is also one of them. First, suppose we have extracted feature points from a large number of images, such as $N $. Now, we want to find a dictionary with $k $words. Each word can be regarded as a set of local neighboring feature points. What should we do? This can be solved with the classic K-means (K-means) algorithm \textsuperscript{\cite{Lloyd1982}}.

K-means is a very simple and effective method, so it is widely used in unsupervised learning. The principle is briefly introduced below. To put it simply, when there are $N $data and want to be classified into $k $categories, then using K-means to do it mainly includes the following steps:
\begin{mdframed}
\begin{enumerate}
\item Randomly selects $k $center points: $c_1, \cdots, c_k $.
\item For each sample, calculate the distance between it and each center point, and take the smallest one as its classification.
\item Recalculates the center point of each class.
\item If each center point changes little, the algorithm converges and exits; otherwise returns to step 2.
\end{enumerate}
\end{mdframed}

K-means' approach is simple and simple and effective, but there are some problems, such as the need to specify the number of clusters, randomly select the center point so that the results of each cluster are different, and some efficiency problems. Later, researchers also developed algorithms such as hierarchical clustering and K-means ++ \textsuperscript{\cite{Arthur2007}} to make up for its shortcomings, but this is all later, we will not discuss them in detail. In short, according to K-means, we can cluster a large number of feature points that have been extracted into a dictionary containing $k $words. The question now becomes how to find the corresponding word in the dictionary based on a feature point in the image.

There is still a simple idea: just compare with each word and take the one that is most similar-this is of course a simple and effective way. However, given the generality of a dictionary \footnote{Would you call a page of paper with only ten words a dictionary? I believe that the dictionary in most people's minds is quite heavy. }, We usually use a large-scale dictionary to ensure that image features in the current use environment have appeared in the dictionary, or at least have similar expressions. If you don't think it is troublesome to compare ten words one by one, then what about ten thousand words? What about 100,000?

Perhaps the reader has learned the data structure, this $O (n) $lookup algorithm is obviously not what we want. If the dictionary is sorted, then binary search can obviously improve search efficiency and achieve logarithmic level of complexity. In practice, we may use more complex data structures, such as Chou-Liu tree \textsuperscript{\cite{Chow1968}} in Fabmap \textsuperscript{\cite{Cummins2008, Cummins2010, Cummins2011}}. But we don't want to write this book as a collection of complex details, so we introduce another simple and practical tree structure \textsuperscript{\cite{Galvez-Lopez2012}}.

In the literature \cite{Galvez-Lopez2012}, a $k $fork tree is used to express the dictionary. The idea is simple (as shown in \autoref{fig: lp-dict}), similar to hierarchical clustering, and is a direct extension of k-means. Suppose we have $N $feature points, and we want to build a tree with a depth of $d $and a fork of $k $each time. The method is as follows And depth, this might remind you of kd-tree \textsuperscript{\cite{Bentley1975}}. The author thinks that although the approaches are different, the meanings they express are really the same.:

\begin{mdframed}
\begin{enumerate}
\item At the root node, k-means is used to aggregate all samples into the $k $class (in practice, k-means ++ is used to ensure cluster uniformity). This resulted in the first layer.
\item For each node in the first layer, the samples belonging to the node are regrouped into the $k $class to obtain the next layer.
\item and so on, and finally get the leaf layer. The leaf layer is called Words.
\end{enumerate}
\end{mdframed}

\begin{figure}[! ht]
\centering
\includegraphics[width = 0.9 \textwidth]{loopclosure/dict}
\caption{K fork tree dictionary diagram. When training the dictionary, K-means clustering is used layer by layer. When searching for words based on known features, you can also compare them layer by layer to find the corresponding words. }
\label{fig: lp-dict}
\end{figure}

In fact, we ended up building words at the leaf level, and the intermediate nodes in the tree structure were only used for quick lookups. Such a $k $branch with a depth of $d $can hold $k ^ d $words. On the other hand, when looking for a word corresponding to a given feature, you only need to compare it with the clustering center of each intermediate node (a total of $d $times) to find the last word, ensuring a logarithmic level Search efficiency.

\subsection{Practice: Creating a dictionary}
Now that we've covered dictionary generation, let's actually demonstrate it. The previous VO part used a lot of ORB feature descriptions, so here we will demonstrate how to generate and use ORB dictionaries.

In this experiment, we select 10 images in the TUM dataset (located in slambook2/ch11/data, as shown in \autoref{fig: lp-data}), which are from a set of actual camera motion trajectories. It can be seen that the first image and the last image are obviously taken from the same place. Now we need to see if the program can detect this. According to the bag of words model, we first generate a dictionary corresponding to these ten images.

\begin{figure}[! htp]
\centering
\includegraphics[width = 1.0 \textwidth]{loopclosure/data}
\caption{Demonstrate the 10 images used in the experiment, collected from trajectories at different times. }
\label{fig: lp-data}
\end{figure}

It needs to be stated that the dictionary is often generated from a larger data set when the actual BoW is used, and it is best to come from a place similar to the target environment. We usually use larger-scale dictionaries-the larger the dictionary, the richer the number of words, the easier it is to find the words corresponding to the current image, but it cannot be larger than our computing power and memory. I don't plan to store a large dictionary file on GitHub, so we will temporarily train a small dictionary from ten images. If you want to pursue better results, you should download more data and train a larger dictionary so that the program will be practical. You can also use a dictionary trained by others, but please note that the dictionary uses the same type of features.

Let's start training the dictionary. First, please install the BoW library used by this program. We use DBoW3 \footnote{The main reason for choosing it is that it has good compatibility with OpenCV3, and it is easy to get started with compilation and use.}: \url{https://github.com/rmsalinas/DBow3}. Readers can also find it from the 3rdparty folder of the code in this book, it is a cmake project, please compile and install it according to the cmake process.

Next consider the training dictionary:

\begin{lstlisting}[language=c++,caption=slambook2/ch11/feature\_training.cpp]
int main(int argc, char **argv) {
	// read the image 
	cout << "reading images... " << endl;
	vector<Mat> images;
	for (int i = 0; i < 10; i++) {
		string path = "./data/" + to_string(i + 1) + ".png";
		images.push_back(imread(path));
	}
	// detect ORB features
	cout << "detecting ORB features ... " << endl;
	Ptr<Feature2D> detector = ORB::create();
	vector<Mat> descriptors;
	for (Mat &image:images) {
		vector<KeyPoint> keypoints;
		Mat descriptor;
		detector->detectAndCompute(image, Mat(), keypoints, descriptor);
		descriptors.push_back(descriptor);
	}
	
	// create vocabulary 
	cout << "creating vocabulary ... " << endl;
	DBoW3::Vocabulary vocab;
	vocab.create(descriptors);
	cout << "vocabulary info: " << vocab << endl;
	vocab.save("vocabulary.yml.gz");
	cout << "done" << endl;
	
	return 0;
}
\end{lstlisting}
Using DBoW3 is very easy. We extract the ORB features of the 10 target images and store them in the vector container, and then call the dictionary generation interface of DBoW3. In the constructor of the DBoW3 :: Vocabulary object, we can specify the number of branches and the depth of the tree, but the default constructor is used here, which is $k = 10, d = 5 $. This is a small dictionary with a maximum capacity of 100,000 words. For image features, we also use the default parameters, which are 500 feature points per image. Finally, we store the dictionary as a compressed file.

Run this program, you will see the following dictionary information output:

\begin{lstlisting}
$build/feature_training
reading images...
detecting ORB features ...
creating vocabulary ...
vocabulary info: Vocabulary: k = 10, L = 5, Weighting = tf-idf, Scoring = L1-norm, Number of words = 4983
done
\end{lstlisting}

We see that the number of branches $k $is 10, and the depth $L $is 5 \footnote{here $L $is the $d $mentioned earlier. }, The number of words is 4983, and the maximum capacity is not reached. But what are the remaining Weighting and Scoring? Literally, Weighting is weighting, Scoring seems to be referring to ratings, but how are ratings calculated?

\section{Similarity calculation}
\subsection{Theoretical section}
Let us discuss the problem of similarity calculation. After having a dictionary, given any feature $f_i $, as long as it is searched layer by layer in the dictionary tree, the corresponding word $w_j $can be found at the end-when the dictionary is large enough, we can think of $f_i $and $w_j $comes from the same type of object (although there is no theoretical guarantee, it is only said in the sense of clustering). Then, assuming that $N $features are extracted from an image, and after finding the words corresponding to these $N $features, we are equivalent to having the distribution of the image in the word list, or a histogram. Intuitively (or ideally), it is equivalent to saying "there is a person and a car in this picture". According to Bag-of-Words, this might be considered a Bag.

Note that in this approach, we treat all words equally-there is, there is, there is no. Is this good? Considering that different words are not the same in terms of importance. For example, words such as "yes" and "yes" may appear in many sentences, and we cannot judge the type of sentence based on them; but if there are words like "document" and "football", it will be of great help to judge sentences Some, it can be said that they provide more information. So to sum up, we want to evaluate the distinctiveness or importance of words and give them different weights for better results.

TF-IDF (Term Frequency–Inverse Document Frequency) \textsuperscript{\cite{Sivic2003, Robertson2004}}, or translation frequency−inverse document frequency \footnote{I personally think that TF-IDF is easier to call, so I will use English abbreviations in the following Not Chinese translation. }, A weighting method commonly used in text retrieval, is also used in BoW models. The idea of ​​the TF part is that a word often appears in an image, and its discrimination is high. On the other hand, the idea of ​​IDF is that the lower the frequency of a word in a dictionary, the higher the degree of discrimination when classifying images.

We can calculate the IDF when building a dictionary: Count the ratio of the number of features in a leaf node $w_i $to the number of all features as the IDF part. Suppose the number of all features is $n $and the number of $w_i $is $n_i $, then the IDF of the word is
\begin{equation}
\mathrm{IDF}_i = \log \frac{n}{n_i}.
\end{equation}

On the other hand, the TF part refers to how often a feature appears in a single image. Suppose the word $w_i $appears $n_i $times in the image $A $, and the total number of words appears $n $, then TF is
\begin{equation}
\mathrm{TF}_i = \frac{n_i}{n}.
\end{equation}

So, the weight of $w_i $is equal to the product of TF times IDF:
\begin{equation}
\eta_i = \mathrm{TF}_i \times \mathrm{IDF}_i.
\end{equation}

After considering the weight, for an image $A $, its feature points can correspond to many words, which make up its Bag-of-Words:
\begin{equation}
A = \left\{ (w_1, \eta_1), (w_2, \eta_2), \ldots, (w_N, \eta_N)  \right\} \buildrel \Delta \over = \bm{v}_A.
\end{equation}

Since similar features may fall into the same class, there will be a lot of zeros in the actual $\bm{v} _A $. Anyway, with the bag of words we describe an image $A $with a single vector $\bm{v} _A $. This vector $\bm{v} _A $is a sparse vector. Its non-zero parts indicate which words are contained in the image $A $, and the value of these parts is the value of TF-IDF.

The next question is: given $\bm{v} _A $and $\bm{v} _B $, how do you calculate their difference? This problem is the same as the norm definition. There are several solutions, such as the $L_1 $norm form mentioned in the document \cite{Nister2006}:
\begin{equation}
s\left( {{\bm{v}_A} - {\bm{v}_B}} \right) = 2\sum\limits_{i = 1}^N {\left| {{\bm{v}_{Ai}}} \right| + \left| {{\bm{v}_{Bi}}} \right| - \left| {{\bm{v}_{Ai}} - {\bm{v}_{Bi}}} \right|}.
\end{equation}

Of course, there are many other ways waiting for you to explore, here we only give an example as a demonstration.至此，我们已说明了如何通过词袋模型来计算任意图像间的相似度。下面通过程序实际演练一下。

\subsection{实践：相似度的计算}
上一节的实践部分中，我们已对十幅图像生成了字典。这次我们使用此字典生成Bag-of-Words并比较它们的差异，看看与实际有什么不同。

\begin{lstlisting}[language=c++,caption=slambook/ch12/loop\_closure.cpp]
int main(int argc, char **argv) {
	// read the images and database  
	cout << "reading database" << endl;
	DBoW3::Vocabulary vocab("./vocabulary.yml.gz");
	// DBoW3::Vocabulary vocab("./vocab_larger.yml.gz");  // use large vocab if you want: 
	if (vocab.empty()) {
		cerr << "Vocabulary does not exist." << endl;
		return 1;
	}
	cout << "reading images... " << endl;
	vector<Mat> images;
	for (int i = 0; i < 10; i++) {
		string path = "./data/" + to_string(i + 1) + ".png";
		images.push_back(imread(path));
	}
	
	// NOTE: in this case we are comparing images with a vocabulary generated by themselves, this may lead to overfit.
	// detect ORB features
	cout << "detecting ORB features ... " << endl;
	Ptr<Feature2D> detector = ORB::create();
	vector<Mat> descriptors;
	for (Mat &image:images) {
		vector<KeyPoint> keypoints;
		Mat descriptor;
		detector->detectAndCompute(image, Mat(), keypoints, descriptor);
		descriptors.push_back(descriptor);
	}
	
	// we can compare the images directly or we can compare one image to a database 
	// images :
	cout << "comparing images with images " << endl;
	for (int i = 0; i < images.size(); i++) {
		DBoW3::BowVector v1;
		vocab.transform(descriptors[i], v1);
		for (int j = i; j < images.size(); j++) {
			DBoW3::BowVector v2;
			vocab.transform(descriptors[j], v2);
			double score = vocab.score(v1, v2);
			cout << "image " << i << " vs image " << j << " : " << score << endl;
		}
		cout << endl;
	}
	
	// or with database 
	cout << "comparing images with database " << endl;
	DBoW3::Database db(vocab, false, 0);
	for (int i = 0; i < descriptors.size(); i++)
	db.add(descriptors[i]);
	cout << "database info: " << db << endl;
	for (int i = 0; i < descriptors.size(); i++) {
		DBoW3::QueryResults ret;
		db.query(descriptors[i], ret, 4);      // max result=4
		cout << "searching for image " << i << " returns " << ret << endl << endl;
	}
	cout << "done." << endl;
}
\end{lstlisting}

本程序演示了两种比对方式：图像之间的直接比较，以及图像与数据库之间的比较——尽管它们是大同小异的。此外，我们输出了每幅图像对应的Bag-of-Words描述向量，读者可以从输出数据中看到它们。

\begin{lstlisting}[language=sh,caption=终端输出：]
$build/feature_training
reading database
reading images... 
detecting ORB features ... 
comparing images with images 
desp 0 size: 500
transform image 0 into BoW vector: size = 455
key value pair = <1, 0.00155622>, <3, 0.00222645>, <12, 0.00222645>, <13, 0.00222645>, <14, 0.00222645>, <22, 0.00222645>, <33, 0.00222645>, <37, 0.00155622>, <38, 0.00222645>, <39, 0.00222645>, <43, 0.00222645>, <57, 0.00155622> ......
\end{lstlisting}

It can be seen that the BoW description vector contains the ID and weight of each word, and they form the entire sparse vector. When we compare two vectors, DBoW3 will calculate a score for us. The calculation method is defined by the previous dictionary construction:

\begin{lstlisting}[language = sh, caption = Terminal output:]
image 0 vs image 0 : 1
image 0 vs image 1 : 0.0234552
image 0 vs image 2 : 0.0225237
image 0 vs image 3 : 0.0254611
image 0 vs image 4 : 0.0253451
image 0 vs image 5 : 0.0272257
image 0 vs image 6 : 0.0217745
image 0 vs image 7 : 0.0231948
image 0 vs image 8 : 0.0311284
image 0 vs image 9 : 0.0525447
\end{lstlisting}

When querying the database, DBoW sorted the above scores and gave the most similar results:
\begin{lstlisting}[language=sh,caption=终端输出：]
searching for image 0 returns 4 results:
<EntryId: 0, Score: 1>
<EntryId: 9, Score: 0.0525447>
<EntryId: 8, Score: 0.0311284>
<EntryId: 5, Score: 0.0272257>

searching for image 1 returns 4 results:
<EntryId: 1, Score: 1>
<EntryId: 2, Score: 0.0339641>
<EntryId: 8, Score: 0.0299387>
<EntryId: 3, Score: 0.0256668>

searching for image 2 returns 4 results:
<EntryId: 2, Score: 1>
<EntryId: 7, Score: 0.036092>
<EntryId: 9, Score: 0.0348702>
<EntryId: 1, Score: 0.0339641>

searching for image 3 returns 4 results:
<EntryId: 3, Score: 1>
<EntryId: 9, Score: 0.0357317>
<EntryId: 8, Score: 0.0278496>
<EntryId: 5, Score: 0.0270168>

searching for image 4 returns 4 results:
<EntryId: 4, Score: 1>
<EntryId: 5, Score: 0.0493492>
<EntryId: 0, Score: 0.0253451>
<EntryId: 6, Score: 0.0253017>

searching for image 5 returns 4 results:
<EntryId: 5, Score: 1>
<EntryId: 4, Score: 0.0493492>
<EntryId: 9, Score: 0.028996>
<EntryId: 6, Score: 0.0277584>

searching for image 6 returns 4 results:
<EntryId: 6, Score: 1>
<EntryId: 8, Score: 0.0306241>
<EntryId: 5, Score: 0.0277584>
<EntryId: 3, Score: 0.0267135>

searching for image 7 returns 4 results:
<EntryId: 7, Score: 1>
<EntryId: 2, Score: 0.036092>
<EntryId: 1, Score: 0.0239091>
<EntryId: 0, Score: 0.0231948>

searching for image 8 returns 4 results:
<EntryId: 8, Score: 1>
<EntryId: 9, Score: 0.0329149>
<EntryId: 0, Score: 0.0311284>
<EntryId: 6, Score: 0.0306241>

searching for image 9 returns 4 results:
<EntryId: 9, Score: 1>
<EntryId: 0, Score: 0.0525447>
<EntryId: 3, Score: 0.0357317>
<EntryId: 2, Score: 0.0348702>
\end{lstlisting}

The reader can look at all the output and see how much different images score from similar images. We see that Figures 1 and 10, which are clearly similar (subscripted as 0 and 9 in C ++, respectively), have a similarity score of about 0.0525; the other images are about 0.02.

In the demonstration experiments in this section, we saw that the similar images 1 and 10 scored significantly higher than other image pairs, but numerically it was not as obvious as we thought. It is said that if the similarity between ourselves and itself is 100%, then we (from a human perspective) think that Figure 1 and Figure 10 also have at least 70 to 80 percent similarity, while other figures may be 20 to 30 percent. However, the experimental results are about 2 \% of irrelevant images and about 5 \% of similar images, which seems less obvious than we think. Is this the result we want to see?

\section{Experimental Analysis and Review}
\subsection{Increase dictionary size}
In the field of machine learning, if the code is not wrong and the results are unsatisfactory, we first suspect "whether the network structure is large enough, the number of layers is deep enough, whether there are enough data samples", etc. This is still due to "good models can't beat bad data "(Partly because of the lack of deeper theoretical analysis). Although we are now studying SLAM, when this happens, we will first wonder: Is the dictionary too small? After all, we are generating a dictionary from ten images, and then calculate the image similarity based on this dictionary.

slambook2/ch11/vocab \_larger.yml.gz is a slightly larger dictionary that we generate-in fact, it is generated for all images of the same data sequence, about 2,900 images. The size of the dictionary is still $k = 10, d = 5 $, which is a maximum of 10,000 words. Readers can use the gen \_vocab \_large.cpp file in the same directory to train the dictionary on their own. Please note that to train a large dictionary, you may need a machine with large memory, and wait patiently for a while. We slightly modified the program in the previous section to use a larger dictionary to detect image similarity:
\begin{lstlisting}[language=sh,caption=终端输出：]
comparing images with database 
database info: Database: Entries = 10, Using direct index = no. Vocabulary: k = 10, L = 5, Weighting = tf-idf, Scoring = L1-norm, Number of words = 99566
searching for image 0 returns 4 results:
<EntryId: 0, Score: 1>
<EntryId: 9, Score: 0.0320906>
<EntryId: 8, Score: 0.0103268>
<EntryId: 4, Score: 0.0066729>

searching for image 1 returns 4 results:
<EntryId: 1, Score: 1>
<EntryId: 2, Score: 0.0238409>
<EntryId: 8, Score: 0.00814409>
<EntryId: 3, Score: 0.00697527>

searching for image 2 returns 4 results:
<EntryId: 2, Score: 1>
<EntryId: 1, Score: 0.0238409>
<EntryId: 5, Score: 0.00897928>
<EntryId: 8, Score: 0.00893477>

searching for image 3 returns 4 results:
<EntryId: 3, Score: 1>
<EntryId: 5, Score: 0.0107005>
<EntryId: 8, Score: 0.00870392>
<EntryId: 6, Score: 0.00720695>

searching for image 4 returns 4 results:
<EntryId: 4, Score: 1>
<EntryId: 6, Score: 0.0069998>
<EntryId: 0, Score: 0.0066729>
<EntryId: 5, Score: 0.0062834>

searching for image 5 returns 4 results:
<EntryId: 5, Score: 1>
<EntryId: 3, Score: 0.0107005>
<EntryId: 2, Score: 0.00897928>
<EntryId: 4, Score: 0.0062834>

searching for image 6 returns 4 results:
<EntryId: 6, Score: 1>
<EntryId: 7, Score: 0.00915307>
<EntryId: 3, Score: 0.00720695>
<EntryId: 4, Score: 0.0069998>

searching for image 7 returns 4 results:
<EntryId: 7, Score: 1>
<EntryId: 6, Score: 0.00915307>
<EntryId: 8, Score: 0.00814517>
<EntryId: 1, Score: 0.00538609>

searching for image 8 returns 4 results:
<EntryId: 8, Score: 1>
<EntryId: 0, Score: 0.0103268>
<EntryId: 2, Score: 0.00893477>
<EntryId: 3, Score: 0.00870392>

searching for image 9 returns 4 results:
<EntryId: 9, Score: 1>
<EntryId: 0, Score: 0.0320906>
<EntryId: 8, Score: 0.00636511>
<EntryId: 1, Score: 0.00587605>
\end{lstlisting}

It can be seen that when the dictionary size increases, the similarity of irrelevant images becomes significantly smaller. For similar images, for example, images 1 and 10, although the scores have dropped slightly, the scores of other images have become more significant. This shows that adding dictionary training samples is beneficial. In the same way, readers can try using a larger dictionary and see how the results change.

\subsection{Handling of similarity scores}
For any two images, we can give a similarity score, but using the absolute size of this score is not necessarily helpful. For example, some environments have inherently similar appearances. For example, offices often have many tables and chairs of the same style; other environments are very different in different places. Considering this situation, we will take a \textbf{transcendental similarity} $s \left (\bm{v} _t, \bm{v} _{t- \Delta t} \right) $, which means The similarity between the keyframe image at a moment and the keyframe at the previous moment. Then, the other scores are normalized with reference to this value:
\begin{equation}
s\left( \bm{v}_t, \bm{v}_{t_j}\right)' = s\left( \bm{v}_t, \bm{v}_{t_j}\right)/s\left( \bm{v}_t, \bm{v}_{t-\Delta t}\right).
\end{equation}

From this perspective, we say: if the similarity between the current frame and a previous key frame exceeds three times the similarity between the current frame and the previous key frame, it is considered that there may be a loopback. This step avoids the introduction of absolute similarity thresholds, allowing the algorithm to adapt to more environments.
\enlargethispage{-5pt}
\subsection{Processing of key frames}
When detecting loops, we must consider the selection of key frames. If the key frames are selected too close, the similarity between the two key frames will be too high, and by comparison, it is not easy to detect loops in historical data. For example, the detection result is often the most similar between the $n $frame, the $n-2 $frame, and the $n-3 $frame. This kind of result seems too ordinary and has little meaning. So in practice, the frames used for loopback detection are preferably sparse, they are not the same as each other, and they can cover the entire environment.

On the other hand, if a loopback is successfully detected, for example, it appears in frames 1 and $n $. Then it is very likely that frames $n + 1 $, $n + 2 $will all form a loop with frame 1. However, confirming that there is a loopback between the first frame and the $n $frame is helpful for trajectory optimization, and the next $n + 1 $frame, $n + 2 $frame will be the first frame The help generated by loopbacks is not so great, because we have used the previous information to eliminate the cumulative error, and more loopbacks will not bring more information. Therefore, we will group "close" loops into one type, so that the algorithm does not repeatedly detect loops of the same type.

\subsection{Validation after detection}
The bag-of-words loop detection algorithm is completely dependent on appearance without using any geometric information, which results in images with similar appearances being easily treated as loops. In addition, since the bag of words does not care about the order of the words, and only cares about the expression of the words, it is more likely to cause perception bias. Therefore, after loopback detection, we usually have a verification step \textsuperscript{\cite{Latif2013, Cadena2012}}.

There are many ways to verify. One is to set up a cache mechanism for loopbacks. It is considered that a single loopback detection is not enough to constitute a good constraint, and a loopback that has been detected for a period of time is considered a correct loopback. This can be seen as a consistency check in time. Another method is spatial consistency detection, that is, feature matching is performed on the two frames detected by the loop, and the camera motion is estimated. Then, put the motion into the previous Pose Graph, and check whether there is a big difference from the previous estimation. In short, the verification part is usually necessary, but how to achieve it is a matter of opinion.

\subsection{Relationship with Machine Learning}
As can be seen from the previous discussion, loop detection is inextricably linked to machine learning. Loopback detection itself is very much like a classification problem. The difference from traditional pattern recognition is that the number of categories in the loop is large, and the samples of each category are small-in extreme cases, when the robot moves, the image changes, and new categories are generated. We can even put Classes are treated as continuous variables rather than discrete variables. Loopback detection, which is equivalent to two images falling into the same class, is rare. From another perspective, loop detection is also equivalent to a study of the "similarity between images" concept. Now that humans can determine whether images are similar, it is very possible for machines to learn such concepts.

From the bag-of-words model, it is an unsupervised machine learning process itself--building a dictionary is equivalent to clustering feature descriptors, and a tree is just a fast-looking data structure for clustered classes. Since it is clustering, combined with the knowledge in machine learning, we can at least ask:

\begin{enumerate}
\item Is it possible to cluster machine learning image features instead of clustering artificially designed features like SURF, ORB?
\item Is there a better way to do clustering than the simpler way of adding tree structure and K-means?
\end{enumerate}

Combined with the current development of machine learning, the learning of binary descriptors and unsupervised clustering are all very promising problems that can be solved in deep learning frameworks. We have also seen the use of machine learning for loop detection. Although the current bag-of-words method is still mainstream, I believe that future deep learning methods are very promising to defeat these artificially designed, "traditional" machine learning methods \textsuperscript{\cite{Gao2015, Gao2015b}}. After all, the bag-of-words method is obviously inferior to neural networks in object recognition, and loop detection is a very similar problem. For example, the improved version of the BoW model, VLAD, has a CNN-based implementation \cite{Arandjelovic2016, AngelinaUy2018}, and there are also some meshes that can be returned from the image to collect the camera pose after training \cite{Kendall2015}, which may become New loop detection algorithm.

\section *{EXERCISE}
\begin{enumerate}
\item Please write a small program to calculate PR curve. It may be easier to use MATLAB or Python because they are good at drawing.
\item To verify the loopback detection algorithm, you need a dataset with manually labeled loopbacks, such as \cite{Cummins2008}. However, it is inconvenient to manually mark loops. We will consider calculating loops based on standard trajectories. That is, if two frames in the track have very similar poses, they are considered loops. Please calculate the loopback in a data set according to the standard trajectory given by the TUM data set. Are these loopback images really similar?
\item Learn the DBoW3 or DBoW2 library, find a few pictures yourself, and see if you can correctly detect loops from them.
\item What are the commonly used measures of similarity scoring?
\item What is the Chow-Liu tree? How is it used for dictionary construction and loop detection?
\item Read the literature \cite{Williams2009}. In addition to the bag of words model, what other methods are used for loop detection?
\end{enumerate}
