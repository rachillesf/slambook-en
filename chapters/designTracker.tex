%!Mode :: "TeX: UTF-8"
\chapter{Practice: Designing the front end}
\begin{mdframed}
\textbf{main target}
\begin{enumerate}[labelindent = 0em, leftmargin = 1.5em]
\item actually designs a visual odometer front end.
\item Understand how the SLAM software framework is built.
\item Understand issues that tend to arise in front-end design and how to fix them.
\end{enumerate}
\end{mdframed}

This lecture is a relatively rare one that is composed of practical parts. We will use the knowledge from the previous two lectures to actually write a visual odometer program. You will manage local robot trajectories and landmarks, and experience how a software framework is composed. During the operation, we will encounter many problems: too fast camera movement, blurred images, mismatches ... will invalidate the algorithm. In order for the program to run stably, we need to deal with all the above situations, which will bring many useful discussions on the implementation of the project.

\newpage
\section{Building VO Framework}
Knowing the principles of bricks and cement does not mean that you can build a great palace.

In my favorite "Minecraft" game, players only have some blocks with different colors and textures. Its nature is extremely simple, and all the player has to do is place these blocks on the open space. Understanding a block is simple, but when you actually pick them up, beginners often can only build simple matchbox houses, and experienced and creative players can use these simple blocks to build houses, gardens, and pavilions. Even the city (\autoref{fig: mcarchitecture} ~) \footnote{The lower left is the author's practice work. The bottom right comes from the Epicwork team work: "Yuanmingyuan". }.

\begin{figure}[!htp]
\centering
\includegraphics[width = 0.9 \linewidth]{designVO/mcarchitecture} \\
\caption{Starting from simple things, gradually build more and more complex but better works. }
\label{fig: mcarchitecture}
\end{figure}

In SLAM, we believe that engineering implementation and understanding of algorithm principles should be at least equally important, and even more emphasis should be placed on how to write practically usable programs. The principles of algorithms are like blocks, we can clearly and clearly discuss their principles and properties, but just understanding a block does not make you build a real building: they require a lot of trial, time, and experience. We Encourage readers to work in a more practical direction-this is often very complicated. Just like in Minecraft, you need to master the structure of various columns, walls, and roofs, the carvings on the walls, and the calculation of the angles of geometric shapes. These are far from as simple as discussing the nature of each block.

The same is true of the specific implementation of SLAM. A practical program will have a lot of engineering design and tricks. It also needs to discuss how to deal with problems after each step. In principle, everyone's implementation of SLAM will be different, and most of the time we can't say which implementation is necessarily the best. However, we usually encounter some common problems: "how to manage map points", "how to deal with mismatches", "how to select key frames", and so on. We hope that readers will have some intuitive feelings about these possible problems-we think this feeling is very important.

\clearpage
Therefore, out of the emphasis on practice, in this chapter we will guide the reader through the process of building a SLAM framework. Just like architecture, we have to discuss trivial but important issues such as column spacing and aspect ratio of the facade. SLAM engineering is complicated. Even if we keep only the core parts, it will take up a lot of space and make this book too verbose. However, please note that although the project after completion is complicated, the intermediate "from simple to complex" process is worthy of detailed discussion and learning value. Therefore, we need to start with a simple data structure, first make a simple visual odometer, and then slowly add some additional features. In other words, we need to show the \textbf{from simple to complex} process to the reader, so that you can understand how a library slowly piles up like a snowman.

The code for this tutorial is in slambook/project. As the development process continues, we will make some changes to the project, so its content will also change. Therefore, we will also keep the intermediate code in the directory, named after the version number, so that readers can view and imitate at any time.

\subsection{determining the program frame}
According to the contents of the first two lectures, we know that there are three types of visual mileometer: monocular, binocular, and RGB-D. Monocular vision is relatively complicated, and RGB-D is the simplest, without initialization and scale problems. Based on the simple and complex guideline, let's start with RGB-D. In order to facilitate the reader's experiments, we will use the data set instead of the actual RGB-D camera (because the reader cannot guarantee a RGB-D camera).

First, let's take a look at how Linux programs are organized. When writing a small-scale library, we usually create some folders and store the source code, header files, documents, test data, configuration files, logs, and other categories in a organized manner. If a library contains a lot of content, we will also break the code into separate small modules for testing. Readers can refer to the organization of OpenCV or g2o to see how a large and medium-sized library is organized. For example, OpenCV has modules such as core, imgproc, and features2d, and each module is responsible for different tasks. g2o has several modules such as core, solvers, and types. But in small programs, we can also put everything together, called the SLAM library.

The SLAM library we are writing now is a small library. The goal is to help readers to integrate the various algorithms used in this book and write their own SLAM programs. Pick a project directory and create the following folders under it to organize the code files:

\begin{enumerate}
\item bin is used to store executable binaries.
\item include/myslam Stores the header files of the SLAM module, mainly .h files. The reason for this is that when you set the include directory to include and reference your own header files, you need to write include \texttt{"} myslam/xxx.h \texttt{"}, which is not easy to be confused with other libraries.
\item src Stores source code files, mainly .cpp files.
\item test Stores test files, which are also .cpp files.
\item lib Holds compiled library files.
\item config stores configuration files.
\item cmake \_modules cmake files for third-party libraries, which are used when using libraries like g2o.
\end{enumerate}

The above is our directory structure, as shown in \autoref{fig: proj} ~. Compared with the scattered main.cpp in each of the previous lectures, this approach seems more organized. Next, we will continue to add new files to these directories, gradually forming a complete program.

\begin{figure}[!htp]
\centering
\includegraphics[width = .9 \linewidth]{designVO/proj.pdf}
\caption{Directory of the project. }
\label{fig: proj}
\end{figure}

\subsection{Determine the basic data structure}
In order to make the program run, we need to design the data unit and the process of the program. This is like the pillars and bricks that make up a house. So, what are the most basic structures in a SLAM program? We abstract the following basic concepts:

\begin{enumerate}
\item \textbf{frame}: A frame is a unit of image captured by the camera. It mainly contains one image (a pair of images in the case of RGB-D). In addition, there are information such as feature points, poses, and internal parameters.
Hell
In visual SLAM we will talk about key-frames. Because the camera collects a lot of data, it is obviously impractical to store all the data. Otherwise, if the camera is left on the table, the memory usage of the program will become higher and higher until it is unacceptable. The common practice is to save some frames that we think are more important, and think that the camera track can be described by these key frames. How to choose key frames is a big problem, and based on engineering experience, there is little theoretical guidance. We will use a key frame selection method in this book, but readers can also consider coming up with new methods on their own.
Hell
\item \textbf{Signpost}: Signpost points are feature points in the image. After camera movement, we can also estimate their 3D position. Usually, the landmark points are placed in a \textbf{map}, and the new frame is matched with the landmark points in the map to estimate the camera pose.
\end{enumerate}

\clearpage
The estimation of the pose and position of the frame is equivalent to a local SLAM problem. In addition, we need some tools to make the program write more smoothly. E.g:

\begin{enumerate}
\item \textbf{configuration file}: During the process of writing a program, you will often encounter various parameters, such as the camera's internal parameters, the number of feature points, the proportion selected during matching, and so on. You can write these numbers in the program, but that is not a good habit. You will often modify these parameters, but you must recompile the program after each modification. As its number increases, it becomes more difficult to modify. Therefore, a better way is to define a configuration file externally, and read the parameter values ​​in the configuration file when the program runs. In this way, you only need to modify the content of the configuration file each time, without having to make any changes to the program itself.
\item \textbf{Coordinate Transformation}: You will often need to perform coordinate transformations between coordinate systems, for example, world coordinates to camera coordinates, camera coordinates to normalized camera coordinates, normalized camera coordinates to pixel coordinates, and so on. It would be more convenient to define a class to put these operations together.
\end{enumerate}

Here we define the concepts of frames and road signs, which are represented by classes in C ++. We try to ensure that a class has separate header and source files, and avoid putting many classes in the same file. Then, put the function declaration in the header file and the implementation in the source file (unless the function is short, it can also be written in the header file). We follow Google's naming conventions and consider writing programs in a way that even beginners can understand. Since our program is biased towards algorithms rather than software engineering, we do not discuss complex class inheritance relationships, interfaces, templates, etc., but focus more on \textbf{the correct implementation of the algorithm and whether it is easy to extend} We will make the data members public, although this should be avoided in C ++ software design. If the reader wants to, you can also change them to private or protected interfaces, and add settings and get interfaces. In a more complex algorithm, we will break it into several steps. For example, feature extraction and matching should be implemented in different functions. In this way, when we want to modify the algorithm flow, we do not need to modify the entire operation flow, Need to adjust the local processing method.

Now let's start writing VO. We set this version as 0.1, indicating that this is the beginning stage. We write a total of 5 classes: Frame is the frame, Camera is the camera model, MapPoint is the feature point/landmark point, Map manages the feature point, and Config provides configuration parameters. Their relationship is shown in \autoref{fig: class0.1} ~. We only write their data members and common methods now, and add them later when more content is used.

\begin{figure}[!ht]
\centering
\includegraphics[width = 1.0 \linewidth]{designVO/class.pdf}
\caption{Schematic diagram of basic class relationships. }
\label{fig: class0.1}
\end{figure}

The Camera class is the simplest, let's implement it first.
\clearpage

\subsection{Camera class}
The Camera class stores the internal and external parameters of the camera, and completes the coordinate transformation between the camera coordinate system, the pixel coordinate system, and the world coordinate system. Of course, in the world coordinate system you need a camera (variable) external parameter, which we pass in as a parameter.

\begin{lstlisting}[language=c++,caption=slambook/project/0.1/include/myslam/camera.h]
#ifndef CAMERA_H
#define CAMERA_H
#include "myslam/common_include.h"
namespace myslam
{
// Pinhole RGB-D camera model
class Camera
{
public:
	typedef std::shared_ptr<Camera> Ptr;
	float   fx_, fy_, cx_, cy_, depth_scale_;  // Camera intrinsics 
	
	Camera();
	Camera ( float fx, float fy, float cx, float cy, float depth_scale=0 ) :
	fx_ ( fx ), fy_ ( fy ), cx_ ( cx ), cy_ ( cy ), depth_scale_ ( depth_scale )
	{}
	
	// coordinate transform: world, camera, pixel
	Vector3d world2camera( const Vector3d& p_w, const SE3& T_c_w );
	Vector3d camera2world( const Vector3d& p_c, const SE3& T_c_w );
	Vector2d camera2pixel( const Vector3d& p_c );
	Vector3d pixel2camera( const Vector2d& p_p, double depth=1 ); 
	Vector3d pixel2world ( const Vector2d& p_p, const SE3& T_c_w, double depth=1 );
	Vector2d world2pixel ( const Vector3d& p_w, const SE3& T_c_w );
};
}
#endif // CAMERA_H
\end{lstlisting}

The description is as follows (from top to bottom):
\begin{enumerate}
\item In this simple example, we give an ifndef macro definition that prevents duplicate references in header files. Without this macro, duplicate definitions of the class would occur when the header file was referenced in two places. Therefore, such a macro is defined in each program header file.
\item We wrap the class definition with the namespace namespace myslam (because it is our own SLAM, the namespace is called myslam). Namespaces can prevent us from accidentally defining functions with the same name in other libraries. Since the macro definitions and namespaces are written once in each file, we will only introduce them here and omit them later.
\item We put some common header files in the common \_include.h file, so we can avoid writing a long list of include each time.
\item We define smart pointers as Camera pointer types, so use Camera :: Ptr when passing parameters in the future.
\item We use Sophus :: SE3 to express the pose of the camera. The Sophus library was introduced in Lie Algebra.
\end{enumerate}

In the source file, give the implementation of the Camera method:
\begin{lstlisting}[language = c ++, caption = slambook/project/0.1/src/camera.cpp]
#include "myslam/camera.h"
namespace myslam
{

Camera::Camera()
{
}

Vector3d Camera::world2camera ( const Vector3d& p_w, const SE3& T_c_w )
{
	return T_c_w*p_w;
}

Vector3d Camera::camera2world ( const Vector3d& p_c, const SE3& T_c_w )
{
	return T_c_w.inverse() *p_c;
}

Vector2d Camera::camera2pixel ( const Vector3d& p_c )
{
	return Vector2d (
		fx_ * p_c ( 0,0 )/p_c ( 2,0 ) + cx_,
		fy_ * p_c ( 1,0 )/p_c ( 2,0 ) + cy_
	);
}

Vector3d Camera::pixel2camera ( const Vector2d& p_p, double depth )
{
	return Vector3d (
		( p_p ( 0,0 )-cx_ ) *depth/fx_,
		( p_p ( 1,0 )-cy_ ) *depth/fy_,
		depth
	);
}

Vector2d Camera::world2pixel ( const Vector3d& p_w, const SE3& T_c_w )
{
	return camera2pixel ( world2camera ( p_w, T_c_w ) );
}

Vector3d Camera::pixel2world ( const Vector2d& p_p, const SE3& T_c_w, double depth )
{
	return camera2world ( pixel2camera ( p_p, depth ), T_c_w );
}
}
\end{lstlisting}

The reader can check whether these methods are consistent with the content of lecture 5. They complete the coordinate transformation between the pixel coordinate system, the camera coordinate system, and the world coordinate system.

\subsection{Frame class}
Let's consider the Frame class. The Frame class is the basic data unit and will be used in many places, but now is the initial design stage, we do not know what may be added in the future. So the Frame class here only provides basic data storage and interfaces. If there are new content later, continue to add it.

\begin{lstlisting}[language=c++,caption=slambook/project/0.1/include/myslam/frame.h]
class Frame
{
public:
	typedef std::shared_ptr<Frame> Ptr;
	unsigned long  id_; // id of this frame
	double time_stamp_; // when it is recorded
	SE3 T_c_w_; // transform from world to camera
	Camera::Ptr camera_; // Pinhole RGB-D Camera model 
	Mat color_, depth_; // color and depth image 

public: // data members 
	Frame();
	Frame( long id, double time_stamp=0, SE3 T_c_w=SE3(), Camera::Ptr camera=nullptr, Mat color=Mat(), Mat depth=Mat() );
	~Frame();

	// factory function
	static Frame::Ptr createFrame(); 
	
	// find the depth in depth map
	double findDepth( const cv::KeyPoint& kp );
	
	// Get Camera Center
	Vector3d getCamCenter() const;
	
	// check if a point is in this frame 
	bool isInFrame( const Vector3d& pt_world );
};
\end{lstlisting}

In Frame, we define the ID, timestamp, pose, camera, and image. These should be the most important information contained in a frame. In the method, we extracted several important methods: creating a frame, finding the depth corresponding to a given point, obtaining the camera's optical center, determining whether a point is in the field of view, and so on. Their implementation is relatively trivial, please refer to frame.cpp for specific implementation of these functions.

\subsection{MapPoint class}
MapPoint represents a waypoint. We will estimate its world coordinates, and will match the feature points extracted from the current frame with the landmark points in the map to estimate the camera's movement, so it also needs to store its corresponding descriptor. In addition, we will record the number of times a point is observed and the number of times it is matched as an indicator of how good it is.

\begin{lstlisting}[language=c++,caption=slambook/project/0.1/include/myslam/mappoint.h]
class MapPoint
{
public:
	typedef shared_ptr<MapPoint> Ptr;
	unsigned long      id_; // ID
	Vector3d    pos_;       // Position in world
	Vector3d    norm_;      // Normal of viewing direction 
	Mat         descriptor_; // Descriptor for matching 
	int         observed_times_;    // being observed by feature matching algo.
	int         correct_times_;     // being an inliner in pose estimation

	MapPoint();
	MapPoint( long id, Vector3d position, Vector3d norm );

	// factory function
	static MapPoint::Ptr createMapPoint();
};
\end{lstlisting}

Similarly, the reader can browse src/map.cpp to see its implementation. So far we have only considered the initialization of these data members.

\subsection{Map class}
The Map class manages all the landmarks and is responsible for adding new landmarks and deleting bad landmarks. The matching process of VO only needs to deal with Map. Of course Map will also have many operations, but at this stage we only define the main data structures.

\begin{lstlisting}[language = c ++, caption = slambook/project/0.1/include/myslam/map.h]
class Map
{
public:
	typedef shared_ptr<Map> Ptr;
	unordered_map<unsigned long, MapPoint::Ptr >  map_points_;        // all landmarks
	unordered_map<unsigned long, Frame::Ptr >     keyframes_;         // all key-frames

	Map() {}
	
	void insertKeyFrame( Frame::Ptr frame );
	void insertMapPoint( MapPoint::Ptr map_point );
};
\end{lstlisting}

The Map class actually stores each key frame and waypoint. It needs both random access and insertion and deletion at any time. Therefore, we use Hash to store it.

\subsection{Config class}

The Config class is responsible for reading parameter files, and can provide parameter values ​​at any time in the program. So we write Config as Singleton. It has only one global object. When we set the parameter file, we create the object and read the parameter file, and then we can access the parameter value anywhere, and finally it is destroyed automatically at the end of the program.

\begin{lstlisting}[language = c ++, caption = slambook/project/0.1/include/myslam/config.h]
class Config
{
private:
	static std::shared_ptr<Config> config_; 
	cv::FileStorage file_;
	
	Config () {} // private constructor makes a singleton
public:
	~Config();  // close the file when deconstructing 
	
	// set a new config file 
	static void setParameterFile( const std::string& filename ); 
	
	// access the parameter values
	template< typename T >
	static T get( const std::string& key )
	{
		return T( Config::config_->file_[key] );
	}
};
\end{lstlisting}

described as follows:
\begin{enumerate}
\item We declare the constructor as private to prevent objects of this class from being created elsewhere. It can only be constructed during setParameterFile. The actual constructed object is a smart pointer for Config: static shared \_ptr \\<Config> config_. The reason for using smart pointers is that they can be destructed automatically, saving us from having to call another function to do the destruction.
\item For file reading, we use the FileStorage class provided by OpenCV. It can read a YAML file and can access any of these fields. Since the actual value of the parameter may be an integer, a floating point number, or a string, we use a template function get to obtain any type of parameter value.
\end{enumerate}

The following is the implementation of Config. Note that we have defined the global pointer for the singleton pattern in this source file:
\begin{lstlisting}[language = c ++, caption = slambook/project/0.1/src/config.cpp]
void Config :: setParameterFile (const std :: string & filename)
{
	if ( config_ == nullptr )
		config_ = shared_ptr<Config>(new Config);
	config_->file_ = cv::FileStorage( filename.c_str(), cv::FileStorage::READ );
	if ( config_->file_.isOpened() == false )
	{
		std::cerr<<"parameter file "<<filename<<" does not exist."<<std::endl;
		config_->file_.release();
		return ;
	}
}
Config::~Config()
{
	if ( file_.isOpened() )
		file_.release();
}
shared_ptr<Config> Config::config_ = nullptr;
\end{lstlisting}

In implementation, we just need to judge whether the parameter file exists. After defining the Config class, we can get the parameters in the parameter file anywhere. For example, when you want to define the camera's focal length $ f_x $, follow these steps:

\begin{enumerate}
\item Add "Camera.fx: 500" to the parameter file.
\item is used in the code:
\begin{lstlisting}[language = c ++]
myslam :: Config :: setParameterFile ("parameter.yaml");
double fx = myslam :: Config :: get <double> ("Camera.fx");
\end{lstlisting}
You will get the value of $ f_x $.
\end{enumerate}
Of course, there is definitely more than one way to implement parameter files. We mainly consider this implementation from the perspective of program development convenience. Of course, the reader can also implement the parameter configuration in a simpler way.

So far, we have defined the basic data structure of the SLAM program and written several basic classes. It's like bricks and cement for building a house. You can call cmake to compile this version 0.1, although it has no substantial features yet. Next, let's consider adding the previously mentioned VO algorithm to the project, and do some tests to adjust the performance of each algorithm. Note that the author deliberately exposes certain design issues, so the implementation you see may not be the best (or good enough).

\section{Basic VO: Feature Extraction and Matching}
Now let's implement VO, first consider the feature point method. Its task is to calculate camera movement and feature point positions based on the input image. Earlier we discussed pose estimation between two or two frames, but we will find that the estimation of two frames alone is not enough. We will cache the feature points into a small map and calculate the position relationship between the current frame and the map. But the procedure will be a bit more complicated, so let's set a small target first, starting from the motion estimation between two or two frames.

\subsection{two-frame visual odometry}
If, as in the previous two lectures, you only care about the motion estimation between two frames, and do not optimize the position of the feature points. Then “stringing” the estimated poses can also get a motion trajectory. This method can be viewed as Pairwise structureless VO, which is the easiest to implement, but the effect is not good. Why not? Let's experience it together. Record the project as version 0.2.

The schematic diagram of VO work between two frames is shown in \autoref{fig: pairwise-VO} ~. In this kind of VO, we have defined two concepts of Reference Frame and Current Frame. Taking the reference frame as the coordinate system, we match the current frame with it and estimate the motion relationship. Assume that the transformation matrix of the reference frame relative to the world coordinates is $ \bm{T} _ \mathrm{rw} $, and the current frame and the world coordinate system are $ \bm{T} _ \mathrm{cw} $. The motion and the transformation matrix of these two frames form a left multiplication relationship:
\[
\bm{T}_\mathrm{cr}, \quad \mathrm{s.t.} \quad \bm{T}_\mathrm{cw} = \bm{T}_\mathrm{cr} \bm{T}_\mathrm{rw}.
\]

From $ t-1 $ to $ t $, we use $ t-1 $ as a reference to find the movement at $ t $. This can be obtained through feature point matching, optical flow, or direct methods, but here we \textbf{only care about motion, not care about structure}. In other words, as long as the motion is successfully obtained through the feature points, we no longer need the feature points for this frame. This approach is certainly flawed, but ignoring the large number of feature points can save a lot of calculations. Then, from the time of $ t $ to $ t + 1 $, we use the time of $ t $ as a reference frame to consider the motion relationship between $ t $ and $ t + 1 $. In this way, a trajectory is obtained.

\begin{figure}[!ht]
\centering
\includegraphics[width = 0.85 \linewidth]{designVO/pairwiseVO} \\
\caption{Two or two frames of VO. }
\label{fig: pairwise-VO}
\end{figure}

The way this VO works is simple, but there are several implementations. We take the traditional matching feature point-PnP method as an example to achieve it again. I hope that the reader can combine the knowledge of the previous lectures to realize the VO of the optical flow/direct method or ICP for motion. In the way of matching feature points, the most important is the feature matching relationship between the reference frame and the current frame. Its process can be summarized as follows:

\begin{mdframed}
\begin{enumerate}
\item Extract keypoints and descriptors for the new current frame.
\item If the system is not initialized, use this frame as the reference frame, calculate the 3D position of the key point according to the depth map, and return to step 1.
\item Estimates the motion between the reference frame and the current frame.
\item Determines whether the above estimation was successful.
\item If successful, use the current frame as the new reference frame and return to step 1.
\item If it fails, record the number of consecutive lost frames. When the continuous loss exceeds a certain number of frames, the VO state is set to lost, and the algorithm ends. If not, go back to step 1.
\end{enumerate}
\end{mdframed}

The VisualOdometry class gives an implementation of the above algorithm.

\begin{lstlisting}[language = c ++, caption = slambook/project/0.2/include/myslam/visual \_odometry.h]
class VisualOdometry
{
public:
	typedef shared_ptr<VisualOdometry> Ptr;
	enum VOState {
		INITIALIZING=-1,
		OK=0,
		LOST
	};

	VOState     state_; // current VO status
	Map::Ptr    map_; // map with all frames and map points
	Frame::Ptr  ref_; // reference frame 
	Frame::Ptr  curr_; // current frame 

	cv::Ptr<cv::ORB> orb_; // orb detector and computer 
	vector<cv::Point3f> pts_3d_ref_; // 3d points in reference frame 
	vector<cv::KeyPoint> keypoints_curr_; // keypoints in current frame
	Mat descriptors_curr_; // descriptor in current frame 
	Mat descriptors_ref_; // descriptor in reference frame 
	vector<cv::DMatch> feature_matches_;

	SE3 T_c_r_estimated_; // the estimated pose of current frame 
	int num_inliers_; // number of inlier features in icp
	int num_lost_; // number of lost times

	// parameters 
	int num_of_features_; // number of features
	double scale_factor_; // scale in image pyramid
	int level_pyramid_; // number of pyramid levels
	float match_ratio_; // ratio for selecting  good matches
	int max_num_lost_; // max number of continuous lost times
	int min_inliers_; // minimum inliers
	
	double key_frame_min_rot; // minimal rotation of two key-frames
	double key_frame_min_trans; // minimal translation of two key-frames

public: // functions 
	VisualOdometry();
	~VisualOdometry();

	bool addFrame( Frame::Ptr frame );      // add a new frame 

protected:  
	// inner operation 
	void extractKeyPoints();
	void computeDescriptors(); 
	void featureMatching();
	void poseEstimationPnP(); 
	void setRef3DPoints();
	
	void addKeyFrame();
	bool checkEstimatedPose(); 
	bool checkKeyFrame();
};
\end{lstlisting}

There are a few things to explain about this VisualOdometry class:

\begin{enumerate}
\item VO itself has several states: setting the first frame, tracking smoothly or missing, you can think of it as a Finite State Machine (FSM). Of course, there can be more states, for example, the monocular VO has at least one initialization state. In our implementation, consider the three simplest states: initialized, normal, and lost.
\item We define some intermediate variables in the class, which can save complicated parameter passing. Because they are defined inside the class, they can be accessed by various functions.
\item The parameters in feature extraction and matching are read from the parameter file. E.g:
\begin{lstlisting}[language = c ++]
VisualOdometry :: VisualOdometry ():
state_ (INITIALIZING), ref_ (nullptr), curr_ (nullptr), map_ (new Map), num_lost_ (0), num_inliers_ (0)
{
	num_of_features_    = Config::get<int> ( "number_of_features" );
	scale_factor_       = Config::get<double> ( "scale_factor" );
	level_pyramid_      = Config::get<int> ( "level_pyramid" );
	match_ratio_        = Config::get<float> ( "match_ratio" );
	...
}
\end{lstlisting}
\item addFrame function is an interface called externally. When using VO, after loading the image data into the Frame class, call addFrame to estimate its pose. This function performs different operations depending on the state of VO:
\begin{lstlisting}[language = c ++]
bool VisualOdometry :: addFrame (Frame :: Ptr frame)
{
	switch ( state_ )
	{
	case INITIALIZING:
	{
		state_ = OK;
		curr_ = ref_ = frame;
		map_->insertKeyFrame ( frame );
		// extract features from first frame 
		extractKeyPoints();
		computeDescriptors();
		// compute the 3d position of features in ref frame 
		setRef3DPoints();
		break;
	}
	case OK:
	{
		curr_ = frame;
		extractKeyPoints();
		computeDescriptors();
		featureMatching();
		poseEstimationPnP();
		if ( checkEstimatedPose() == true ) // a good estimation
		{
			curr_->T_c_w_ = T_c_r_estimated_ * ref_->T_c_w_;  // T_c_w = T_c_r*T_r_w 
			ref_ = curr_;
			setRef3DPoints();
			num_lost_ = 0;
			if ( checkKeyFrame() == true ) // is a key-frame
			{
				addKeyFrame();
			}
		}
		else // bad estimation due to various reasons
		{
			num_lost_++;
			if ( num_lost_ > max_num_lost_ )
			{
				state_ = LOST;
			}
			return false;
		}
		break;
	}
	case LOST:
	{
		cout<<"vo has lost."<<endl;
		break;
	}
	}
	return true;
}
\end{lstlisting}
\end{enumerate}

It is worth mentioning that, for various reasons, the above-mentioned VO algorithm we designed may fail at each step. For example, it is difficult to mention features in the picture, feature points lack depth values, mismatches, errors in motion estimation, and so on. Therefore, to design a robust VO, it is necessary (explicitly) to take into account all the above possible mistakes-that naturally makes the program very complicated. In checkEstimatedPose, we do a simple test based on the number of inliers and the size of the movement: we think that the inside point cannot be too small, and the movement cannot be too large. Of course, readers can also think about other ways to detect problems and try the results.

We have omitted the rest of the implementation of the VisualOdometry class, and the reader can find all the source code on GitHub. Finally, we add the test program of this VO to the test and use the data set to observe the estimated motion effect:

\begin{lstlisting}[language = c ++, caption = slambook/project/0.2/test/run \_vo.cpp]
int main (int argc, char ** argv)
{
	if ( argc != 2 )
	{
		cout<<"usage: run_vo parameter_file"<<endl;
		return 1;
	}

	myslam::Config::setParameterFile ( argv[1] );
	myslam::VisualOdometry::Ptr vo ( new myslam::VisualOdometry );

	string dataset_dir = myslam::Config::get<string> ( "dataset_dir" );
	cout<<"dataset: "<<dataset_dir<<endl;
	ifstream fin ( dataset_dir+"/associate.txt" );
	if ( !fin )
	{
		cout<<"please generate the associate file called associate.txt!"<<endl;
		return 1;
	}

	vector<string> rgb_files, depth_files;
	vector<double> rgb_times, depth_times;
	while ( !fin.eof() )
	{
		string rgb_time, rgb_file, depth_time, depth_file;
		fin>>rgb_time>>rgb_file>>depth_time>>depth_file;
		rgb_times.push_back ( atof ( rgb_time.c_str() ) );
		depth_times.push_back ( atof ( depth_time.c_str() ) );
		rgb_files.push_back ( dataset_dir+"/"+rgb_file );
		depth_files.push_back ( dataset_dir+"/"+depth_file );
		
		if ( fin.good() == false )
			break;
	}
	
	myslam::Camera::Ptr camera ( new myslam::Camera );
	
	// visualization
	cv::viz::Viz3d vis("Visual Odometry");
	cv::viz::WCoordinateSystem world_coor(1.0), camera_coor(0.5);
	cv::Point3d cam_pos( 0, -1.0, -1.0 ), cam_focal_point(0,0,0), cam_y_dir(0,1,0);
	cv::Affine3d cam_pose = cv::viz::makeCameraPose( cam_pos, cam_focal_point, cam_y_dir );
	vis.setViewerPose( cam_pose );
	
	world_coor.setRenderingProperty(cv::viz::LINE_WIDTH, 2.0);
	camera_coor.setRenderingProperty(cv::viz::LINE_WIDTH, 1.0);
	vis.showWidget( "World", world_coor );
	vis.showWidget( "Camera", camera_coor );
	
	cout<<"read total "<<rgb_files.size() <<" entries"<<endl;
	for ( int i=0; i<rgb_files.size(); i++ )
	{
		Mat color = cv::imread ( rgb_files[i] );
		Mat depth = cv::imread ( depth_files[i], -1 );
		if ( color.data==nullptr || depth.data==nullptr )
		break;
		myslam::Frame::Ptr pFrame = myslam::Frame::createFrame();
		pFrame->camera_ = camera;
		pFrame->color_ = color;
		pFrame->depth_ = depth;
		pFrame->time_stamp_ = rgb_times[i];
		
		boost::timer timer;
		vo->addFrame ( pFrame );
		cout<<"VO costs time: "<<timer.elapsed()<<endl;
		
		if ( vo->state_ == myslam::VisualOdometry::LOST )
			break;
		SE3 Tcw = pFrame->T_c_w_.inverse();
		
		// show the map and the camera pose 
		cv::Affine3d M(
			cv::Affine3d::Mat3( 
				Tcw.rotation_matrix()(0,0), Tcw.rotation_matrix()(0,1), Tcw.rotation_matrix()(0,2),
				Tcw.rotation_matrix()(1,0), Tcw.rotation_matrix()(1,1), Tcw.rotation_matrix()(1,2),
				Tcw.rotation_matrix()(2,0), Tcw.rotation_matrix()(2,1), Tcw.rotation_matrix()(2,2)
			), 
			cv::Affine3d::Vec3(
				Tcw.translation()(0,0), Tcw.translation()(1,0), Tcw.translation()(2,0)
			)
		);
		cv::imshow("image", color );
		cv::waitKey(1);
		vis.setWidgetPose( "Camera", M);
		vis.spinOnce(1, false);
	}
	return 0;
}
\end{lstlisting}

In order to run this program, several things need to be done:

\begin{enumerate}
\item Because we use the Viz module of OpenCV 3 to display the estimated pose, please make sure you have OpenCV 3 installed and the viz module has been compiled and installed.
\item Prepare one of the TUM datasets. For simplicity, I recommend fr1 \_xyz. Please use associate.py to generate a pairing file associate.txt. The TUM dataset format is described in the ~ \ref{sec: LKFlow} ~ section.
\item Fill in the path of your data set in config/default.yaml, refer to the author's writing. Then, use
\begin{lstlisting}[language = sh]
bin/run_vo config/default.yaml
\end{lstlisting}
Run the program and you can see the real-time demo, as shown in \autoref{fig: vo02exp} ~.
\end{enumerate}

\begin{figure}[!htp]
\centering
\includegraphics[width = 0.9 \linewidth]{designVO/vo02exp} \\
\caption{Vo demo for version 0.2. }
\label{fig: vo02exp}
\end{figure}

In the demo program, you can see the image of the current frame and its estimated position. We have drawn the world axis (large) and the current frame (small). The corresponding relationship between the color and the axis is: blue—Z, red \mbox{色 —X}, green—Y. You can intuitively feel the movement of the camera, which is roughly consistent with our human feelings, although there is still a certain gap between the effect and the expectations. The program also outputs the time for a single calculation of VO. On the author's machine, it can run at a speed of about 30ms. Reducing the number of feature points can increase the calculation speed. The reader can modify the operating parameters and data set to see how it performs in various situations.

\subsection{discussion}
In this section, we have implemented a simple visual odometry between two or two frames, but its effect is not ideal in terms of speed or accuracy. It seems that this seemingly simple idea does not give good results. Let's consider the possible reasons:

\begin{enumerate}
\item
For pose estimation, we used the PnP solution obtained by RANSAC. Because RANSAC uses only a few random points to calculate PnP, although the inlier can be determined, this method is susceptible to noise. In the case of noise at 3D-2D points, we need to use RANSAC's solution as the initial value, and then use nonlinear optimization to find an optimal value. The next section will show that this approach is better than the current one.
Hell
\item
Since the current VO is unstructured, the 3D positions of feature points are treated as true values ​​to estimate motion. But in fact, the depth map of RGB-D must have certain errors, especially those where the depth is too close or too far. And, because feature points are often located at the edges of objects, depth measurements in those places are often inaccurate. So the current method is not accurate enough, we need to optimize the feature points together.
Hell
\item only considers the reference frame/current frame, on the one hand, the pose estimation is too dependent on the reference frame. If the reference frame quality is too poor, for example, in the case of severe occlusion and lighting changes, the tracking is easily lost. In addition, when the reference frame pose is inaccurate, a significant \textbf{drift} will occur. On the other hand, using only two frames of data obviously does not make full use of all the information. A more natural way is to compare the current frame and the map, rather than the current frame and the reference frame. Therefore, we need to care about \textbf{how to match the current frame with the map, and how to optimize the map points}.
Hell
\item Since the running time of each step is output, we can have a general understanding of the amount of calculation (\autoref{table: runtime-exp1}).
Hell
\begin{table}[!ht]
\centering
\caption{Elapsed steps of a cycle}
\label{table: runtime-exp1}
\begin{tabu}{c | c | c | c | c | c | c}
\toprule
Item & Feature Extraction & Descriptor Calculation & Feature Matching & PnP Solving & Others & Total \\\midrule
Time & 0.0102 & 0.0087 & 0.0118 & 0.0011 & 0.0001 & 0.0319 \\
\bottomrule
\end{tabu}
\end{table}
Hell
It can be seen that the extraction and matching of feature points occupy most of the calculation time, and the seemingly complex PnP optimization, the amount of calculation is basically negligible compared to it. Therefore, how to improve the speed of feature extraction and matching algorithms will be an important topic of feature point methods. One predictable way is to use direct method/optical flow, which can effectively avoid heavy feature calculation work. The direct method and optical flow method have been discussed before in this book, and the reader may wish to try it out by themselves.
\end{enumerate}

\section{Improved: Optimizing PnP results}
Next, we follow the previous content and try some ways to improve VO. In this section, we will try to estimate the camera pose using RANSAC PnP plus iterative optimization to see if it improves the effect of the previous section.

\clearpage
The solution of nonlinear optimization problems has been introduced in lectures 6 and 7. Since the goal of this section is to estimate the pose rather than the structure, we use the camera pose $ \bm{\xi} $ as the optimization variable to construct the optimization problem by minimizing reprojection errors. As before, we customize an optimized edge in g2o. It only optimizes one pose, so it is a unary edge.
\begin{lstlisting}[language=c++,caption=slambook/project/0.3/include/myslam/g2o\_types.h]
class EdgeProjectXYZ2UVPoseOnly: public g2o::BaseUnaryEdge<2, Eigen::Vector2d, g2o::VertexSE3Expmap >
{
public:
	EIGEN_MAKE_ALIGNED_OPERATOR_NEW
	
	virtual void computeError();
	virtual void linearizeOplus();
	
	virtual bool read( std::istream& in ){}
	virtual bool write(std::ostream& os) const {};
	
	Vector3d point_;
	Camera* camera_;
};
\end{lstlisting}

Put the 3D point and camera model into its member variables to facilitate the calculation of reprojection error and Jacobian matrix:

\begin{lstlisting}[language=c++,caption=slambook/project/0.3/src/g2o\_types.cpp]
void EdgeProjectXYZ2UVPoseOnly::computeError()
{
const g2o::VertexSE3Expmap* pose = static_cast<const g2o::VertexSE3Expmap*> ( _vertices[0] );
	_error = _measurement - camera_->camera2pixel ( 
		pose->estimate().map(point_) 
	);
}

void EdgeProjectXYZ2UVPoseOnly::linearizeOplus()
{
	g2o::VertexSE3Expmap* pose = static_cast<g2o::VertexSE3Expmap*> ( _vertices[0] );
	g2o::SE3Quat T ( pose->estimate() );
	Vector3d xyz_trans = T.map ( point_ );
	double x = xyz_trans[0];
	double y = xyz_trans[1];
	double z = xyz_trans[2];
	double z_2 = z*z;
	
	_jacobianOplusXi ( 0,0 ) =  x*y/z_2 *camera_->fx_;
	_jacobianOplusXi ( 0,1 ) = - ( 1+ ( x*x/z_2 ) ) *camera_->fx_;
	_jacobianOplusXi ( 0,2 ) = y/z * camera_->fx_;
	_jacobianOplusXi ( 0,3 ) = -1./z * camera_->fx_;
	_jacobianOplusXi ( 0,4 ) = 0;
	_jacobianOplusXi ( 0,5 ) = x/z_2 * camera_->fx_;
	
	_jacobianOplusXi ( 1,0 ) = ( 1+y*y/z_2 ) *camera_->fy_;
	_jacobianOplusXi ( 1,1 ) = -x*y/z_2 *camera_->fy_;
	_jacobianOplusXi ( 1,2 ) = -x/z *camera_->fy_;
	_jacobianOplusXi ( 1,3 ) = 0;
	_jacobianOplusXi ( 1,4 ) = -1./z *camera_->fy_;
	_jacobianOplusXi ( 1,5 ) = y/z_2 *camera_->fy_;
}
\end{lstlisting}

然后，在之前的PoseEstimationPnP函数中，修改成以RANSAC PnP结果为初值，再调用g2o进行优化的形式：
\begin{lstlisting}[language=c++,caption=slambook/project/0.3/src/visual\_odometry.cpp]
void VisualOdometry::poseEstimationPnP()
{
	... 
	// using bundle adjustment to optimize the pose 
	typedef g2o::BlockSolver<g2o::BlockSolverTraits<6,2>> Block;
	Block::LinearSolverType* linearSolver = new g2o::LinearSolverDense<Block::PoseMatrixType>();
	Block* solver_ptr = new Block( linearSolver );
	g2o::OptimizationAlgorithmLevenberg* solver = new g2o::OptimizationAlgorithmLevenberg ( solver_ptr );
	g2o::SparseOptimizer optimizer;
	optimizer.setAlgorithm ( solver );
	
	g2o::VertexSE3Expmap* pose = new g2o::VertexSE3Expmap();
	pose->setId ( 0 );
	pose->setEstimate ( g2o::SE3Quat (
		T_c_r_estimated_.rotation_matrix(), T_c_r_estimated_.translation()
	) );
	optimizer.addVertex ( pose );
	
	// edges
	for ( int i=0; i<inliers.rows; i++ )
{
		int index = inliers.at<int>(i,0);
		// 3D -> 2D projection
		EdgeProjectXYZ2UVPoseOnly* edge = new EdgeProjectXYZ2UVPoseOnly();
		edge->setId(i);
		edge->setVertex(0, pose);
		edge->camera_ = curr_->camera_.get();
		edge->point_ = Vector3d( pts3d[index].x, pts3d[index].y, pts3d[index].z );
		edge->setMeasurement( Vector2d(pts2d[index].x, pts2d[index].y) );
		edge->setInformation( Eigen::Matrix2d::Identity() );
		optimizer.addEdge( edge );
	}
	
	optimizer.initializeOptimization();
	optimizer.optimize(10);
	
	T_c_r_estimated_ = SE3 (
		pose->estimate().rotation(),
		pose->estimate().translation()
	);
\end{lstlisting}

The reader is asked to run this program and compare the previous results. You will notice that the estimated motion has stabilized a lot. At the same time, because the new optimization is still unstructured and small in scale, the impact on the calculation time can be ignored. The overall visual odometer calculation time is still around 30ms.

\subsection *{discussion}
We find that the quality of the estimated results is significantly improved compared with pure RANSAC PnP after the iterative optimization method is introduced. Although we still only use the information between two or two frames, the resulting motion is more accurate and smooth. From this improvement we see the importance of optimization. However, the 0.3 version of VO is still affected by the limitations of matching between two frames. Once a frame in a video sequence is lost, subsequent frames cannot match the previous frame. Next, we introduce maps into VO.

\section{Improvement: Partial Map}
In this section, we put the feature points matched by VO into the map, and match the current frame with the map points to calculate the pose. The difference between this approach and the previous is shown in \autoref{fig: vo04} ~.

\begin{figure}[!htp]
\centering
\includegraphics[width = 0.9 \linewidth]{designVO/vo04} \\
\caption{The difference between the working principle of two or two frames VO and map VO. }
\label{fig: vo04}
\end{figure}

In the comparison between two frames, we only calculate the feature matching and motion relationship between the reference frame and the current frame, and set the current frame as the new reference frame after the calculation. In VO using maps, each frame contributes some information to the map, such as adding new feature points or updating the position estimates of old feature points. The location of feature points in the map is often in world coordinates. Therefore, when the current frame arrives, we find the feature matching and motion relationship between it and the map, that is, directly calculate $ \bm{T} _{\mathrm{cw}} $.

\newpage
The benefit of this is that we are able to maintain a constantly updated map. As long as the map is correct, even if something goes wrong in the middle of the frame, there is still hope to find the correct position of those subsequent frames. Please note that we have not discussed the \textbf{建 图} problem of SLAM in detail at this time, so the map here is only a temporary concept, which refers to the set of feature points formed by caching feature points of each frame to a place .

Maps can be divided into \textbf{Local} maps and \textbf{Global} maps. Because of different uses, maps are often discussed separately. As the name suggests, the local map describes nearby feature point information-we only keep feature points that are closer to the camera's current location, and discard feature points that are far away or out of the field of view. These feature points are used to find the camera position by matching the current frame, so we hope it can be done faster. On the other hand, the global map records all feature points since SLAM was run. Obviously, it is larger in size and is mainly used to express the entire environment, but positioning it directly on the global map places too much burden on the computer. It is mainly used for loop detection and map expression.

In visual odometers, we are more concerned with local maps that can be used directly for positioning (if we decide to use maps). So in this lecture we will maintain a local map. As the camera moves, we add new feature points to the map. We still want to remind readers: whether to use maps depends on your contradictory grasp of precision-efficiency. We can completely use pairless unstructured VO for efficiency reasons; we can also build local maps and even consider map optimization for better accuracy.

One troublesome part of a local map is maintaining its scale. In order to ensure real-time, we need to ensure that the map size is not too large (otherwise matching will consume a lot of time). In addition, there are some acceleration methods for single frame and map feature matching, but due to the technical complexity, we will not give them in our routine.

Now, let's implement the map point class. We slightly improved the MapPoint class that was not used before, mainly its constructor and generator functions.
\begin{lstlisting}[language=c++,caption=slambook/project/0.4/include/myslam/mappoint.h]
class MapPoint
{
	public:
	typedef shared_ptr<MapPoint> Ptr;
	unsigned long      id_; // ID
	static unsigned long factory_id_; // factory id
	bool  good_; // whether a good point 
	Vector3d pos_; // Position in world
	Vector3d norm_; // Normal of viewing direction 
	Mat descriptor_; // Descriptor for matching 
	
	list<Frame*> observed_frames_;   // key-frames that can observe this point 
	
	int matched_times_; // being an inliner in pose estimation
	int visible_times_;  // being visible in current frame 
	
	MapPoint();
	MapPoint ( 
		unsigned long id, 
		const Vector3d& position, 
		const Vector3d& norm, 
		Frame* frame=nullptr, 
		const Mat& descriptor=Mat() 
	);
	
	inline cv::Point3f getPositionCV() const {
		return cv::Point3f( pos_(0,0), pos_(1,0), pos_(2,0) );
	}
	
	static MapPoint::Ptr createMapPoint();
	static MapPoint::Ptr createMapPoint ( 
		const Vector3d& pos_world, 
		const Vector3d& norm_,
		const Mat& descriptor,
		Frame* frame 
	);
};
\end{lstlisting}

The main modification is on the VisualOdometry class. Due to changes in the workflow, we have modified several of its main functions, such as adding and deleting maps in each loop, counting the number of times each map point has been observed, etc. \footnote{Of course, from a C ++ design perspective In general, retaining the previous approach and using inheritance will reuse existing code more effectively. }. These things are trivial, so we recommend readers to take a closer look at the source code provided on GitHub. Focus on the following:

\begin{enumerate}
\item After extracting the feature points of the first frame, put all the feature points of the first frame into the map:
\begin{lstlisting}[language = c ++]
void VisualOdometry::addKeyFrame()
{
	if ( map_->keyframes_.empty() )
	{
		// first key-frame, add all 3d points into map
		for ( size_t i=0; i<keypoints_curr_.size(); i++ )
		{
			double d = curr_->findDepth ( keypoints_curr_[i] );
			if ( d < 0 ) 
				continue;
			Vector3d p_world = ref_->camera_->pixel2world (
				Vector2d ( keypoints_curr_[i].pt.x, keypoints_curr_[i].pt.y ), curr_->T_c_w_, d
			);
			Vector3d n = p_world - ref_->getCamCenter();
			n.normalize();
			MapPoint::Ptr map_point = MapPoint::createMapPoint(
				p_world, n, descriptors_curr_.row(i).clone(), curr_.get()
			);
			map_->insertMapPoint( map_point );
}
}
	map_->insertKeyFrame ( curr_ );
	ref_ = curr_;
}
\end{lstlisting}
	\item 后续的帧中，使用OptimizeMap函数对地图进行优化。包括删除不在视野内的点，在匹配数量减少时添加新点，等等。
\begin{lstlisting}[language=c++]
void VisualOdometry::optimizeMap()
{
	// remove the hardly seen and no visible points 
	for ( auto iter = map_->map_points_.begin(); iter != map_->map_points_.end(); )
	{
		if ( !curr_->isInFrame(iter->second->pos_) )
		{
			iter = map_->map_points_.erase(iter);
			continue;
		}
		float match_ratio = float(iter->second->matched_times_)/iter->second->visible_times_;
		if ( match_ratio < map_point_erase_ratio_ )
		{
			iter = map_->map_points_.erase(iter);
			continue;
		}
		double angle = getViewAngle( curr_, iter->second );
		if ( angle > M_PI/6. )
		{
			iter = map_->map_points_.erase(iter);
			continue;
		}
		if ( iter->second->good_ == false )
		{
			// TODO try triangulate this map point 
		}
		iter++;
	}
	
	if ( match_2dkp_index_.size()<100 )
		addMapPoints();
	if ( map_->map_points_.size() > 1000 )  
	{
		// TODO map is too large, remove some one 
		map_point_erase_ratio_ += 0.05;
	}
	else 
		map_point_erase_ratio_ = 0.1;
	cout<<"map points: "<<map_->map_points_.size()<<endl;
}
\end{lstlisting}
We have intentionally left some places blank, please ask interested readers to complete it by themselves. For example, you can use triangulation to update the world coordinates of feature points, or consider strategies to better manage map size dynamically. These questions are open.

\item Feature matching code. Before matching, we take some candidate points (points that appear in the field of view) from the map and then match them with the feature descriptors of the current frame.
\begin{lstlisting}[language=c++]
void VisualOdometry::featureMatching()
{
	boost::timer timer;
	vector<cv::DMatch> matches;
	// select the candidates in map 
	Mat desp_map;
	vector<MapPoint::Ptr> candidate;
	for ( auto& allpoints: map_->map_points_ )
	{
		MapPoint::Ptr& p = allpoints.second;
		// check if p in curr frame image 
		if ( curr_->isInFrame(p->pos_) )
		{
			// add to candidate 
			p->visible_times_++;
			candidate.push_back( p );
			desp_map.push_back( p->descriptor_ );
		}
	}
	
	matcher_flann_.match ( desp_map, descriptors_curr_, matches );
	// select the best matches
	float min_dis = std::min_element (
		matches.begin(), matches.end(),
		[] ( const cv::DMatch& m1, const cv::DMatch& m2 )
		{
			return m1.distance < m2.distance;
		} )->distance;
	
	match_3dpts_.clear();
	match_2dkp_index_.clear();
	for ( cv::DMatch& m : matches )
	{
		if ( m.distance < max<float> ( min_dis*match_ratio_, 30.0 ) )
		{
			match_3dpts_.push_back( candidate[m.queryIdx] );
			match_2dkp_index_.push_back( m.trainIdx );
		}
	}
	cout<<"good matches: "<<match_3dpts_.size() <<endl;
	cout<<"match cost time: "<<timer.elapsed() <<endl;
}
\end{lstlisting}

\end{enumerate}

In addition to existing maps, we have introduced the concept of "key-frames". Key frames are used in many visual SLAMs, but this concept is mainly used by the backend, so we will discuss the detailed processing of key frames in the next few lectures. In practice, we certainly don't want to do detailed optimization and loop detection for each image, because it is too resource-intensive after all. At least when the camera is standing still, we don't want the entire model (both maps and trajectories) to get bigger and bigger. Therefore, the main object of back-end optimization is key frames.

\clearpage
The key frames are some special frames during camera movement. Here, the meaning of "special" can be specified by us. In common practice, every time the camera moves through a certain interval, a new key frame is taken and saved \footnote{This is easy to implement in Lie algebra, please think about how to achieve it. }. The poses of these key frames will be carefully optimized, and those things located between the two key frames, except for contributing some map points to the map, are naturally ignored.

The implementation in this section will also extract some key frames to make some data preparations for back-end optimization. Now the reader can compile this project and see the results of its operation. The routines in this section project the points of the local map onto the image plane and display them. If the pose estimates are correct, they should look like \textbf{fixed in space}. Conversely, if you feel that a certain feature point moves unnaturally, it may be that the camera pose estimation is not accurate enough, or the position of the feature point is not accurate enough.

We did not provide map optimization in version 0.4, and readers are recommended to try it out by themselves. The principles used are mainly least squares and triangulation, which have already been introduced in the first two lectures and will not be too difficult.

\begin{figure}[!htp]
\centering
\includegraphics[width = 0.9 \linewidth]{resources/designVO/vo04}
\caption{0.4 version of VO's running screenshot, marked the landmark projection points at two different times. }
\label{fig: vo04exp}
\end{figure}

\section{Summary}

As a practice, this lecture leads the reader to implement a simple visual odometer from scratch, in order to allow the reader to have an empirical understanding of the algorithms introduced in the previous two lectures. Without this lecture, it is difficult for you to personally experience questions such as "how many ORB feature points can the VO of a feature point handle in real time" \footnote{Of course, it depends on the performance of your machine. }. We see that the visual odometer can estimate the camera movement and the position of feature points in a local time, but this local approach has obvious disadvantages:

\begin{enumerate}
\item is easy to lose. Once lost, we either "wait for the camera to turn back" (save the reference frame and compare it with the new frame) or reset the entire VO to track the new image data.
\item Trajectory drift. The main reason is that the error of each estimation will accumulate to the next estimation, resulting in inaccurate long-term trajectories. A larger local map can alleviate this phenomenon, but it always exists.
\end{enumerate}

It is worth mentioning that if you only care about the movement in a short period of time, or the accuracy of the VO has met the needs of the application, then sometimes you may only need a visual odometer instead of a complete SLAM. For example, in some drone control or AR game applications, users do not need a globally consistent map, so the portable VO may be a better choice. However, the goal of this book is to introduce the entire SLAM, so we have to go further and see how backend and loopback detection work.

\section*{EXERCISE}
\begin{enumerate}
\item Do you understand the C ++ techniques used in this book? If you don't understand something, use search engines to supplement your knowledge, including: range-based for loops, lambda expressions, smart pointers, singleton patterns in design patterns, and more.
\item Based on version 0.3 or 0.4, add code to optimize the map. Alternatively, you can also perform triangulation according to the PnP result to eliminate the error of the RGB-D depth value.
\item Observe how this code handles mismatches. What is RANSAC? Read the literature \cite{wiki:RANSAC} or search for related materials to learn more.
\end{enumerate}
